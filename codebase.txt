Directory structure:
└── crawler_system_v0_local_test/
    ├── README.md
    ├── docker-compose-producer-network.yml
    ├── docker-compose-worker-network.yml
    ├── Dockerfile
    ├── genenv.py
    ├── local.ini
    ├── mysql-network.yml
    ├── portainer-network.yml
    ├── pyproject.toml
    ├── rabbitmq-network.yml
    ├── requirements.txt
    ├── uv.lock
    ├── v0_local_test.md
    ├── .python-version
    ├── crawler/
    │   ├── __init__.py
    │   ├── check_crawler_config.py
    │   ├── config.py
    │   ├── logging_config.py
    │   ├── worker.py
    │   ├── database/
    │   │   ├── connection.py
    │   │   ├── models.py
    │   │   ├── repository.py
    │   │   ├── schemas.py
    │   │   ├── category_classification_data/
    │   │   │   ├── apply_classification.py
    │   │   │   ├── check_all_category_parents.py
    │   │   │   ├── get_root_categories_script.py
    │   │   │   ├── job_classification_scheme.md
    │   │   │   ├── major_categories.json
    │   │   │   ├── mapping.json
    │   │   │   ├── root_categories.json
    │   │   │   └── verify_classification.py
    │   │   └── scripts/
    │   │       ├── fix_salary_data.py
    │   │       ├── get_category_ids.py
    │   │       ├── pandas_sql_config.py
    │   │       └── temp_count_db.py
    │   ├── geocoding/
    │   │   ├── client.py
    │   │   └── task.py
    │   ├── project_104/
    │   │   ├── 104人力銀行_crawl.ipynb
    │   │   ├── client_104.py
    │   │   ├── config_104.py
    │   │   ├── local_fetch_104_url_data.py
    │   │   ├── page_api_data_104.txt
    │   │   ├── parser_apidata_104.py
    │   │   ├── producer_category_104.py
    │   │   ├── producer_jobs_104.py
    │   │   ├── producer_urls_104.py
    │   │   ├── single_url_api_data_104.py
    │   │   ├── single_url_api_data_104.txt
    │   │   ├── task_category_104.py
    │   │   ├── task_jobs_104.py
    │   │   └── task_urls_104.py
    │   ├── project_1111/
    │   │   ├── 1111_人力銀行_crawl.ipynb
    │   │   ├── client_1111.py
    │   │   ├── config_1111.py
    │   │   ├── page_api_data_1111.txt
    │   │   ├── parser_apidata_1111.py
    │   │   ├── producer_category_1111.py
    │   │   ├── producer_jobs_1111.py
    │   │   ├── producer_urls_1111.py
    │   │   ├── task_category_1111.py
    │   │   ├── task_jobs_1111.py
    │   │   └── task_urls_1111.py
    │   ├── project_cakeresume/
    │   │   ├── Cake_me_crawl.ipynb
    │   │   ├── client_cakeresume.py
    │   │   ├── config_cakeresume.py
    │   │   ├── parser_cakeresume.py
    │   │   ├── producer_category_cakeresume.py
    │   │   ├── producer_jobs_cakeresume.py
    │   │   ├── producer_urls_cakeresume.py
    │   │   ├── task_category_cakeresume.py
    │   │   ├── task_jobs_cakeresume.py
    │   │   └── task_urls_cakeresume.py
    │   ├── project_yes123/
    │   │   ├── client_yes123.py
    │   │   ├── config_yes123.py
    │   │   ├── hight_Level_refine_url_yes123.py
    │   │   ├── producer_category_yes123.py
    │   │   ├── producer_jobs_yes123.py
    │   │   ├── producer_urls_yes123.py
    │   │   ├── task_category_yes123.py
    │   │   ├── task_jobs_yes123.py
    │   │   ├── task_urls_yes123.py
    │   │   ├── yes123_人力銀行_crawl.ipynb
    │   │   └── yes123_人力銀行_jobcat_json.txt
    │   └── utils/
    │       ├── clean_text.py
    │       └── salary_parser.py
    └── docs/
        ├── development_manual.md
        ├── project_104_docker_manual.md
        ├── project_104_local_test_plan.md
        ├── project_1111_local_test_plan.md
        ├── project_cakeresume_local_test_plan.md
        └── project_yes123_local_test_plan.md

================================================
FILE: README.md
================================================
[Binary file]


================================================
FILE: docker-compose-producer-network.yml
================================================
# version: '3.0'

services:
  producer_104_jobs:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_jobs_producer"
    command: python -m crawler.project_104.producer_jobs_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

  producer_104_category:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_category_producer"
    command: python -m crawler.project_104.producer_category_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

  producer_104_urls:
    image: benitorhuang/crawler_jobs:0.0.2
    hostname: "crawler_104_urls_producer"
    command: python -m crawler.project_104.producer_urls_104
    environment:
      - TZ=Asia/Taipei
    networks:
      - my_network

networks:
  my_network:
    external: true


================================================
FILE: docker-compose-worker-network.yml
================================================
# version: '3.0'  # 使用 Docker Compose 的版本 3.0，適合大部分部署場景

services:
  crawler_104:  # 定義一個服務，名稱為 crawler_twse
    image: benitorhuang/crawler_jobs:0.0.2 

    hostname: "crawler_104_category"
    command: celery -A crawler.worker worker --loglevel=info --hostname=%h -Q producer_jobs_104,producer_urls_104,producer_category_104  
    # 啟動容器後執行的命令，這裡是啟動 Celery worker，指定 app 為 crawler.worker，設定日誌等級為 info，
    # 使用主機名稱當作 worker 名稱（%h），並將此 worker 加入名為 "twse" 的任務佇列 (queue)

    restart: always  # 若容器停止或崩潰，自動重新啟動
    environment:
      - TZ=Asia/Taipei  # 設定時區為台北（UTC+8）
    networks:
      - my_network  # 將此服務連接到 my_network 網路

networks:
  my_network:
    # 加入已經存在的網路
    external: true



================================================
FILE: Dockerfile
================================================
# Stage 1: Builder
FROM python:3.13-slim-bullseye AS builder

WORKDIR /app

RUN apt-get update && apt-get install -y build-essential curl && rm -rf /var/lib/apt/lists/*

COPY requirements.txt uv.lock .
RUN curl -LsSf https://astral.sh/uv/install.sh | sh && /root/.local/bin/uv pip install -r requirements.txt --system


# Stage 2: Runner
FROM python:3.13-slim-bullseye AS runner

WORKDIR /app

# Copy only the installed packages from the builder stage
COPY --from=builder /usr/local/lib/python3.13/site-packages /usr/local/lib/python3.13/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copy the application code
COPY . .

ENV PYTHONPATH="/app"
ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8

CMD ["/bin/bash"]


================================================
FILE: genenv.py
================================================
import os
from configparser import ConfigParser
import structlog
import sys

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)


def generate_env_file():
    config_path = "local.ini"

    if not os.path.exists(config_path):
        logger.critical(
            "local.ini not found. Please ensure it exists in the project root.",
            path=config_path,
        )
        sys.exit(1)

    local_config = ConfigParser()
    try:
        local_config.read(config_path)
    except Exception as e:
        logger.critical(
            "Failed to read local.ini configuration file.",
            path=config_path,
            error=e,
            exc_info=True,
        )
        sys.exit(1)

    # Determine which section to use based on APP_ENV environment variable
    app_env = os.environ.get("APP_ENV", "").upper()

    selected_section_name = "DEFAULT"  # Default fallback
    if app_env and app_env in local_config:
        selected_section_name = app_env
    elif "DEFAULT" not in local_config:
        logger.critical(
            "Neither APP_ENV specified section nor 'DEFAULT' section found in local.ini.",
            app_env=app_env,
        )
        sys.exit(1)

    section = local_config[selected_section_name]
    logger.info("Using configuration section.", section_name=selected_section_name)

    env_content = ""
    for key, value in section.items():
        env_content += f"{key.upper()}={value}\n"

    env_file_path = ".env"
    try:
        with open(env_file_path, "w", encoding="utf8") as env_file:
            env_file.write(env_content)
        logger.info(
            ".env file generated successfully.",
            path=env_file_path,
            section_used=selected_section_name,
        )
    except Exception as e:
        logger.critical(
            "Failed to write .env file.", path=env_file_path, error=e, exc_info=True
        )
        sys.exit(1)


if __name__ == "__main__":
    generate_env_file()



================================================
FILE: local.ini
================================================
# dev 環境
[DEV]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = 127.0.0.1
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = 127.0.0.1
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = console
# Producer Batching
PRODUCER_BATCH_SIZE = 20000000
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5
JOB_LISTING_BASE_URL_CAKERESUME = https://www.cakeresume.com/jobs


# Docker 環境
[DOCKER]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = rabbitmq
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = localhost
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = console
# Producer Batching
PRODUCER_BATCH_SIZE = 20000000
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5
JOB_LISTING_BASE_URL_CAKERESUME = https://www.cakeresume.com/jobs


# Prod 環境
[PRODUCTION]
# Worker
WORKER_ACCOUNT = worker
WORKER_PASSWORD = worker
# RabbitMQ
RABBITMQ_HOST = rabbitmq
RABBITMQ_PORT = 5672
# MySQL
MYSQL_DATABASE = crawler_db
MYSQL_HOST = 127.0.0.1
MYSQL_PORT = 3306
MYSQL_ACCOUNT = root
MYSQL_ROOT_PASSWORD = root_password
MYSQL_PASSWORD = root_password
# Logging
LOG_LEVEL = INFO
LOG_FORMATTER = json
# Producer Batching
PRODUCER_BATCH_SIZE = 20000000
PRODUCER_DISPATCH_INTERVAL_SECONDS = 1.0
# URL Crawler General Settings
URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = 20
URL_CRAWLER_UPLOAD_BATCH_SIZE = 30
URL_CRAWLER_SLEEP_MIN_SECONDS = 0.5
URL_CRAWLER_SLEEP_MAX_SECONDS = 1.5
JOB_LISTING_BASE_URL_CAKERESUME = https://www.cakeresume.com/jobs


================================================
FILE: mysql-network.yml
================================================
version: '3.8'

services:
  crawler_jobs_mysql:
    image: mysql:8.0
    # 設定 mysql 使用原生認證的密碼 hash
    command: mysqld --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_ACCOUNT: ${MYSQL_ACCOUNT}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    ports:
      - "3306:3306"

    restart: always
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - my_network



  crawler_jobs_phpmyadmin:
    image: phpmyadmin/phpmyadmin
    links: 
          - crawler_jobs_mysql:db
    restart: always
    ports:
      - "8080:80"
    networks:
      - my_network

networks:
  my_network:
    external: true

volumes:
  mysql_data:


================================================
FILE: portainer-network.yml
================================================
version: '3.8'

services:
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: always
    ports:
      - "9000:9000"
      - "8000:8000" # For Edge Agent communication
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      - my_network

  portainer_agent:
    image: portainer/agent:latest
    container_name: portainer_agent
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/volumes:/var/lib/docker/volumes
    networks:
      - my_network

networks:
  my_network:
    external: true

volumes:
  portainer_data:


================================================
FILE: pyproject.toml
================================================
[project]
name = "crawler-jobs"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "celery>=5.5.3",
    "ruff>=0.12.5",
    "structlog>=25.4.0",
    "python-dotenv",
    "requests>=2.32.4",
    "pymysql==1.1.0",
    "tenacity>=8.2.3",
    "pydantic>=2.0.0",
    "sqlalchemy>=2.0.41",
    "gitingest>=0.1.5",
    "pandas>=2.3.1",
    "pytest>=8.4.1",
    "beautifulsoup4",
    "lxml",
]

[tool.ruff]
exclude = ["*.ipynb"]

[tool.pytest.ini_options]
python_files = "pytest_*.py"



================================================
FILE: rabbitmq-network.yml
================================================
version: '3'
services:

  rabbitmq:
    # 使用 RabbitMQ 官方管理版的輕量 Alpine 版本映像檔
    image: 'rabbitmq:3.12-management-alpine'
    restart: always  # 若容器停止或崩潰，自動重新啟動
    ports: 
      - '5672:5672'       # 對外開放 RabbitMQ 的 AMQP 通訊埠（應用程式通訊埠）
      - '15672:15672'     # 對外開放 RabbitMQ 的管理介面（Web UI）埠口
    environment:
      RABBITMQ_DEFAULT_USER: "worker"       # 預設使用者名稱設定為 worker
      RABBITMQ_DEFAULT_PASS: "worker"       # 預設密碼設定為 worker
      RABBITMQ_DEFAULT_VHOST: "/"            # 預設虛擬主機 (Virtual Host)，用於隔離不同環境的訊息隊列
    networks:
      - my_network                           # 將服務加入名為 my_network 的自訂網路

  flower:
    # 使用 Flower 映像來監控 Celery 的任務佇列狀況
    image: mher/flower:0.9.5
    command: ["flower", "--broker=amqp://worker:worker@rabbitmq", "--port=5555"]  
    restart: always  # 若容器停止或崩潰，自動重新啟動
    # 啟動 Flower，設定 RabbitMQ 為 broker，並監聽 5555 埠口
    ports: 
      - 5555:5555                           # 映射 Flower 的監控介面埠口到宿主機
    depends_on:
      - rabbitmq                           # 確保 RabbitMQ 先啟動後，Flower 再啟動
    networks:
      - my_network                         # 將服務加入 my_network 網路

networks:
  my_network:
    # 加入已經存在的網路
    external: true


================================================
FILE: requirements.txt
================================================
# This file was autogenerated by uv via the following command:
#    uv pip compile pyproject.toml -o requirements.txt
amqp==5.3.1
    # via kombu
annotated-types==0.7.0
    # via pydantic
anyio==4.9.0
    # via
    #   httpx
    #   starlette
    #   watchfiles
beautifulsoup4==4.13.4
    # via crawler-jobs (pyproject.toml)
billiard==4.2.1
    # via celery
celery==5.5.3
    # via crawler-jobs (pyproject.toml)
certifi==2025.7.14
    # via
    #   httpcore
    #   httpx
    #   requests
    #   sentry-sdk
charset-normalizer==3.4.2
    # via requests
click==8.2.1
    # via
    #   celery
    #   click-didyoumean
    #   click-plugins
    #   click-repl
    #   gitingest
    #   rich-toolkit
    #   typer
    #   uvicorn
click-didyoumean==0.3.1
    # via celery
click-plugins==1.1.1.2
    # via celery
click-repl==0.3.0
    # via celery
deprecated==1.2.18
    # via limits
dnspython==2.7.0
    # via email-validator
email-validator==2.2.0
    # via
    #   fastapi
    #   pydantic
fastapi==0.116.1
    # via gitingest
fastapi-cli==0.0.8
    # via fastapi
fastapi-cloud-cli==0.1.5
    # via fastapi-cli
gitingest==0.1.5
    # via crawler-jobs (pyproject.toml)
greenlet==3.2.3
    # via sqlalchemy
h11==0.16.0
    # via
    #   httpcore
    #   uvicorn
httpcore==1.0.9
    # via httpx
httptools==0.6.4
    # via uvicorn
httpx==0.28.1
    # via
    #   fastapi
    #   fastapi-cloud-cli
idna==3.10
    # via
    #   anyio
    #   email-validator
    #   httpx
    #   requests
iniconfig==2.1.0
    # via pytest
jinja2==3.1.6
    # via fastapi
kombu==5.5.4
    # via celery
limits==5.4.0
    # via slowapi
lxml==6.0.0
    # via crawler-jobs (pyproject.toml)
markdown-it-py==3.0.0
    # via rich
markupsafe==3.0.2
    # via jinja2
mdurl==0.1.2
    # via markdown-it-py
numpy==2.3.2
    # via pandas
packaging==25.0
    # via
    #   kombu
    #   limits
    #   pytest
pandas==2.3.1
    # via crawler-jobs (pyproject.toml)
pathspec==0.12.1
    # via gitingest
pluggy==1.6.0
    # via pytest
prompt-toolkit==3.0.51
    # via click-repl
pydantic==2.11.7
    # via
    #   crawler-jobs (pyproject.toml)
    #   fastapi
    #   fastapi-cloud-cli
    #   gitingest
pydantic-core==2.33.2
    # via pydantic
pygments==2.19.2
    # via
    #   pytest
    #   rich
pymysql==1.1.0
    # via crawler-jobs (pyproject.toml)
pytest==8.4.1
    # via crawler-jobs (pyproject.toml)
python-dateutil==2.9.0.post0
    # via
    #   celery
    #   pandas
python-dotenv==1.1.1
    # via
    #   crawler-jobs (pyproject.toml)
    #   gitingest
    #   uvicorn
python-multipart==0.0.20
    # via fastapi
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via uvicorn
regex==2024.11.6
    # via tiktoken
requests==2.32.4
    # via
    #   crawler-jobs (pyproject.toml)
    #   tiktoken
rich==14.1.0
    # via
    #   rich-toolkit
    #   typer
rich-toolkit==0.14.9
    # via
    #   fastapi-cli
    #   fastapi-cloud-cli
rignore==0.6.4
    # via fastapi-cloud-cli
ruff==0.12.5
    # via crawler-jobs (pyproject.toml)
sentry-sdk==2.33.2
    # via fastapi-cloud-cli
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
slowapi==0.1.9
    # via gitingest
sniffio==1.3.1
    # via anyio
soupsieve==2.7
    # via beautifulsoup4
sqlalchemy==2.0.41
    # via crawler-jobs (pyproject.toml)
starlette==0.47.2
    # via
    #   fastapi
    #   gitingest
structlog==25.4.0
    # via crawler-jobs (pyproject.toml)
tenacity==9.1.2
    # via crawler-jobs (pyproject.toml)
tiktoken==0.9.0
    # via gitingest
tomli==2.2.1
    # via gitingest
typer==0.16.0
    # via
    #   fastapi-cli
    #   fastapi-cloud-cli
typing-extensions==4.14.1
    # via
    #   beautifulsoup4
    #   fastapi
    #   limits
    #   pydantic
    #   pydantic-core
    #   rich-toolkit
    #   sqlalchemy
    #   typer
    #   typing-inspection
typing-inspection==0.4.1
    # via pydantic
tzdata==2025.2
    # via
    #   kombu
    #   pandas
urllib3==2.5.0
    # via
    #   requests
    #   sentry-sdk
uvicorn==0.35.0
    # via
    #   fastapi
    #   fastapi-cli
    #   fastapi-cloud-cli
    #   gitingest
uvloop==0.21.0
    # via uvicorn
vine==5.1.0
    # via
    #   amqp
    #   celery
    #   kombu
watchfiles==1.1.0
    # via uvicorn
wcwidth==0.2.13
    # via prompt-toolkit
websockets==15.0.1
    # via uvicorn
wrapt==1.17.2
    # via deprecated



================================================
FILE: uv.lock
================================================
version = 1
revision = 2
requires-python = ">=3.13"

[[package]]
name = "amqp"
version = "5.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "vine" },
]
sdist = { url = "https://files.pythonhosted.org/packages/79/fc/ec94a357dfc6683d8c86f8b4cfa5416a4c36b28052ec8260c77aca96a443/amqp-5.3.1.tar.gz", hash = "sha256:cddc00c725449522023bad949f70fff7b48f0b1ade74d170a6f10ab044739432", size = 129013, upload-time = "2024-11-12T19:55:44.051Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/26/99/fc813cd978842c26c82534010ea849eee9ab3a13ea2b74e95cb9c99e747b/amqp-5.3.1-py3-none-any.whl", hash = "sha256:43b3319e1b4e7d1251833a93d672b4af1e40f3d632d479b98661a95f117880a2", size = 50944, upload-time = "2024-11-12T19:55:41.782Z" },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081, upload-time = "2024-05-20T21:33:25.928Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643, upload-time = "2024-05-20T21:33:24.1Z" },
]

[[package]]
name = "anyio"
version = "4.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/7d/4c1bd541d4dffa1b52bd83fb8527089e097a106fc90b467a7313b105f840/anyio-4.9.0.tar.gz", hash = "sha256:673c0c244e15788651a4ff38710fea9675823028a6f08a5eda409e0c9840a028", size = 190949, upload-time = "2025-03-17T00:02:54.77Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl", hash = "sha256:9f76d541cad6e36af7beb62e978876f3b41e3e04f2c1fbf0884604c0a9c4d93c", size = 100916, upload-time = "2025-03-17T00:02:52.713Z" },
]

[[package]]
name = "beautifulsoup4"
version = "4.13.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "soupsieve" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d8/e4/0c4c39e18fd76d6a628d4dd8da40543d136ce2d1752bd6eeeab0791f4d6b/beautifulsoup4-4.13.4.tar.gz", hash = "sha256:dbb3c4e1ceae6aefebdaf2423247260cd062430a410e38c66f2baa50a8437195", size = 621067, upload-time = "2025-04-15T17:05:13.836Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/cd/30110dc0ffcf3b131156077b90e9f60ed75711223f306da4db08eff8403b/beautifulsoup4-4.13.4-py3-none-any.whl", hash = "sha256:9bbbb14bfde9d79f38b8cd5f8c7c85f4b8f2523190ebed90e950a8dea4cb1c4b", size = 187285, upload-time = "2025-04-15T17:05:12.221Z" },
]

[[package]]
name = "billiard"
version = "4.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/58/1546c970afcd2a2428b1bfafecf2371d8951cc34b46701bea73f4280989e/billiard-4.2.1.tar.gz", hash = "sha256:12b641b0c539073fc8d3f5b8b7be998956665c4233c7c1fcd66a7e677c4fb36f", size = 155031, upload-time = "2024-09-21T13:40:22.491Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/da/43b15f28fe5f9e027b41c539abc5469052e9d48fd75f8ff094ba2a0ae767/billiard-4.2.1-py3-none-any.whl", hash = "sha256:40b59a4ac8806ba2c2369ea98d876bc6108b051c227baffd928c644d15d8f3cb", size = 86766, upload-time = "2024-09-21T13:40:20.188Z" },
]

[[package]]
name = "celery"
version = "5.5.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "billiard" },
    { name = "click" },
    { name = "click-didyoumean" },
    { name = "click-plugins" },
    { name = "click-repl" },
    { name = "kombu" },
    { name = "python-dateutil" },
    { name = "vine" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/7d/6c289f407d219ba36d8b384b42489ebdd0c84ce9c413875a8aae0c85f35b/celery-5.5.3.tar.gz", hash = "sha256:6c972ae7968c2b5281227f01c3a3f984037d21c5129d07bf3550cc2afc6b10a5", size = 1667144, upload-time = "2025-06-01T11:08:12.563Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c9/af/0dcccc7fdcdf170f9a1585e5e96b6fb0ba1749ef6be8c89a6202284759bd/celery-5.5.3-py3-none-any.whl", hash = "sha256:0b5761a07057acee94694464ca482416b959568904c9dfa41ce8413a7d65d525", size = 438775, upload-time = "2025-06-01T11:08:09.94Z" },
]

[[package]]
name = "certifi"
version = "2025.7.14"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/b3/76/52c535bcebe74590f296d6c77c86dabf761c41980e1347a2422e4aa2ae41/certifi-2025.7.14.tar.gz", hash = "sha256:8ea99dbdfaaf2ba2f9bac77b9249ef62ec5218e7c2b2e903378ed5fccf765995", size = 163981, upload-time = "2025-07-14T03:29:28.449Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4f/52/34c6cf5bb9285074dc3531c437b3919e825d976fde097a7a73f79e726d03/certifi-2025.7.14-py3-none-any.whl", hash = "sha256:6b31f564a415d79ee77df69d757bb49a5bb53bd9f756cbbe24394ffd6fc1f4b2", size = 162722, upload-time = "2025-07-14T03:29:26.863Z" },
]

[[package]]
name = "charset-normalizer"
version = "3.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e4/33/89c2ced2b67d1c2a61c19c6751aa8902d46ce3dacb23600a283619f5a12d/charset_normalizer-3.4.2.tar.gz", hash = "sha256:5baececa9ecba31eff645232d59845c07aa030f0c81ee70184a90d35099a0e63", size = 126367, upload-time = "2025-05-02T08:34:42.01Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/12/a93df3366ed32db1d907d7593a94f1fe6293903e3e92967bebd6950ed12c/charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:926ca93accd5d36ccdabd803392ddc3e03e6d4cd1cf17deff3b989ab8e9dbcf0", size = 199622, upload-time = "2025-05-02T08:32:56.363Z" },
    { url = "https://files.pythonhosted.org/packages/04/93/bf204e6f344c39d9937d3c13c8cd5bbfc266472e51fc8c07cb7f64fcd2de/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eba9904b0f38a143592d9fc0e19e2df0fa2e41c3c3745554761c5f6447eedabf", size = 143435, upload-time = "2025-05-02T08:32:58.551Z" },
    { url = "https://files.pythonhosted.org/packages/22/2a/ea8a2095b0bafa6c5b5a55ffdc2f924455233ee7b91c69b7edfcc9e02284/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3fddb7e2c84ac87ac3a947cb4e66d143ca5863ef48e4a5ecb83bd48619e4634e", size = 153653, upload-time = "2025-05-02T08:33:00.342Z" },
    { url = "https://files.pythonhosted.org/packages/b6/57/1b090ff183d13cef485dfbe272e2fe57622a76694061353c59da52c9a659/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98f862da73774290f251b9df8d11161b6cf25b599a66baf087c1ffe340e9bfd1", size = 146231, upload-time = "2025-05-02T08:33:02.081Z" },
    { url = "https://files.pythonhosted.org/packages/e2/28/ffc026b26f441fc67bd21ab7f03b313ab3fe46714a14b516f931abe1a2d8/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6c9379d65defcab82d07b2a9dfbfc2e95bc8fe0ebb1b176a3190230a3ef0e07c", size = 148243, upload-time = "2025-05-02T08:33:04.063Z" },
    { url = "https://files.pythonhosted.org/packages/c0/0f/9abe9bd191629c33e69e47c6ef45ef99773320e9ad8e9cb08b8ab4a8d4cb/charset_normalizer-3.4.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e635b87f01ebc977342e2697d05b56632f5f879a4f15955dfe8cef2448b51691", size = 150442, upload-time = "2025-05-02T08:33:06.418Z" },
    { url = "https://files.pythonhosted.org/packages/67/7c/a123bbcedca91d5916c056407f89a7f5e8fdfce12ba825d7d6b9954a1a3c/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1c95a1e2902a8b722868587c0e1184ad5c55631de5afc0eb96bc4b0d738092c0", size = 145147, upload-time = "2025-05-02T08:33:08.183Z" },
    { url = "https://files.pythonhosted.org/packages/ec/fe/1ac556fa4899d967b83e9893788e86b6af4d83e4726511eaaad035e36595/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:ef8de666d6179b009dce7bcb2ad4c4a779f113f12caf8dc77f0162c29d20490b", size = 153057, upload-time = "2025-05-02T08:33:09.986Z" },
    { url = "https://files.pythonhosted.org/packages/2b/ff/acfc0b0a70b19e3e54febdd5301a98b72fa07635e56f24f60502e954c461/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:32fc0341d72e0f73f80acb0a2c94216bd704f4f0bce10aedea38f30502b271ff", size = 156454, upload-time = "2025-05-02T08:33:11.814Z" },
    { url = "https://files.pythonhosted.org/packages/92/08/95b458ce9c740d0645feb0e96cea1f5ec946ea9c580a94adfe0b617f3573/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:289200a18fa698949d2b39c671c2cc7a24d44096784e76614899a7ccf2574b7b", size = 154174, upload-time = "2025-05-02T08:33:13.707Z" },
    { url = "https://files.pythonhosted.org/packages/78/be/8392efc43487ac051eee6c36d5fbd63032d78f7728cb37aebcc98191f1ff/charset_normalizer-3.4.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4a476b06fbcf359ad25d34a057b7219281286ae2477cc5ff5e3f70a246971148", size = 149166, upload-time = "2025-05-02T08:33:15.458Z" },
    { url = "https://files.pythonhosted.org/packages/44/96/392abd49b094d30b91d9fbda6a69519e95802250b777841cf3bda8fe136c/charset_normalizer-3.4.2-cp313-cp313-win32.whl", hash = "sha256:aaeeb6a479c7667fbe1099af9617c83aaca22182d6cf8c53966491a0f1b7ffb7", size = 98064, upload-time = "2025-05-02T08:33:17.06Z" },
    { url = "https://files.pythonhosted.org/packages/e9/b0/0200da600134e001d91851ddc797809e2fe0ea72de90e09bec5a2fbdaccb/charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl", hash = "sha256:aa6af9e7d59f9c12b33ae4e9450619cf2488e2bbe9b44030905877f0b2324980", size = 105641, upload-time = "2025-05-02T08:33:18.753Z" },
    { url = "https://files.pythonhosted.org/packages/20/94/c5790835a017658cbfabd07f3bfb549140c3ac458cfc196323996b10095a/charset_normalizer-3.4.2-py3-none-any.whl", hash = "sha256:7f56930ab0abd1c45cd15be65cc741c28b1c9a34876ce8c17a2fa107810c0af0", size = 52626, upload-time = "2025-05-02T08:34:40.053Z" },
]

[[package]]
name = "click"
version = "8.2.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/60/6c/8ca2efa64cf75a977a0d7fac081354553ebe483345c734fb6b6515d96bbc/click-8.2.1.tar.gz", hash = "sha256:27c491cc05d968d271d5a1db13e3b5a184636d9d930f148c50b038f0d0646202", size = 286342, upload-time = "2025-05-20T23:19:49.832Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/85/32/10bb5764d90a8eee674e9dc6f4db6a0ab47c8c4d0d83c27f7c39ac415a4d/click-8.2.1-py3-none-any.whl", hash = "sha256:61a3265b914e850b85317d0b3109c7f8cd35a670f963866005d6ef1d5175a12b", size = 102215, upload-time = "2025-05-20T23:19:47.796Z" },
]

[[package]]
name = "click-didyoumean"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
]
sdist = { url = "https://files.pythonhosted.org/packages/30/ce/217289b77c590ea1e7c24242d9ddd6e249e52c795ff10fac2c50062c48cb/click_didyoumean-0.3.1.tar.gz", hash = "sha256:4f82fdff0dbe64ef8ab2279bd6aa3f6a99c3b28c05aa09cbfc07c9d7fbb5a463", size = 3089, upload-time = "2024-03-24T08:22:07.499Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1b/5b/974430b5ffdb7a4f1941d13d83c64a0395114503cc357c6b9ae4ce5047ed/click_didyoumean-0.3.1-py3-none-any.whl", hash = "sha256:5c4bb6007cfea5f2fd6583a2fb6701a22a41eb98957e63d0fac41c10e7c3117c", size = 3631, upload-time = "2024-03-24T08:22:06.356Z" },
]

[[package]]
name = "click-plugins"
version = "1.1.1.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c3/a4/34847b59150da33690a36da3681d6bbc2ec14ee9a846bc30a6746e5984e4/click_plugins-1.1.1.2.tar.gz", hash = "sha256:d7af3984a99d243c131aa1a828331e7630f4a88a9741fd05c927b204bcf92261", size = 8343, upload-time = "2025-06-25T00:47:37.555Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3d/9a/2abecb28ae875e39c8cad711eb1186d8d14eab564705325e77e4e6ab9ae5/click_plugins-1.1.1.2-py2.py3-none-any.whl", hash = "sha256:008d65743833ffc1f5417bf0e78e8d2c23aab04d9745ba817bd3e71b0feb6aa6", size = 11051, upload-time = "2025-06-25T00:47:36.731Z" },
]

[[package]]
name = "click-repl"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "prompt-toolkit" },
]
sdist = { url = "https://files.pythonhosted.org/packages/cb/a2/57f4ac79838cfae6912f997b4d1a64a858fb0c86d7fcaae6f7b58d267fca/click-repl-0.3.0.tar.gz", hash = "sha256:17849c23dba3d667247dc4defe1757fff98694e90fe37474f3feebb69ced26a9", size = 10449, upload-time = "2023-06-15T12:43:51.141Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/52/40/9d857001228658f0d59e97ebd4c346fe73e138c6de1bce61dc568a57c7f8/click_repl-0.3.0-py3-none-any.whl", hash = "sha256:fb7e06deb8da8de86180a33a9da97ac316751c094c6899382da7feeeeb51b812", size = 10289, upload-time = "2023-06-15T12:43:48.626Z" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697, upload-time = "2022-10-25T02:36:22.414Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335, upload-time = "2022-10-25T02:36:20.889Z" },
]

[[package]]
name = "crawler-jobs"
version = "0.1.0"
source = { virtual = "." }
dependencies = [
    { name = "beautifulsoup4" },
    { name = "celery" },
    { name = "gitingest" },
    { name = "lxml" },
    { name = "pandas" },
    { name = "pydantic" },
    { name = "pymysql" },
    { name = "pytest" },
    { name = "python-dotenv" },
    { name = "requests" },
    { name = "ruff" },
    { name = "sqlalchemy" },
    { name = "structlog" },
    { name = "tenacity" },
]

[package.metadata]
requires-dist = [
    { name = "beautifulsoup4" },
    { name = "celery", specifier = ">=5.5.3" },
    { name = "gitingest", specifier = ">=0.1.5" },
    { name = "lxml" },
    { name = "pandas", specifier = ">=2.3.1" },
    { name = "pydantic", specifier = ">=2.0.0" },
    { name = "pymysql", specifier = "==1.1.0" },
    { name = "pytest", specifier = ">=8.4.1" },
    { name = "python-dotenv" },
    { name = "requests", specifier = ">=2.32.4" },
    { name = "ruff", specifier = ">=0.12.5" },
    { name = "sqlalchemy", specifier = ">=2.0.41" },
    { name = "structlog", specifier = ">=25.4.0" },
    { name = "tenacity", specifier = ">=8.2.3" },
]

[[package]]
name = "gitingest"
version = "0.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "httpx" },
    { name = "loguru" },
    { name = "pathspec" },
    { name = "pydantic" },
    { name = "python-dotenv" },
    { name = "starlette" },
    { name = "tiktoken" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d6/fe/a915f0c32a3d7920206a677f73c185b3eadf4ec151fb05aedd52e64713f7/gitingest-0.3.1.tar.gz", hash = "sha256:4587cab873d4e08bdb16d612bb153c23e0ce59771a1d57a438239c5e39f05ebf", size = 70681, upload-time = "2025-07-31T13:56:19.845Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/15/f200ab2e73287e67d1dce6fbacf421552ae9fbafdc5f0cc8dd0d2fe4fc47/gitingest-0.3.1-py3-none-any.whl", hash = "sha256:8143a5e6a7140ede9f680e13d3931ac07c82ac9bd8bab9ad1fba017c8c1e8666", size = 68343, upload-time = "2025-07-31T13:56:17.729Z" },
]

[[package]]
name = "greenlet"
version = "3.2.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c9/92/bb85bd6e80148a4d2e0c59f7c0c2891029f8fd510183afc7d8d2feeed9b6/greenlet-3.2.3.tar.gz", hash = "sha256:8b0dd8ae4c0d6f5e54ee55ba935eeb3d735a9b58a8a1e5b5cbab64e01a39f365", size = 185752, upload-time = "2025-06-05T16:16:09.955Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b1/cf/f5c0b23309070ae93de75c90d29300751a5aacefc0a3ed1b1d8edb28f08b/greenlet-3.2.3-cp313-cp313-macosx_11_0_universal2.whl", hash = "sha256:500b8689aa9dd1ab26872a34084503aeddefcb438e2e7317b89b11eaea1901ad", size = 270732, upload-time = "2025-06-05T16:10:08.26Z" },
    { url = "https://files.pythonhosted.org/packages/48/ae/91a957ba60482d3fecf9be49bc3948f341d706b52ddb9d83a70d42abd498/greenlet-3.2.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:a07d3472c2a93117af3b0136f246b2833fdc0b542d4a9799ae5f41c28323faef", size = 639033, upload-time = "2025-06-05T16:38:53.983Z" },
    { url = "https://files.pythonhosted.org/packages/6f/df/20ffa66dd5a7a7beffa6451bdb7400d66251374ab40b99981478c69a67a8/greenlet-3.2.3-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:8704b3768d2f51150626962f4b9a9e4a17d2e37c8a8d9867bbd9fa4eb938d3b3", size = 652999, upload-time = "2025-06-05T16:41:37.89Z" },
    { url = "https://files.pythonhosted.org/packages/51/b4/ebb2c8cb41e521f1d72bf0465f2f9a2fd803f674a88db228887e6847077e/greenlet-3.2.3-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:5035d77a27b7c62db6cf41cf786cfe2242644a7a337a0e155c80960598baab95", size = 647368, upload-time = "2025-06-05T16:48:21.467Z" },
    { url = "https://files.pythonhosted.org/packages/8e/6a/1e1b5aa10dced4ae876a322155705257748108b7fd2e4fae3f2a091fe81a/greenlet-3.2.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:2d8aa5423cd4a396792f6d4580f88bdc6efcb9205891c9d40d20f6e670992efb", size = 650037, upload-time = "2025-06-05T16:13:06.402Z" },
    { url = "https://files.pythonhosted.org/packages/26/f2/ad51331a157c7015c675702e2d5230c243695c788f8f75feba1af32b3617/greenlet-3.2.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:2c724620a101f8170065d7dded3f962a2aea7a7dae133a009cada42847e04a7b", size = 608402, upload-time = "2025-06-05T16:12:51.91Z" },
    { url = "https://files.pythonhosted.org/packages/26/bc/862bd2083e6b3aff23300900a956f4ea9a4059de337f5c8734346b9b34fc/greenlet-3.2.3-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:873abe55f134c48e1f2a6f53f7d1419192a3d1a4e873bace00499a4e45ea6af0", size = 1119577, upload-time = "2025-06-05T16:36:49.787Z" },
    { url = "https://files.pythonhosted.org/packages/86/94/1fc0cc068cfde885170e01de40a619b00eaa8f2916bf3541744730ffb4c3/greenlet-3.2.3-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:024571bbce5f2c1cfff08bf3fbaa43bbc7444f580ae13b0099e95d0e6e67ed36", size = 1147121, upload-time = "2025-06-05T16:12:42.527Z" },
    { url = "https://files.pythonhosted.org/packages/27/1a/199f9587e8cb08a0658f9c30f3799244307614148ffe8b1e3aa22f324dea/greenlet-3.2.3-cp313-cp313-win_amd64.whl", hash = "sha256:5195fb1e75e592dd04ce79881c8a22becdfa3e6f500e7feb059b1e6fdd54d3e3", size = 297603, upload-time = "2025-06-05T16:20:12.651Z" },
    { url = "https://files.pythonhosted.org/packages/d8/ca/accd7aa5280eb92b70ed9e8f7fd79dc50a2c21d8c73b9a0856f5b564e222/greenlet-3.2.3-cp314-cp314-macosx_11_0_universal2.whl", hash = "sha256:3d04332dddb10b4a211b68111dabaee2e1a073663d117dc10247b5b1642bac86", size = 271479, upload-time = "2025-06-05T16:10:47.525Z" },
    { url = "https://files.pythonhosted.org/packages/55/71/01ed9895d9eb49223280ecc98a557585edfa56b3d0e965b9fa9f7f06b6d9/greenlet-3.2.3-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:8186162dffde068a465deab08fc72c767196895c39db26ab1c17c0b77a6d8b97", size = 683952, upload-time = "2025-06-05T16:38:55.125Z" },
    { url = "https://files.pythonhosted.org/packages/ea/61/638c4bdf460c3c678a0a1ef4c200f347dff80719597e53b5edb2fb27ab54/greenlet-3.2.3-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.whl", hash = "sha256:f4bfbaa6096b1b7a200024784217defedf46a07c2eee1a498e94a1b5f8ec5728", size = 696917, upload-time = "2025-06-05T16:41:38.959Z" },
    { url = "https://files.pythonhosted.org/packages/22/cc/0bd1a7eb759d1f3e3cc2d1bc0f0b487ad3cc9f34d74da4b80f226fde4ec3/greenlet-3.2.3-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:ed6cfa9200484d234d8394c70f5492f144b20d4533f69262d530a1a082f6ee9a", size = 692443, upload-time = "2025-06-05T16:48:23.113Z" },
    { url = "https://files.pythonhosted.org/packages/67/10/b2a4b63d3f08362662e89c103f7fe28894a51ae0bc890fabf37d1d780e52/greenlet-3.2.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:02b0df6f63cd15012bed5401b47829cfd2e97052dc89da3cfaf2c779124eb892", size = 692995, upload-time = "2025-06-05T16:13:07.972Z" },
    { url = "https://files.pythonhosted.org/packages/5a/c6/ad82f148a4e3ce9564056453a71529732baf5448ad53fc323e37efe34f66/greenlet-3.2.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:86c2d68e87107c1792e2e8d5399acec2487a4e993ab76c792408e59394d52141", size = 655320, upload-time = "2025-06-05T16:12:53.453Z" },
    { url = "https://files.pythonhosted.org/packages/5c/4f/aab73ecaa6b3086a4c89863d94cf26fa84cbff63f52ce9bc4342b3087a06/greenlet-3.2.3-cp314-cp314-win_amd64.whl", hash = "sha256:8c47aae8fbbfcf82cc13327ae802ba13c9c36753b67e760023fd116bc124a62a", size = 301236, upload-time = "2025-06-05T16:15:20.111Z" },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250, upload-time = "2025-04-24T03:35:25.427Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515, upload-time = "2025-04-24T03:35:24.344Z" },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484, upload-time = "2025-04-24T22:06:22.219Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784, upload-time = "2025-04-24T22:06:20.566Z" },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406, upload-time = "2024-12-06T15:37:23.222Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517, upload-time = "2024-12-06T15:37:21.509Z" },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490, upload-time = "2024-09-15T18:07:39.745Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442, upload-time = "2024-09-15T18:07:37.964Z" },
]

[[package]]
name = "iniconfig"
version = "2.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f2/97/ebf4da567aa6827c909642694d71c9fcf53e5b504f2d96afea02718862f3/iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7", size = 4793, upload-time = "2025-03-19T20:09:59.721Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2c/e1/e6716421ea10d38022b952c159d5161ca1193197fb744506875fbb87ea7b/iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760", size = 6050, upload-time = "2025-03-19T20:10:01.071Z" },
]

[[package]]
name = "kombu"
version = "5.5.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "amqp" },
    { name = "packaging" },
    { name = "tzdata" },
    { name = "vine" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0f/d3/5ff936d8319ac86b9c409f1501b07c426e6ad41966fedace9ef1b966e23f/kombu-5.5.4.tar.gz", hash = "sha256:886600168275ebeada93b888e831352fe578168342f0d1d5833d88ba0d847363", size = 461992, upload-time = "2025-06-01T10:19:22.281Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/70/a07dcf4f62598c8ad579df241af55ced65bed76e42e45d3c368a6d82dbc1/kombu-5.5.4-py3-none-any.whl", hash = "sha256:a12ed0557c238897d8e518f1d1fdf84bd1516c5e305af2dacd85c2015115feb8", size = 210034, upload-time = "2025-06-01T10:19:20.436Z" },
]

[[package]]
name = "loguru"
version = "0.7.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "win32-setctime", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3a/05/a1dae3dffd1116099471c643b8924f5aa6524411dc6c63fdae648c4f1aca/loguru-0.7.3.tar.gz", hash = "sha256:19480589e77d47b8d85b2c827ad95d49bf31b0dcde16593892eb51dd18706eb6", size = 63559, upload-time = "2024-12-06T11:20:56.608Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0c/29/0348de65b8cc732daa3e33e67806420b2ae89bdce2b04af740289c5c6c8c/loguru-0.7.3-py3-none-any.whl", hash = "sha256:31a33c10c8e1e10422bfd431aeb5d351c7cf7fa671e3c4df004162264b28220c", size = 61595, upload-time = "2024-12-06T11:20:54.538Z" },
]

[[package]]
name = "lxml"
version = "6.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c5/ed/60eb6fa2923602fba988d9ca7c5cdbd7cf25faa795162ed538b527a35411/lxml-6.0.0.tar.gz", hash = "sha256:032e65120339d44cdc3efc326c9f660f5f7205f3a535c1fdbf898b29ea01fb72", size = 4096938, upload-time = "2025-06-26T16:28:19.373Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/21/6e7c060822a3c954ff085e5e1b94b4a25757c06529eac91e550f3f5cd8b8/lxml-6.0.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:6da7cd4f405fd7db56e51e96bff0865b9853ae70df0e6720624049da76bde2da", size = 8414372, upload-time = "2025-06-26T16:26:39.079Z" },
    { url = "https://files.pythonhosted.org/packages/a4/f6/051b1607a459db670fc3a244fa4f06f101a8adf86cda263d1a56b3a4f9d5/lxml-6.0.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b34339898bb556a2351a1830f88f751679f343eabf9cf05841c95b165152c9e7", size = 4593940, upload-time = "2025-06-26T16:26:41.891Z" },
    { url = "https://files.pythonhosted.org/packages/8e/74/dd595d92a40bda3c687d70d4487b2c7eff93fd63b568acd64fedd2ba00fe/lxml-6.0.0-cp313-cp313-manylinux2010_i686.manylinux2014_i686.manylinux_2_12_i686.manylinux_2_17_i686.whl", hash = "sha256:51a5e4c61a4541bd1cd3ba74766d0c9b6c12d6a1a4964ef60026832aac8e79b3", size = 5214329, upload-time = "2025-06-26T16:26:44.669Z" },
    { url = "https://files.pythonhosted.org/packages/52/46/3572761efc1bd45fcafb44a63b3b0feeb5b3f0066886821e94b0254f9253/lxml-6.0.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:d18a25b19ca7307045581b18b3ec9ead2b1db5ccd8719c291f0cd0a5cec6cb81", size = 4947559, upload-time = "2025-06-28T18:47:31.091Z" },
    { url = "https://files.pythonhosted.org/packages/94/8a/5e40de920e67c4f2eef9151097deb9b52d86c95762d8ee238134aff2125d/lxml-6.0.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:d4f0c66df4386b75d2ab1e20a489f30dc7fd9a06a896d64980541506086be1f1", size = 5102143, upload-time = "2025-06-28T18:47:33.612Z" },
    { url = "https://files.pythonhosted.org/packages/7c/4b/20555bdd75d57945bdabfbc45fdb1a36a1a0ff9eae4653e951b2b79c9209/lxml-6.0.0-cp313-cp313-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:9f4b481b6cc3a897adb4279216695150bbe7a44c03daba3c894f49d2037e0a24", size = 5021931, upload-time = "2025-06-26T16:26:47.503Z" },
    { url = "https://files.pythonhosted.org/packages/b6/6e/cf03b412f3763d4ca23b25e70c96a74cfece64cec3addf1c4ec639586b13/lxml-6.0.0-cp313-cp313-manylinux_2_27_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:8a78d6c9168f5bcb20971bf3329c2b83078611fbe1f807baadc64afc70523b3a", size = 5645469, upload-time = "2025-07-03T19:19:13.32Z" },
    { url = "https://files.pythonhosted.org/packages/d4/dd/39c8507c16db6031f8c1ddf70ed95dbb0a6d466a40002a3522c128aba472/lxml-6.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:2ae06fbab4f1bb7db4f7c8ca9897dc8db4447d1a2b9bee78474ad403437bcc29", size = 5247467, upload-time = "2025-06-26T16:26:49.998Z" },
    { url = "https://files.pythonhosted.org/packages/4d/56/732d49def0631ad633844cfb2664563c830173a98d5efd9b172e89a4800d/lxml-6.0.0-cp313-cp313-manylinux_2_31_armv7l.whl", hash = "sha256:1fa377b827ca2023244a06554c6e7dc6828a10aaf74ca41965c5d8a4925aebb4", size = 4720601, upload-time = "2025-06-26T16:26:52.564Z" },
    { url = "https://files.pythonhosted.org/packages/8f/7f/6b956fab95fa73462bca25d1ea7fc8274ddf68fb8e60b78d56c03b65278e/lxml-6.0.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:1676b56d48048a62ef77a250428d1f31f610763636e0784ba67a9740823988ca", size = 5060227, upload-time = "2025-06-26T16:26:55.054Z" },
    { url = "https://files.pythonhosted.org/packages/97/06/e851ac2924447e8b15a294855caf3d543424364a143c001014d22c8ca94c/lxml-6.0.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:0e32698462aacc5c1cf6bdfebc9c781821b7e74c79f13e5ffc8bfe27c42b1abf", size = 4790637, upload-time = "2025-06-26T16:26:57.384Z" },
    { url = "https://files.pythonhosted.org/packages/06/d4/fd216f3cd6625022c25b336c7570d11f4a43adbaf0a56106d3d496f727a7/lxml-6.0.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:4d6036c3a296707357efb375cfc24bb64cd955b9ec731abf11ebb1e40063949f", size = 5662049, upload-time = "2025-07-03T19:19:16.409Z" },
    { url = "https://files.pythonhosted.org/packages/52/03/0e764ce00b95e008d76b99d432f1807f3574fb2945b496a17807a1645dbd/lxml-6.0.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:7488a43033c958637b1a08cddc9188eb06d3ad36582cebc7d4815980b47e27ef", size = 5272430, upload-time = "2025-06-26T16:27:00.031Z" },
    { url = "https://files.pythonhosted.org/packages/5f/01/d48cc141bc47bc1644d20fe97bbd5e8afb30415ec94f146f2f76d0d9d098/lxml-6.0.0-cp313-cp313-win32.whl", hash = "sha256:5fcd7d3b1d8ecb91445bd71b9c88bdbeae528fefee4f379895becfc72298d181", size = 3612896, upload-time = "2025-06-26T16:27:04.251Z" },
    { url = "https://files.pythonhosted.org/packages/f4/87/6456b9541d186ee7d4cb53bf1b9a0d7f3b1068532676940fdd594ac90865/lxml-6.0.0-cp313-cp313-win_amd64.whl", hash = "sha256:2f34687222b78fff795feeb799a7d44eca2477c3d9d3a46ce17d51a4f383e32e", size = 4013132, upload-time = "2025-06-26T16:27:06.415Z" },
    { url = "https://files.pythonhosted.org/packages/b7/42/85b3aa8f06ca0d24962f8100f001828e1f1f1a38c954c16e71154ed7d53a/lxml-6.0.0-cp313-cp313-win_arm64.whl", hash = "sha256:21db1ec5525780fd07251636eb5f7acb84003e9382c72c18c542a87c416ade03", size = 3672642, upload-time = "2025-06-26T16:27:09.888Z" },
]

[[package]]
name = "numpy"
version = "2.3.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/37/7d/3fec4199c5ffb892bed55cff901e4f39a58c81df9c44c280499e92cad264/numpy-2.3.2.tar.gz", hash = "sha256:e0486a11ec30cdecb53f184d496d1c6a20786c81e55e41640270130056f8ee48", size = 20489306, upload-time = "2025-07-24T21:32:07.553Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1c/c0/c6bb172c916b00700ed3bf71cb56175fd1f7dbecebf8353545d0b5519f6c/numpy-2.3.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:c8d9727f5316a256425892b043736d63e89ed15bbfe6556c5ff4d9d4448ff3b3", size = 20949074, upload-time = "2025-07-24T20:43:07.813Z" },
    { url = "https://files.pythonhosted.org/packages/20/4e/c116466d22acaf4573e58421c956c6076dc526e24a6be0903219775d862e/numpy-2.3.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:efc81393f25f14d11c9d161e46e6ee348637c0a1e8a54bf9dedc472a3fae993b", size = 14177311, upload-time = "2025-07-24T20:43:29.335Z" },
    { url = "https://files.pythonhosted.org/packages/78/45/d4698c182895af189c463fc91d70805d455a227261d950e4e0f1310c2550/numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:dd937f088a2df683cbb79dda9a772b62a3e5a8a7e76690612c2737f38c6ef1b6", size = 5106022, upload-time = "2025-07-24T20:43:37.999Z" },
    { url = "https://files.pythonhosted.org/packages/9f/76/3e6880fef4420179309dba72a8c11f6166c431cf6dee54c577af8906f914/numpy-2.3.2-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:11e58218c0c46c80509186e460d79fbdc9ca1eb8d8aee39d8f2dc768eb781089", size = 6640135, upload-time = "2025-07-24T20:43:49.28Z" },
    { url = "https://files.pythonhosted.org/packages/34/fa/87ff7f25b3c4ce9085a62554460b7db686fef1e0207e8977795c7b7d7ba1/numpy-2.3.2-cp313-cp313-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:5ad4ebcb683a1f99f4f392cc522ee20a18b2bb12a2c1c42c3d48d5a1adc9d3d2", size = 14278147, upload-time = "2025-07-24T20:44:10.328Z" },
    { url = "https://files.pythonhosted.org/packages/1d/0f/571b2c7a3833ae419fe69ff7b479a78d313581785203cc70a8db90121b9a/numpy-2.3.2-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:938065908d1d869c7d75d8ec45f735a034771c6ea07088867f713d1cd3bbbe4f", size = 16635989, upload-time = "2025-07-24T20:44:34.88Z" },
    { url = "https://files.pythonhosted.org/packages/24/5a/84ae8dca9c9a4c592fe11340b36a86ffa9fd3e40513198daf8a97839345c/numpy-2.3.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:66459dccc65d8ec98cc7df61307b64bf9e08101f9598755d42d8ae65d9a7a6ee", size = 16053052, upload-time = "2025-07-24T20:44:58.872Z" },
    { url = "https://files.pythonhosted.org/packages/57/7c/e5725d99a9133b9813fcf148d3f858df98511686e853169dbaf63aec6097/numpy-2.3.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:a7af9ed2aa9ec5950daf05bb11abc4076a108bd3c7db9aa7251d5f107079b6a6", size = 18577955, upload-time = "2025-07-24T20:45:26.714Z" },
    { url = "https://files.pythonhosted.org/packages/ae/11/7c546fcf42145f29b71e4d6f429e96d8d68e5a7ba1830b2e68d7418f0bbd/numpy-2.3.2-cp313-cp313-win32.whl", hash = "sha256:906a30249315f9c8e17b085cc5f87d3f369b35fedd0051d4a84686967bdbbd0b", size = 6311843, upload-time = "2025-07-24T20:49:24.444Z" },
    { url = "https://files.pythonhosted.org/packages/aa/6f/a428fd1cb7ed39b4280d057720fed5121b0d7754fd2a9768640160f5517b/numpy-2.3.2-cp313-cp313-win_amd64.whl", hash = "sha256:c63d95dc9d67b676e9108fe0d2182987ccb0f11933c1e8959f42fa0da8d4fa56", size = 12782876, upload-time = "2025-07-24T20:49:43.227Z" },
    { url = "https://files.pythonhosted.org/packages/65/85/4ea455c9040a12595fb6c43f2c217257c7b52dd0ba332c6a6c1d28b289fe/numpy-2.3.2-cp313-cp313-win_arm64.whl", hash = "sha256:b05a89f2fb84d21235f93de47129dd4f11c16f64c87c33f5e284e6a3a54e43f2", size = 10192786, upload-time = "2025-07-24T20:49:59.443Z" },
    { url = "https://files.pythonhosted.org/packages/80/23/8278f40282d10c3f258ec3ff1b103d4994bcad78b0cba9208317f6bb73da/numpy-2.3.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:4e6ecfeddfa83b02318f4d84acf15fbdbf9ded18e46989a15a8b6995dfbf85ab", size = 21047395, upload-time = "2025-07-24T20:45:58.821Z" },
    { url = "https://files.pythonhosted.org/packages/1f/2d/624f2ce4a5df52628b4ccd16a4f9437b37c35f4f8a50d00e962aae6efd7a/numpy-2.3.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:508b0eada3eded10a3b55725b40806a4b855961040180028f52580c4729916a2", size = 14300374, upload-time = "2025-07-24T20:46:20.207Z" },
    { url = "https://files.pythonhosted.org/packages/f6/62/ff1e512cdbb829b80a6bd08318a58698867bca0ca2499d101b4af063ee97/numpy-2.3.2-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:754d6755d9a7588bdc6ac47dc4ee97867271b17cee39cb87aef079574366db0a", size = 5228864, upload-time = "2025-07-24T20:46:30.58Z" },
    { url = "https://files.pythonhosted.org/packages/7d/8e/74bc18078fff03192d4032cfa99d5a5ca937807136d6f5790ce07ca53515/numpy-2.3.2-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:a9f66e7d2b2d7712410d3bc5684149040ef5f19856f20277cd17ea83e5006286", size = 6737533, upload-time = "2025-07-24T20:46:46.111Z" },
    { url = "https://files.pythonhosted.org/packages/19/ea/0731efe2c9073ccca5698ef6a8c3667c4cf4eea53fcdcd0b50140aba03bc/numpy-2.3.2-cp313-cp313t-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:de6ea4e5a65d5a90c7d286ddff2b87f3f4ad61faa3db8dabe936b34c2275b6f8", size = 14352007, upload-time = "2025-07-24T20:47:07.1Z" },
    { url = "https://files.pythonhosted.org/packages/cf/90/36be0865f16dfed20f4bc7f75235b963d5939707d4b591f086777412ff7b/numpy-2.3.2-cp313-cp313t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a3ef07ec8cbc8fc9e369c8dcd52019510c12da4de81367d8b20bc692aa07573a", size = 16701914, upload-time = "2025-07-24T20:47:32.459Z" },
    { url = "https://files.pythonhosted.org/packages/94/30/06cd055e24cb6c38e5989a9e747042b4e723535758e6153f11afea88c01b/numpy-2.3.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:27c9f90e7481275c7800dc9c24b7cc40ace3fdb970ae4d21eaff983a32f70c91", size = 16132708, upload-time = "2025-07-24T20:47:58.129Z" },
    { url = "https://files.pythonhosted.org/packages/9a/14/ecede608ea73e58267fd7cb78f42341b3b37ba576e778a1a06baffbe585c/numpy-2.3.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:07b62978075b67eee4065b166d000d457c82a1efe726cce608b9db9dd66a73a5", size = 18651678, upload-time = "2025-07-24T20:48:25.402Z" },
    { url = "https://files.pythonhosted.org/packages/40/f3/2fe6066b8d07c3685509bc24d56386534c008b462a488b7f503ba82b8923/numpy-2.3.2-cp313-cp313t-win32.whl", hash = "sha256:c771cfac34a4f2c0de8e8c97312d07d64fd8f8ed45bc9f5726a7e947270152b5", size = 6441832, upload-time = "2025-07-24T20:48:37.181Z" },
    { url = "https://files.pythonhosted.org/packages/0b/ba/0937d66d05204d8f28630c9c60bc3eda68824abde4cf756c4d6aad03b0c6/numpy-2.3.2-cp313-cp313t-win_amd64.whl", hash = "sha256:72dbebb2dcc8305c431b2836bcc66af967df91be793d63a24e3d9b741374c450", size = 12927049, upload-time = "2025-07-24T20:48:56.24Z" },
    { url = "https://files.pythonhosted.org/packages/e9/ed/13542dd59c104d5e654dfa2ac282c199ba64846a74c2c4bcdbc3a0f75df1/numpy-2.3.2-cp313-cp313t-win_arm64.whl", hash = "sha256:72c6df2267e926a6d5286b0a6d556ebe49eae261062059317837fda12ddf0c1a", size = 10262935, upload-time = "2025-07-24T20:49:13.136Z" },
    { url = "https://files.pythonhosted.org/packages/c9/7c/7659048aaf498f7611b783e000c7268fcc4dcf0ce21cd10aad7b2e8f9591/numpy-2.3.2-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:448a66d052d0cf14ce9865d159bfc403282c9bc7bb2a31b03cc18b651eca8b1a", size = 20950906, upload-time = "2025-07-24T20:50:30.346Z" },
    { url = "https://files.pythonhosted.org/packages/80/db/984bea9d4ddf7112a04cfdfb22b1050af5757864cfffe8e09e44b7f11a10/numpy-2.3.2-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:546aaf78e81b4081b2eba1d105c3b34064783027a06b3ab20b6eba21fb64132b", size = 14185607, upload-time = "2025-07-24T20:50:51.923Z" },
    { url = "https://files.pythonhosted.org/packages/e4/76/b3d6f414f4eca568f469ac112a3b510938d892bc5a6c190cb883af080b77/numpy-2.3.2-cp314-cp314-macosx_14_0_arm64.whl", hash = "sha256:87c930d52f45df092f7578889711a0768094debf73cfcde105e2d66954358125", size = 5114110, upload-time = "2025-07-24T20:51:01.041Z" },
    { url = "https://files.pythonhosted.org/packages/9e/d2/6f5e6826abd6bca52392ed88fe44a4b52aacb60567ac3bc86c67834c3a56/numpy-2.3.2-cp314-cp314-macosx_14_0_x86_64.whl", hash = "sha256:8dc082ea901a62edb8f59713c6a7e28a85daddcb67454c839de57656478f5b19", size = 6642050, upload-time = "2025-07-24T20:51:11.64Z" },
    { url = "https://files.pythonhosted.org/packages/c4/43/f12b2ade99199e39c73ad182f103f9d9791f48d885c600c8e05927865baf/numpy-2.3.2-cp314-cp314-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:af58de8745f7fa9ca1c0c7c943616c6fe28e75d0c81f5c295810e3c83b5be92f", size = 14296292, upload-time = "2025-07-24T20:51:33.488Z" },
    { url = "https://files.pythonhosted.org/packages/5d/f9/77c07d94bf110a916b17210fac38680ed8734c236bfed9982fd8524a7b47/numpy-2.3.2-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:fed5527c4cf10f16c6d0b6bee1f89958bccb0ad2522c8cadc2efd318bcd545f5", size = 16638913, upload-time = "2025-07-24T20:51:58.517Z" },
    { url = "https://files.pythonhosted.org/packages/9b/d1/9d9f2c8ea399cc05cfff8a7437453bd4e7d894373a93cdc46361bbb49a7d/numpy-2.3.2-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:095737ed986e00393ec18ec0b21b47c22889ae4b0cd2d5e88342e08b01141f58", size = 16071180, upload-time = "2025-07-24T20:52:22.827Z" },
    { url = "https://files.pythonhosted.org/packages/4c/41/82e2c68aff2a0c9bf315e47d61951099fed65d8cb2c8d9dc388cb87e947e/numpy-2.3.2-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:b5e40e80299607f597e1a8a247ff8d71d79c5b52baa11cc1cce30aa92d2da6e0", size = 18576809, upload-time = "2025-07-24T20:52:51.015Z" },
    { url = "https://files.pythonhosted.org/packages/14/14/4b4fd3efb0837ed252d0f583c5c35a75121038a8c4e065f2c259be06d2d8/numpy-2.3.2-cp314-cp314-win32.whl", hash = "sha256:7d6e390423cc1f76e1b8108c9b6889d20a7a1f59d9a60cac4a050fa734d6c1e2", size = 6366410, upload-time = "2025-07-24T20:56:44.949Z" },
    { url = "https://files.pythonhosted.org/packages/11/9e/b4c24a6b8467b61aced5c8dc7dcfce23621baa2e17f661edb2444a418040/numpy-2.3.2-cp314-cp314-win_amd64.whl", hash = "sha256:b9d0878b21e3918d76d2209c924ebb272340da1fb51abc00f986c258cd5e957b", size = 12918821, upload-time = "2025-07-24T20:57:06.479Z" },
    { url = "https://files.pythonhosted.org/packages/0e/0f/0dc44007c70b1007c1cef86b06986a3812dd7106d8f946c09cfa75782556/numpy-2.3.2-cp314-cp314-win_arm64.whl", hash = "sha256:2738534837c6a1d0c39340a190177d7d66fdf432894f469728da901f8f6dc910", size = 10477303, upload-time = "2025-07-24T20:57:22.879Z" },
    { url = "https://files.pythonhosted.org/packages/8b/3e/075752b79140b78ddfc9c0a1634d234cfdbc6f9bbbfa6b7504e445ad7d19/numpy-2.3.2-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:4d002ecf7c9b53240be3bb69d80f86ddbd34078bae04d87be81c1f58466f264e", size = 21047524, upload-time = "2025-07-24T20:53:22.086Z" },
    { url = "https://files.pythonhosted.org/packages/fe/6d/60e8247564a72426570d0e0ea1151b95ce5bd2f1597bb878a18d32aec855/numpy-2.3.2-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:293b2192c6bcce487dbc6326de5853787f870aeb6c43f8f9c6496db5b1781e45", size = 14300519, upload-time = "2025-07-24T20:53:44.053Z" },
    { url = "https://files.pythonhosted.org/packages/4d/73/d8326c442cd428d47a067070c3ac6cc3b651a6e53613a1668342a12d4479/numpy-2.3.2-cp314-cp314t-macosx_14_0_arm64.whl", hash = "sha256:0a4f2021a6da53a0d580d6ef5db29947025ae8b35b3250141805ea9a32bbe86b", size = 5228972, upload-time = "2025-07-24T20:53:53.81Z" },
    { url = "https://files.pythonhosted.org/packages/34/2e/e71b2d6dad075271e7079db776196829019b90ce3ece5c69639e4f6fdc44/numpy-2.3.2-cp314-cp314t-macosx_14_0_x86_64.whl", hash = "sha256:9c144440db4bf3bb6372d2c3e49834cc0ff7bb4c24975ab33e01199e645416f2", size = 6737439, upload-time = "2025-07-24T20:54:04.742Z" },
    { url = "https://files.pythonhosted.org/packages/15/b0/d004bcd56c2c5e0500ffc65385eb6d569ffd3363cb5e593ae742749b2daa/numpy-2.3.2-cp314-cp314t-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:f92d6c2a8535dc4fe4419562294ff957f83a16ebdec66df0805e473ffaad8bd0", size = 14352479, upload-time = "2025-07-24T20:54:25.819Z" },
    { url = "https://files.pythonhosted.org/packages/11/e3/285142fcff8721e0c99b51686426165059874c150ea9ab898e12a492e291/numpy-2.3.2-cp314-cp314t-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:cefc2219baa48e468e3db7e706305fcd0c095534a192a08f31e98d83a7d45fb0", size = 16702805, upload-time = "2025-07-24T20:54:50.814Z" },
    { url = "https://files.pythonhosted.org/packages/33/c3/33b56b0e47e604af2c7cd065edca892d180f5899599b76830652875249a3/numpy-2.3.2-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:76c3e9501ceb50b2ff3824c3589d5d1ab4ac857b0ee3f8f49629d0de55ecf7c2", size = 16133830, upload-time = "2025-07-24T20:55:17.306Z" },
    { url = "https://files.pythonhosted.org/packages/6e/ae/7b1476a1f4d6a48bc669b8deb09939c56dd2a439db1ab03017844374fb67/numpy-2.3.2-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:122bf5ed9a0221b3419672493878ba4967121514b1d7d4656a7580cd11dddcbf", size = 18652665, upload-time = "2025-07-24T20:55:46.665Z" },
    { url = "https://files.pythonhosted.org/packages/14/ba/5b5c9978c4bb161034148ade2de9db44ec316fab89ce8c400db0e0c81f86/numpy-2.3.2-cp314-cp314t-win32.whl", hash = "sha256:6f1ae3dcb840edccc45af496f312528c15b1f79ac318169d094e85e4bb35fdf1", size = 6514777, upload-time = "2025-07-24T20:55:57.66Z" },
    { url = "https://files.pythonhosted.org/packages/eb/46/3dbaf0ae7c17cdc46b9f662c56da2054887b8d9e737c1476f335c83d33db/numpy-2.3.2-cp314-cp314t-win_amd64.whl", hash = "sha256:087ffc25890d89a43536f75c5fe8770922008758e8eeeef61733957041ed2f9b", size = 13111856, upload-time = "2025-07-24T20:56:17.318Z" },
    { url = "https://files.pythonhosted.org/packages/c1/9e/1652778bce745a67b5fe05adde60ed362d38eb17d919a540e813d30f6874/numpy-2.3.2-cp314-cp314t-win_arm64.whl", hash = "sha256:092aeb3449833ea9c0bf0089d70c29ae480685dd2377ec9cdbbb620257f84631", size = 10544226, upload-time = "2025-07-24T20:56:34.509Z" },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727, upload-time = "2025-04-19T11:48:59.673Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469, upload-time = "2025-04-19T11:48:57.875Z" },
]

[[package]]
name = "pandas"
version = "2.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "numpy" },
    { name = "python-dateutil" },
    { name = "pytz" },
    { name = "tzdata" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d1/6f/75aa71f8a14267117adeeed5d21b204770189c0a0025acbdc03c337b28fc/pandas-2.3.1.tar.gz", hash = "sha256:0a95b9ac964fe83ce317827f80304d37388ea77616b1425f0ae41c9d2d0d7bb2", size = 4487493, upload-time = "2025-07-07T19:20:04.079Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/32/ed/ff0a67a2c5505e1854e6715586ac6693dd860fbf52ef9f81edee200266e7/pandas-2.3.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:9026bd4a80108fac2239294a15ef9003c4ee191a0f64b90f170b40cfb7cf2d22", size = 11531393, upload-time = "2025-07-07T19:19:12.245Z" },
    { url = "https://files.pythonhosted.org/packages/c7/db/d8f24a7cc9fb0972adab0cc80b6817e8bef888cfd0024eeb5a21c0bb5c4a/pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:6de8547d4fdb12421e2d047a2c446c623ff4c11f47fddb6b9169eb98ffba485a", size = 10668750, upload-time = "2025-07-07T19:19:14.612Z" },
    { url = "https://files.pythonhosted.org/packages/0f/b0/80f6ec783313f1e2356b28b4fd8d2148c378370045da918c73145e6aab50/pandas-2.3.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:782647ddc63c83133b2506912cc6b108140a38a37292102aaa19c81c83db2928", size = 11342004, upload-time = "2025-07-07T19:19:16.857Z" },
    { url = "https://files.pythonhosted.org/packages/e9/e2/20a317688435470872885e7fc8f95109ae9683dec7c50be29b56911515a5/pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2ba6aff74075311fc88504b1db890187a3cd0f887a5b10f5525f8e2ef55bfdb9", size = 12050869, upload-time = "2025-07-07T19:19:19.265Z" },
    { url = "https://files.pythonhosted.org/packages/55/79/20d746b0a96c67203a5bee5fb4e00ac49c3e8009a39e1f78de264ecc5729/pandas-2.3.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:e5635178b387bd2ba4ac040f82bc2ef6e6b500483975c4ebacd34bec945fda12", size = 12750218, upload-time = "2025-07-07T19:19:21.547Z" },
    { url = "https://files.pythonhosted.org/packages/7c/0f/145c8b41e48dbf03dd18fdd7f24f8ba95b8254a97a3379048378f33e7838/pandas-2.3.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:6f3bf5ec947526106399a9e1d26d40ee2b259c66422efdf4de63c848492d91bb", size = 13416763, upload-time = "2025-07-07T19:19:23.939Z" },
    { url = "https://files.pythonhosted.org/packages/b2/c0/54415af59db5cdd86a3d3bf79863e8cc3fa9ed265f0745254061ac09d5f2/pandas-2.3.1-cp313-cp313-win_amd64.whl", hash = "sha256:1c78cf43c8fde236342a1cb2c34bcff89564a7bfed7e474ed2fffa6aed03a956", size = 10987482, upload-time = "2025-07-07T19:19:42.699Z" },
    { url = "https://files.pythonhosted.org/packages/48/64/2fd2e400073a1230e13b8cd604c9bc95d9e3b962e5d44088ead2e8f0cfec/pandas-2.3.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:8dfc17328e8da77be3cf9f47509e5637ba8f137148ed0e9b5241e1baf526e20a", size = 12029159, upload-time = "2025-07-07T19:19:26.362Z" },
    { url = "https://files.pythonhosted.org/packages/d8/0a/d84fd79b0293b7ef88c760d7dca69828d867c89b6d9bc52d6a27e4d87316/pandas-2.3.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:ec6c851509364c59a5344458ab935e6451b31b818be467eb24b0fe89bd05b6b9", size = 11393287, upload-time = "2025-07-07T19:19:29.157Z" },
    { url = "https://files.pythonhosted.org/packages/50/ae/ff885d2b6e88f3c7520bb74ba319268b42f05d7e583b5dded9837da2723f/pandas-2.3.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:911580460fc4884d9b05254b38a6bfadddfcc6aaef856fb5859e7ca202e45275", size = 11309381, upload-time = "2025-07-07T19:19:31.436Z" },
    { url = "https://files.pythonhosted.org/packages/85/86/1fa345fc17caf5d7780d2699985c03dbe186c68fee00b526813939062bb0/pandas-2.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2f4d6feeba91744872a600e6edbbd5b033005b431d5ae8379abee5bcfa479fab", size = 11883998, upload-time = "2025-07-07T19:19:34.267Z" },
    { url = "https://files.pythonhosted.org/packages/81/aa/e58541a49b5e6310d89474333e994ee57fea97c8aaa8fc7f00b873059bbf/pandas-2.3.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:fe37e757f462d31a9cd7580236a82f353f5713a80e059a29753cf938c6775d96", size = 12704705, upload-time = "2025-07-07T19:19:36.856Z" },
    { url = "https://files.pythonhosted.org/packages/d5/f9/07086f5b0f2a19872554abeea7658200824f5835c58a106fa8f2ae96a46c/pandas-2.3.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:5db9637dbc24b631ff3707269ae4559bce4b7fd75c1c4d7e13f40edc42df4444", size = 13189044, upload-time = "2025-07-07T19:19:39.999Z" },
]

[[package]]
name = "pathspec"
version = "0.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ca/bc/f35b8446f4531a7cb215605d100cd88b7ac6f44ab3fc94870c120ab3adbf/pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712", size = 51043, upload-time = "2023-12-10T22:30:45Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08", size = 31191, upload-time = "2023-12-10T22:30:43.14Z" },
]

[[package]]
name = "pluggy"
version = "1.6.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f9/e2/3e91f31a7d2b083fe6ef3fa267035b518369d9511ffab804f839851d2779/pluggy-1.6.0.tar.gz", hash = "sha256:7dcc130b76258d33b90f61b658791dede3486c3e6bfb003ee5c9bfb396dd22f3", size = 69412, upload-time = "2025-05-15T12:30:07.975Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/54/20/4d324d65cc6d9205fabedc306948156824eb9f0ee1633355a8f7ec5c66bf/pluggy-1.6.0-py3-none-any.whl", hash = "sha256:e920276dd6813095e9377c0bc5566d94c932c33b27a3e3945d8389c374dd4746", size = 20538, upload-time = "2025-05-15T12:30:06.134Z" },
]

[[package]]
name = "prompt-toolkit"
version = "3.0.51"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wcwidth" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bb/6e/9d084c929dfe9e3bfe0c6a47e31f78a25c54627d64a66e884a8bf5474f1c/prompt_toolkit-3.0.51.tar.gz", hash = "sha256:931a162e3b27fc90c86f1b48bb1fb2c528c2761475e57c9c06de13311c7b54ed", size = 428940, upload-time = "2025-04-15T09:18:47.731Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/4f/5249960887b1fbe561d9ff265496d170b55a735b76724f10ef19f9e40716/prompt_toolkit-3.0.51-py3-none-any.whl", hash = "sha256:52742911fde84e2d423e2f9a4cf1de7d7ac4e51958f648d9540e0fb8db077b07", size = 387810, upload-time = "2025-04-15T09:18:44.753Z" },
]

[[package]]
name = "pydantic"
version = "2.11.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/00/dd/4325abf92c39ba8623b5af936ddb36ffcfe0beae70405d456ab1fb2f5b8c/pydantic-2.11.7.tar.gz", hash = "sha256:d989c3c6cb79469287b1569f7447a17848c998458d49ebe294e975b9baf0f0db", size = 788350, upload-time = "2025-06-14T08:33:17.137Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6a/c0/ec2b1c8712ca690e5d61979dee872603e92b8a32f94cc1b72d53beab008a/pydantic-2.11.7-py3-none-any.whl", hash = "sha256:dde5df002701f6de26248661f6835bbe296a47bf73990135c7d07ce741b9623b", size = 444782, upload-time = "2025-06-14T08:33:14.905Z" },
]

[[package]]
name = "pydantic-core"
version = "2.33.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ad/88/5f2260bdfae97aabf98f1778d43f69574390ad787afb646292a638c923d4/pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc", size = 435195, upload-time = "2025-04-23T18:33:52.104Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/8c/99040727b41f56616573a28771b1bfa08a3d3fe74d3d513f01251f79f172/pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f", size = 2015688, upload-time = "2025-04-23T18:31:53.175Z" },
    { url = "https://files.pythonhosted.org/packages/3a/cc/5999d1eb705a6cefc31f0b4a90e9f7fc400539b1a1030529700cc1b51838/pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6", size = 1844808, upload-time = "2025-04-23T18:31:54.79Z" },
    { url = "https://files.pythonhosted.org/packages/6f/5e/a0a7b8885c98889a18b6e376f344da1ef323d270b44edf8174d6bce4d622/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef", size = 1885580, upload-time = "2025-04-23T18:31:57.393Z" },
    { url = "https://files.pythonhosted.org/packages/3b/2a/953581f343c7d11a304581156618c3f592435523dd9d79865903272c256a/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a", size = 1973859, upload-time = "2025-04-23T18:31:59.065Z" },
    { url = "https://files.pythonhosted.org/packages/e6/55/f1a813904771c03a3f97f676c62cca0c0a4138654107c1b61f19c644868b/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916", size = 2120810, upload-time = "2025-04-23T18:32:00.78Z" },
    { url = "https://files.pythonhosted.org/packages/aa/c3/053389835a996e18853ba107a63caae0b9deb4a276c6b472931ea9ae6e48/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a", size = 2676498, upload-time = "2025-04-23T18:32:02.418Z" },
    { url = "https://files.pythonhosted.org/packages/eb/3c/f4abd740877a35abade05e437245b192f9d0ffb48bbbbd708df33d3cda37/pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d", size = 2000611, upload-time = "2025-04-23T18:32:04.152Z" },
    { url = "https://files.pythonhosted.org/packages/59/a7/63ef2fed1837d1121a894d0ce88439fe3e3b3e48c7543b2a4479eb99c2bd/pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56", size = 2107924, upload-time = "2025-04-23T18:32:06.129Z" },
    { url = "https://files.pythonhosted.org/packages/04/8f/2551964ef045669801675f1cfc3b0d74147f4901c3ffa42be2ddb1f0efc4/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5", size = 2063196, upload-time = "2025-04-23T18:32:08.178Z" },
    { url = "https://files.pythonhosted.org/packages/26/bd/d9602777e77fc6dbb0c7db9ad356e9a985825547dce5ad1d30ee04903918/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e", size = 2236389, upload-time = "2025-04-23T18:32:10.242Z" },
    { url = "https://files.pythonhosted.org/packages/42/db/0e950daa7e2230423ab342ae918a794964b053bec24ba8af013fc7c94846/pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162", size = 2239223, upload-time = "2025-04-23T18:32:12.382Z" },
    { url = "https://files.pythonhosted.org/packages/58/4d/4f937099c545a8a17eb52cb67fe0447fd9a373b348ccfa9a87f141eeb00f/pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849", size = 1900473, upload-time = "2025-04-23T18:32:14.034Z" },
    { url = "https://files.pythonhosted.org/packages/a0/75/4a0a9bac998d78d889def5e4ef2b065acba8cae8c93696906c3a91f310ca/pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9", size = 1955269, upload-time = "2025-04-23T18:32:15.783Z" },
    { url = "https://files.pythonhosted.org/packages/f9/86/1beda0576969592f1497b4ce8e7bc8cbdf614c352426271b1b10d5f0aa64/pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9", size = 1893921, upload-time = "2025-04-23T18:32:18.473Z" },
    { url = "https://files.pythonhosted.org/packages/a4/7d/e09391c2eebeab681df2b74bfe6c43422fffede8dc74187b2b0bf6fd7571/pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac", size = 1806162, upload-time = "2025-04-23T18:32:20.188Z" },
    { url = "https://files.pythonhosted.org/packages/f1/3d/847b6b1fed9f8ed3bb95a9ad04fbd0b212e832d4f0f50ff4d9ee5a9f15cf/pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5", size = 1981560, upload-time = "2025-04-23T18:32:22.354Z" },
    { url = "https://files.pythonhosted.org/packages/6f/9a/e73262f6c6656262b5fdd723ad90f518f579b7bc8622e43a942eec53c938/pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9", size = 1935777, upload-time = "2025-04-23T18:32:25.088Z" },
]

[[package]]
name = "pygments"
version = "2.19.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/b0/77/a5b8c569bf593b0140bde72ea885a803b82086995367bf2037de0159d924/pygments-2.19.2.tar.gz", hash = "sha256:636cb2477cec7f8952536970bc533bc43743542f70392ae026374600add5b887", size = 4968631, upload-time = "2025-06-21T13:39:12.283Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c7/21/705964c7812476f378728bdf590ca4b771ec72385c533964653c68e86bdc/pygments-2.19.2-py3-none-any.whl", hash = "sha256:86540386c03d588bb81d44bc3928634ff26449851e99741617ecb9037ee5ec0b", size = 1225217, upload-time = "2025-06-21T13:39:07.939Z" },
]

[[package]]
name = "pymysql"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/41/9d/ee68dee1c8821c839bb31e6e5f40e61035a5278f7c1307dde758f0c90452/PyMySQL-1.1.0.tar.gz", hash = "sha256:4f13a7df8bf36a51e81dd9f3605fede45a4878fe02f9236349fd82a3f0612f96", size = 47240, upload-time = "2023-06-26T05:34:02.058Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/30/20467e39523d0cfc2b6227902d3687a16364307260c75e6a1cb4422b0c62/PyMySQL-1.1.0-py3-none-any.whl", hash = "sha256:8969ec6d763c856f7073c4c64662882675702efcb114b4bcbb955aea3a069fa7", size = 44768, upload-time = "2023-06-26T05:33:59.951Z" },
]

[[package]]
name = "pytest"
version = "8.4.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/08/ba/45911d754e8eba3d5a841a5ce61a65a685ff1798421ac054f85aa8747dfb/pytest-8.4.1.tar.gz", hash = "sha256:7c67fd69174877359ed9371ec3af8a3d2b04741818c51e5e99cc1742251fa93c", size = 1517714, upload-time = "2025-06-18T05:48:06.109Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/29/16/c8a903f4c4dffe7a12843191437d7cd8e32751d5de349d45d3fe69544e87/pytest-8.4.1-py3-none-any.whl", hash = "sha256:539c70ba6fcead8e78eebbf1115e8b589e7565830d7d006a8723f19ac8a0afb7", size = 365474, upload-time = "2025-06-18T05:48:03.955Z" },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432, upload-time = "2024-03-01T18:36:20.211Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892, upload-time = "2024-03-01T18:36:18.57Z" },
]

[[package]]
name = "python-dotenv"
version = "1.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/b0/4bc07ccd3572a2f9df7e6782f52b0c6c90dcbb803ac4a167702d7d0dfe1e/python_dotenv-1.1.1.tar.gz", hash = "sha256:a8a6399716257f45be6a007360200409fce5cda2661e3dec71d23dc15f6189ab", size = 41978, upload-time = "2025-06-24T04:21:07.341Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5f/ed/539768cf28c661b5b068d66d96a2f155c4971a5d55684a514c1a0e0dec2f/python_dotenv-1.1.1-py3-none-any.whl", hash = "sha256:31f23644fe2602f88ff55e1f5c79ba497e01224ee7737937930c448e4d0e24dc", size = 20556, upload-time = "2025-06-24T04:21:06.073Z" },
]

[[package]]
name = "pytz"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f8/bf/abbd3cdfb8fbc7fb3d4d38d320f2441b1e7cbe29be4f23797b4a2b5d8aac/pytz-2025.2.tar.gz", hash = "sha256:360b9e3dbb49a209c21ad61809c7fb453643e048b38924c765813546746e81c3", size = 320884, upload-time = "2025-03-25T02:25:00.538Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl", hash = "sha256:5ddf76296dd8c44c26eb8f4b6f35488f3ccbf6fbbd7adee0b7262d43f0ec2f00", size = 509225, upload-time = "2025-03-25T02:24:58.468Z" },
]

[[package]]
name = "regex"
version = "2025.7.34"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0b/de/e13fa6dc61d78b30ba47481f99933a3b49a57779d625c392d8036770a60d/regex-2025.7.34.tar.gz", hash = "sha256:9ead9765217afd04a86822dfcd4ed2747dfe426e887da413b15ff0ac2457e21a", size = 400714, upload-time = "2025-07-31T00:21:16.262Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/15/16/b709b2119975035169a25aa8e4940ca177b1a2e25e14f8d996d09130368e/regex-2025.7.34-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:c3c9740a77aeef3f5e3aaab92403946a8d34437db930a0280e7e81ddcada61f5", size = 485334, upload-time = "2025-07-31T00:19:56.58Z" },
    { url = "https://files.pythonhosted.org/packages/94/a6/c09136046be0595f0331bc58a0e5f89c2d324cf734e0b0ec53cf4b12a636/regex-2025.7.34-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:69ed3bc611540f2ea70a4080f853741ec698be556b1df404599f8724690edbcd", size = 289942, upload-time = "2025-07-31T00:19:57.943Z" },
    { url = "https://files.pythonhosted.org/packages/36/91/08fc0fd0f40bdfb0e0df4134ee37cfb16e66a1044ac56d36911fd01c69d2/regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:d03c6f9dcd562c56527c42b8530aad93193e0b3254a588be1f2ed378cdfdea1b", size = 285991, upload-time = "2025-07-31T00:19:59.837Z" },
    { url = "https://files.pythonhosted.org/packages/be/2f/99dc8f6f756606f0c214d14c7b6c17270b6bbe26d5c1f05cde9dbb1c551f/regex-2025.7.34-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:6164b1d99dee1dfad33f301f174d8139d4368a9fb50bf0a3603b2eaf579963ad", size = 797415, upload-time = "2025-07-31T00:20:01.668Z" },
    { url = "https://files.pythonhosted.org/packages/62/cf/2fcdca1110495458ba4e95c52ce73b361cf1cafd8a53b5c31542cde9a15b/regex-2025.7.34-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:1e4f4f62599b8142362f164ce776f19d79bdd21273e86920a7b604a4275b4f59", size = 862487, upload-time = "2025-07-31T00:20:03.142Z" },
    { url = "https://files.pythonhosted.org/packages/90/38/899105dd27fed394e3fae45607c1983e138273ec167e47882fc401f112b9/regex-2025.7.34-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:72a26dcc6a59c057b292f39d41465d8233a10fd69121fa24f8f43ec6294e5415", size = 910717, upload-time = "2025-07-31T00:20:04.727Z" },
    { url = "https://files.pythonhosted.org/packages/ee/f6/4716198dbd0bcc9c45625ac4c81a435d1c4d8ad662e8576dac06bab35b17/regex-2025.7.34-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d5273fddf7a3e602695c92716c420c377599ed3c853ea669c1fe26218867002f", size = 801943, upload-time = "2025-07-31T00:20:07.1Z" },
    { url = "https://files.pythonhosted.org/packages/40/5d/cff8896d27e4e3dd11dd72ac78797c7987eb50fe4debc2c0f2f1682eb06d/regex-2025.7.34-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:c1844be23cd40135b3a5a4dd298e1e0c0cb36757364dd6cdc6025770363e06c1", size = 786664, upload-time = "2025-07-31T00:20:08.818Z" },
    { url = "https://files.pythonhosted.org/packages/10/29/758bf83cf7b4c34f07ac3423ea03cee3eb3176941641e4ccc05620f6c0b8/regex-2025.7.34-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:dde35e2afbbe2272f8abee3b9fe6772d9b5a07d82607b5788e8508974059925c", size = 856457, upload-time = "2025-07-31T00:20:10.328Z" },
    { url = "https://files.pythonhosted.org/packages/d7/30/c19d212b619963c5b460bfed0ea69a092c6a43cba52a973d46c27b3e2975/regex-2025.7.34-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:f3f6e8e7af516a7549412ce57613e859c3be27d55341a894aacaa11703a4c31a", size = 849008, upload-time = "2025-07-31T00:20:11.823Z" },
    { url = "https://files.pythonhosted.org/packages/9e/b8/3c35da3b12c87e3cc00010ef6c3a4ae787cff0bc381aa3d251def219969a/regex-2025.7.34-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:469142fb94a869beb25b5f18ea87646d21def10fbacb0bcb749224f3509476f0", size = 788101, upload-time = "2025-07-31T00:20:13.729Z" },
    { url = "https://files.pythonhosted.org/packages/47/80/2f46677c0b3c2b723b2c358d19f9346e714113865da0f5f736ca1a883bde/regex-2025.7.34-cp313-cp313-win32.whl", hash = "sha256:da7507d083ee33ccea1310447410c27ca11fb9ef18c95899ca57ff60a7e4d8f1", size = 264401, upload-time = "2025-07-31T00:20:15.233Z" },
    { url = "https://files.pythonhosted.org/packages/be/fa/917d64dd074682606a003cba33585c28138c77d848ef72fc77cbb1183849/regex-2025.7.34-cp313-cp313-win_amd64.whl", hash = "sha256:9d644de5520441e5f7e2db63aec2748948cc39ed4d7a87fd5db578ea4043d997", size = 275368, upload-time = "2025-07-31T00:20:16.711Z" },
    { url = "https://files.pythonhosted.org/packages/65/cd/f94383666704170a2154a5df7b16be28f0c27a266bffcd843e58bc84120f/regex-2025.7.34-cp313-cp313-win_arm64.whl", hash = "sha256:7bf1c5503a9f2cbd2f52d7e260acb3131b07b6273c470abb78568174fe6bde3f", size = 268482, upload-time = "2025-07-31T00:20:18.189Z" },
    { url = "https://files.pythonhosted.org/packages/ac/23/6376f3a23cf2f3c00514b1cdd8c990afb4dfbac3cb4a68b633c6b7e2e307/regex-2025.7.34-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:8283afe7042d8270cecf27cca558873168e771183d4d593e3c5fe5f12402212a", size = 485385, upload-time = "2025-07-31T00:20:19.692Z" },
    { url = "https://files.pythonhosted.org/packages/73/5b/6d4d3a0b4d312adbfd6d5694c8dddcf1396708976dd87e4d00af439d962b/regex-2025.7.34-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:6c053f9647e3421dd2f5dff8172eb7b4eec129df9d1d2f7133a4386319b47435", size = 289788, upload-time = "2025-07-31T00:20:21.941Z" },
    { url = "https://files.pythonhosted.org/packages/92/71/5862ac9913746e5054d01cb9fb8125b3d0802c0706ef547cae1e7f4428fa/regex-2025.7.34-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:a16dd56bbcb7d10e62861c3cd000290ddff28ea142ffb5eb3470f183628011ac", size = 286136, upload-time = "2025-07-31T00:20:26.146Z" },
    { url = "https://files.pythonhosted.org/packages/27/df/5b505dc447eb71278eba10d5ec940769ca89c1af70f0468bfbcb98035dc2/regex-2025.7.34-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:69c593ff5a24c0d5c1112b0df9b09eae42b33c014bdca7022d6523b210b69f72", size = 797753, upload-time = "2025-07-31T00:20:27.919Z" },
    { url = "https://files.pythonhosted.org/packages/86/38/3e3dc953d13998fa047e9a2414b556201dbd7147034fbac129392363253b/regex-2025.7.34-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:98d0ce170fcde1a03b5df19c5650db22ab58af375aaa6ff07978a85c9f250f0e", size = 863263, upload-time = "2025-07-31T00:20:29.803Z" },
    { url = "https://files.pythonhosted.org/packages/68/e5/3ff66b29dde12f5b874dda2d9dec7245c2051f2528d8c2a797901497f140/regex-2025.7.34-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:d72765a4bff8c43711d5b0f5b452991a9947853dfa471972169b3cc0ba1d0751", size = 910103, upload-time = "2025-07-31T00:20:31.313Z" },
    { url = "https://files.pythonhosted.org/packages/9e/fe/14176f2182125977fba3711adea73f472a11f3f9288c1317c59cd16ad5e6/regex-2025.7.34-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:4494f8fd95a77eb434039ad8460e64d57baa0434f1395b7da44015bef650d0e4", size = 801709, upload-time = "2025-07-31T00:20:33.323Z" },
    { url = "https://files.pythonhosted.org/packages/5a/0d/80d4e66ed24f1ba876a9e8e31b709f9fd22d5c266bf5f3ab3c1afe683d7d/regex-2025.7.34-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:4f42b522259c66e918a0121a12429b2abcf696c6f967fa37bdc7b72e61469f98", size = 786726, upload-time = "2025-07-31T00:20:35.252Z" },
    { url = "https://files.pythonhosted.org/packages/12/75/c3ebb30e04a56c046f5c85179dc173818551037daae2c0c940c7b19152cb/regex-2025.7.34-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:aaef1f056d96a0a5d53ad47d019d5b4c66fe4be2da87016e0d43b7242599ffc7", size = 857306, upload-time = "2025-07-31T00:20:37.12Z" },
    { url = "https://files.pythonhosted.org/packages/b1/b2/a4dc5d8b14f90924f27f0ac4c4c4f5e195b723be98adecc884f6716614b6/regex-2025.7.34-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:656433e5b7dccc9bc0da6312da8eb897b81f5e560321ec413500e5367fcd5d47", size = 848494, upload-time = "2025-07-31T00:20:38.818Z" },
    { url = "https://files.pythonhosted.org/packages/0d/21/9ac6e07a4c5e8646a90b56b61f7e9dac11ae0747c857f91d3d2bc7c241d9/regex-2025.7.34-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:e91eb2c62c39705e17b4d42d4b86c4e86c884c0d15d9c5a47d0835f8387add8e", size = 787850, upload-time = "2025-07-31T00:20:40.478Z" },
    { url = "https://files.pythonhosted.org/packages/be/6c/d51204e28e7bc54f9a03bb799b04730d7e54ff2718862b8d4e09e7110a6a/regex-2025.7.34-cp314-cp314-win32.whl", hash = "sha256:f978ddfb6216028c8f1d6b0f7ef779949498b64117fc35a939022f67f810bdcb", size = 269730, upload-time = "2025-07-31T00:20:42.253Z" },
    { url = "https://files.pythonhosted.org/packages/74/52/a7e92d02fa1fdef59d113098cb9f02c5d03289a0e9f9e5d4d6acccd10677/regex-2025.7.34-cp314-cp314-win_amd64.whl", hash = "sha256:4b7dc33b9b48fb37ead12ffc7bdb846ac72f99a80373c4da48f64b373a7abeae", size = 278640, upload-time = "2025-07-31T00:20:44.42Z" },
    { url = "https://files.pythonhosted.org/packages/d1/78/a815529b559b1771080faa90c3ab401730661f99d495ab0071649f139ebd/regex-2025.7.34-cp314-cp314-win_arm64.whl", hash = "sha256:4b8c4d39f451e64809912c82392933d80fe2e4a87eeef8859fcc5380d0173c64", size = 271757, upload-time = "2025-07-31T00:20:46.355Z" },
]

[[package]]
name = "requests"
version = "2.32.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e1/0a/929373653770d8a0d7ea76c37de6e41f11eb07559b103b1c02cafb3f7cf8/requests-2.32.4.tar.gz", hash = "sha256:27d0316682c8a29834d3264820024b62a36942083d52caf2f14c0591336d3422", size = 135258, upload-time = "2025-06-09T16:43:07.34Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl", hash = "sha256:27babd3cda2a6d50b30443204ee89830707d396671944c998b5975b031ac2b2c", size = 64847, upload-time = "2025-06-09T16:43:05.728Z" },
]

[[package]]
name = "ruff"
version = "0.12.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/81/0bd3594fa0f690466e41bd033bdcdf86cba8288345ac77ad4afbe5ec743a/ruff-0.12.7.tar.gz", hash = "sha256:1fc3193f238bc2d7968772c82831a4ff69252f673be371fb49663f0068b7ec71", size = 5197814, upload-time = "2025-07-29T22:32:35.877Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/d2/6cb35e9c85e7a91e8d22ab32ae07ac39cc34a71f1009a6f9e4a2a019e602/ruff-0.12.7-py3-none-linux_armv6l.whl", hash = "sha256:76e4f31529899b8c434c3c1dede98c4483b89590e15fb49f2d46183801565303", size = 11852189, upload-time = "2025-07-29T22:31:41.281Z" },
    { url = "https://files.pythonhosted.org/packages/63/5b/a4136b9921aa84638f1a6be7fb086f8cad0fde538ba76bda3682f2599a2f/ruff-0.12.7-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:789b7a03e72507c54fb3ba6209e4bb36517b90f1a3569ea17084e3fd295500fb", size = 12519389, upload-time = "2025-07-29T22:31:54.265Z" },
    { url = "https://files.pythonhosted.org/packages/a8/c9/3e24a8472484269b6b1821794141f879c54645a111ded4b6f58f9ab0705f/ruff-0.12.7-py3-none-macosx_11_0_arm64.whl", hash = "sha256:2e1c2a3b8626339bb6369116e7030a4cf194ea48f49b64bb505732a7fce4f4e3", size = 11743384, upload-time = "2025-07-29T22:31:59.575Z" },
    { url = "https://files.pythonhosted.org/packages/26/7c/458dd25deeb3452c43eaee853c0b17a1e84169f8021a26d500ead77964fd/ruff-0.12.7-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:32dec41817623d388e645612ec70d5757a6d9c035f3744a52c7b195a57e03860", size = 11943759, upload-time = "2025-07-29T22:32:01.95Z" },
    { url = "https://files.pythonhosted.org/packages/7f/8b/658798472ef260ca050e400ab96ef7e85c366c39cf3dfbef4d0a46a528b6/ruff-0.12.7-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:47ef751f722053a5df5fa48d412dbb54d41ab9b17875c6840a58ec63ff0c247c", size = 11654028, upload-time = "2025-07-29T22:32:04.367Z" },
    { url = "https://files.pythonhosted.org/packages/a8/86/9c2336f13b2a3326d06d39178fd3448dcc7025f82514d1b15816fe42bfe8/ruff-0.12.7-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a828a5fc25a3efd3e1ff7b241fd392686c9386f20e5ac90aa9234a5faa12c423", size = 13225209, upload-time = "2025-07-29T22:32:06.952Z" },
    { url = "https://files.pythonhosted.org/packages/76/69/df73f65f53d6c463b19b6b312fd2391dc36425d926ec237a7ed028a90fc1/ruff-0.12.7-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:5726f59b171111fa6a69d82aef48f00b56598b03a22f0f4170664ff4d8298efb", size = 14182353, upload-time = "2025-07-29T22:32:10.053Z" },
    { url = "https://files.pythonhosted.org/packages/58/1e/de6cda406d99fea84b66811c189b5ea139814b98125b052424b55d28a41c/ruff-0.12.7-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:74e6f5c04c4dd4aba223f4fe6e7104f79e0eebf7d307e4f9b18c18362124bccd", size = 13631555, upload-time = "2025-07-29T22:32:12.644Z" },
    { url = "https://files.pythonhosted.org/packages/6f/ae/625d46d5164a6cc9261945a5e89df24457dc8262539ace3ac36c40f0b51e/ruff-0.12.7-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5d0bfe4e77fba61bf2ccadf8cf005d6133e3ce08793bbe870dd1c734f2699a3e", size = 12667556, upload-time = "2025-07-29T22:32:15.312Z" },
    { url = "https://files.pythonhosted.org/packages/55/bf/9cb1ea5e3066779e42ade8d0cd3d3b0582a5720a814ae1586f85014656b6/ruff-0.12.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:06bfb01e1623bf7f59ea749a841da56f8f653d641bfd046edee32ede7ff6c606", size = 12939784, upload-time = "2025-07-29T22:32:17.69Z" },
    { url = "https://files.pythonhosted.org/packages/55/7f/7ead2663be5627c04be83754c4f3096603bf5e99ed856c7cd29618c691bd/ruff-0.12.7-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:e41df94a957d50083fd09b916d6e89e497246698c3f3d5c681c8b3e7b9bb4ac8", size = 11771356, upload-time = "2025-07-29T22:32:20.134Z" },
    { url = "https://files.pythonhosted.org/packages/17/40/a95352ea16edf78cd3a938085dccc55df692a4d8ba1b3af7accbe2c806b0/ruff-0.12.7-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:4000623300563c709458d0ce170c3d0d788c23a058912f28bbadc6f905d67afa", size = 11612124, upload-time = "2025-07-29T22:32:22.645Z" },
    { url = "https://files.pythonhosted.org/packages/4d/74/633b04871c669e23b8917877e812376827c06df866e1677f15abfadc95cb/ruff-0.12.7-py3-none-musllinux_1_2_i686.whl", hash = "sha256:69ffe0e5f9b2cf2b8e289a3f8945b402a1b19eff24ec389f45f23c42a3dd6fb5", size = 12479945, upload-time = "2025-07-29T22:32:24.765Z" },
    { url = "https://files.pythonhosted.org/packages/be/34/c3ef2d7799c9778b835a76189c6f53c179d3bdebc8c65288c29032e03613/ruff-0.12.7-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:a07a5c8ffa2611a52732bdc67bf88e243abd84fe2d7f6daef3826b59abbfeda4", size = 12998677, upload-time = "2025-07-29T22:32:27.022Z" },
    { url = "https://files.pythonhosted.org/packages/77/ab/aca2e756ad7b09b3d662a41773f3edcbd262872a4fc81f920dc1ffa44541/ruff-0.12.7-py3-none-win32.whl", hash = "sha256:c928f1b2ec59fb77dfdf70e0419408898b63998789cc98197e15f560b9e77f77", size = 11756687, upload-time = "2025-07-29T22:32:29.381Z" },
    { url = "https://files.pythonhosted.org/packages/b4/71/26d45a5042bc71db22ddd8252ca9d01e9ca454f230e2996bb04f16d72799/ruff-0.12.7-py3-none-win_amd64.whl", hash = "sha256:9c18f3d707ee9edf89da76131956aba1270c6348bfee8f6c647de841eac7194f", size = 12912365, upload-time = "2025-07-29T22:32:31.517Z" },
    { url = "https://files.pythonhosted.org/packages/4c/9b/0b8aa09817b63e78d94b4977f18b1fcaead3165a5ee49251c5d5c245bb2d/ruff-0.12.7-py3-none-win_arm64.whl", hash = "sha256:dfce05101dbd11833a0776716d5d1578641b7fddb537fe7fa956ab85d1769b69", size = 11982083, upload-time = "2025-07-29T22:32:33.881Z" },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031, upload-time = "2024-12-04T17:35:28.174Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050, upload-time = "2024-12-04T17:35:26.475Z" },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372, upload-time = "2024-02-25T23:20:04.057Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235, upload-time = "2024-02-25T23:20:01.196Z" },
]

[[package]]
name = "soupsieve"
version = "2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/3f/f4/4a80cd6ef364b2e8b65b15816a843c0980f7a5a2b4dc701fc574952aa19f/soupsieve-2.7.tar.gz", hash = "sha256:ad282f9b6926286d2ead4750552c8a6142bc4c783fd66b0293547c8fe6ae126a", size = 103418, upload-time = "2025-04-20T18:50:08.518Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e7/9c/0e6afc12c269578be5c0c1c9f4b49a8d32770a080260c333ac04cc1c832d/soupsieve-2.7-py3-none-any.whl", hash = "sha256:6e60cc5c1ffaf1cebcc12e8188320b72071e922c2e897f737cadce79ad5d30c4", size = 36677, upload-time = "2025-04-20T18:50:07.196Z" },
]

[[package]]
name = "sqlalchemy"
version = "2.0.42"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "greenlet", marker = "(python_full_version < '3.14' and platform_machine == 'AMD64') or (python_full_version < '3.14' and platform_machine == 'WIN32') or (python_full_version < '3.14' and platform_machine == 'aarch64') or (python_full_version < '3.14' and platform_machine == 'amd64') or (python_full_version < '3.14' and platform_machine == 'ppc64le') or (python_full_version < '3.14' and platform_machine == 'win32') or (python_full_version < '3.14' and platform_machine == 'x86_64')" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/5a/03/a0af991e3a43174d6b83fca4fb399745abceddd1171bdabae48ce877ff47/sqlalchemy-2.0.42.tar.gz", hash = "sha256:160bedd8a5c28765bd5be4dec2d881e109e33b34922e50a3b881a7681773ac5f", size = 9749972, upload-time = "2025-07-29T12:48:09.323Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/7e/25d8c28b86730c9fb0e09156f601d7a96d1c634043bf8ba36513eb78887b/sqlalchemy-2.0.42-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:941804f55c7d507334da38133268e3f6e5b0340d584ba0f277dd884197f4ae8c", size = 2127905, upload-time = "2025-07-29T13:29:22.249Z" },
    { url = "https://files.pythonhosted.org/packages/e5/a1/9d8c93434d1d983880d976400fcb7895a79576bd94dca61c3b7b90b1ed0d/sqlalchemy-2.0.42-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:95d3d06a968a760ce2aa6a5889fefcbdd53ca935735e0768e1db046ec08cbf01", size = 2115726, upload-time = "2025-07-29T13:29:23.496Z" },
    { url = "https://files.pythonhosted.org/packages/a2/cc/d33646fcc24c87cc4e30a03556b611a4e7bcfa69a4c935bffb923e3c89f4/sqlalchemy-2.0.42-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4cf10396a8a700a0f38ccd220d940be529c8f64435c5d5b29375acab9267a6c9", size = 3246007, upload-time = "2025-07-29T13:26:44.166Z" },
    { url = "https://files.pythonhosted.org/packages/67/08/4e6c533d4c7f5e7c4cbb6fe8a2c4e813202a40f05700d4009a44ec6e236d/sqlalchemy-2.0.42-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9cae6c2b05326d7c2c7c0519f323f90e0fb9e8afa783c6a05bb9ee92a90d0f04", size = 3250919, upload-time = "2025-07-29T13:22:33.74Z" },
    { url = "https://files.pythonhosted.org/packages/5c/82/f680e9a636d217aece1b9a8030d18ad2b59b5e216e0c94e03ad86b344af3/sqlalchemy-2.0.42-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:f50f7b20677b23cfb35b6afcd8372b2feb348a38e3033f6447ee0704540be894", size = 3180546, upload-time = "2025-07-29T13:26:45.648Z" },
    { url = "https://files.pythonhosted.org/packages/7d/a2/8c8f6325f153894afa3775584c429cc936353fb1db26eddb60a549d0ff4b/sqlalchemy-2.0.42-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9d88a1c0d66d24e229e3938e1ef16ebdbd2bf4ced93af6eff55225f7465cf350", size = 3216683, upload-time = "2025-07-29T13:22:34.977Z" },
    { url = "https://files.pythonhosted.org/packages/39/44/3a451d7fa4482a8ffdf364e803ddc2cfcafc1c4635fb366f169ecc2c3b11/sqlalchemy-2.0.42-cp313-cp313-win32.whl", hash = "sha256:45c842c94c9ad546c72225a0c0d1ae8ef3f7c212484be3d429715a062970e87f", size = 2093990, upload-time = "2025-07-29T13:16:13.036Z" },
    { url = "https://files.pythonhosted.org/packages/4b/9e/9bce34f67aea0251c8ac104f7bdb2229d58fb2e86a4ad8807999c4bee34b/sqlalchemy-2.0.42-cp313-cp313-win_amd64.whl", hash = "sha256:eb9905f7f1e49fd57a7ed6269bc567fcbbdac9feadff20ad6bd7707266a91577", size = 2120473, upload-time = "2025-07-29T13:16:14.502Z" },
    { url = "https://files.pythonhosted.org/packages/ee/55/ba2546ab09a6adebc521bf3974440dc1d8c06ed342cceb30ed62a8858835/sqlalchemy-2.0.42-py3-none-any.whl", hash = "sha256:defcdff7e661f0043daa381832af65d616e060ddb54d3fe4476f51df7eaa1835", size = 1922072, upload-time = "2025-07-29T13:09:17.061Z" },
]

[[package]]
name = "starlette"
version = "0.47.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/04/57/d062573f391d062710d4088fa1369428c38d51460ab6fedff920efef932e/starlette-0.47.2.tar.gz", hash = "sha256:6ae9aa5db235e4846decc1e7b79c4f346adf41e9777aebeb49dfd09bbd7023d8", size = 2583948, upload-time = "2025-07-20T17:31:58.522Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f7/1f/b876b1f83aef204198a42dc101613fefccb32258e5428b5f9259677864b4/starlette-0.47.2-py3-none-any.whl", hash = "sha256:c5847e96134e5c5371ee9fac6fdf1a67336d5815e09eb2a01fdb57a351ef915b", size = 72984, upload-time = "2025-07-20T17:31:56.738Z" },
]

[[package]]
name = "structlog"
version = "25.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/79/b9/6e672db4fec07349e7a8a8172c1a6ae235c58679ca29c3f86a61b5e59ff3/structlog-25.4.0.tar.gz", hash = "sha256:186cd1b0a8ae762e29417095664adf1d6a31702160a46dacb7796ea82f7409e4", size = 1369138, upload-time = "2025-06-02T08:21:12.971Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a0/4a/97ee6973e3a73c74c8120d59829c3861ea52210667ec3e7a16045c62b64d/structlog-25.4.0-py3-none-any.whl", hash = "sha256:fe809ff5c27e557d14e613f45ca441aabda051d119ee5a0102aaba6ce40eed2c", size = 68720, upload-time = "2025-06-02T08:21:11.43Z" },
]

[[package]]
name = "tenacity"
version = "9.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0a/d4/2b0cd0fe285e14b36db076e78c93766ff1d529d70408bd1d2a5a84f1d929/tenacity-9.1.2.tar.gz", hash = "sha256:1169d376c297e7de388d18b4481760d478b0e99a777cad3a9c86e556f4b697cb", size = 48036, upload-time = "2025-04-02T08:25:09.966Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/30/643397144bfbfec6f6ef821f36f33e57d35946c44a2352d3c9f0ae847619/tenacity-9.1.2-py3-none-any.whl", hash = "sha256:f77bf36710d8b73a50b2dd155c97b870017ad21afe6ab300326b0371b3b05138", size = 28248, upload-time = "2025-04-02T08:25:07.678Z" },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991, upload-time = "2025-02-14T06:03:01.003Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919, upload-time = "2025-02-14T06:02:37.494Z" },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877, upload-time = "2025-02-14T06:02:39.516Z" },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095, upload-time = "2025-02-14T06:02:41.791Z" },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649, upload-time = "2025-02-14T06:02:43Z" },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465, upload-time = "2025-02-14T06:02:45.046Z" },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669, upload-time = "2025-02-14T06:02:47.341Z" },
]

[[package]]
name = "typing-extensions"
version = "4.14.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/98/5a/da40306b885cc8c09109dc2e1abd358d5684b1425678151cdaed4731c822/typing_extensions-4.14.1.tar.gz", hash = "sha256:38b39f4aeeab64884ce9f74c94263ef78f3c22467c8724005483154c26648d36", size = 107673, upload-time = "2025-07-04T13:28:34.16Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b5/00/d631e67a838026495268c2f6884f3711a15a9a2a96cd244fdaea53b823fb/typing_extensions-4.14.1-py3-none-any.whl", hash = "sha256:d1e1e3b58374dc93031d6eda2420a48ea44a36c2b4766a4fdeb3710755731d76", size = 43906, upload-time = "2025-07-04T13:28:32.743Z" },
]

[[package]]
name = "typing-inspection"
version = "0.4.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f8/b1/0c11f5058406b3af7609f121aaa6b609744687f1d158b3c3a5bf4cc94238/typing_inspection-0.4.1.tar.gz", hash = "sha256:6ae134cc0203c33377d43188d4064e9b357dba58cff3185f22924610e70a9d28", size = 75726, upload-time = "2025-05-21T18:55:23.885Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl", hash = "sha256:389055682238f53b04f7badcb49b989835495a96700ced5dab2d8feae4b26f51", size = 14552, upload-time = "2025-05-21T18:55:22.152Z" },
]

[[package]]
name = "tzdata"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/95/32/1a225d6164441be760d75c2c42e2780dc0873fe382da3e98a2e1e48361e5/tzdata-2025.2.tar.gz", hash = "sha256:b60a638fcc0daffadf82fe0f57e53d06bdec2f36c4df66280ae79bce6bd6f2b9", size = 196380, upload-time = "2025-03-23T13:54:43.652Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl", hash = "sha256:1a403fada01ff9221ca8044d701868fa132215d84beb92242d9acd2147f667a8", size = 347839, upload-time = "2025-03-23T13:54:41.845Z" },
]

[[package]]
name = "urllib3"
version = "2.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/15/22/9ee70a2574a4f4599c47dd506532914ce044817c7752a79b6a51286319bc/urllib3-2.5.0.tar.gz", hash = "sha256:3fc47733c7e419d4bc3f6b3dc2b4f890bb743906a30d56ba4a5bfa4bbff92760", size = 393185, upload-time = "2025-06-18T14:07:41.644Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl", hash = "sha256:e6b01673c0fa6a13e374b50871808eb3bf7046c4b125b216f6bf1cc604cff0dc", size = 129795, upload-time = "2025-06-18T14:07:40.39Z" },
]

[[package]]
name = "vine"
version = "5.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bd/e4/d07b5f29d283596b9727dd5275ccbceb63c44a1a82aa9e4bfd20426762ac/vine-5.1.0.tar.gz", hash = "sha256:8b62e981d35c41049211cf62a0a1242d8c1ee9bd15bb196ce38aefd6799e61e0", size = 48980, upload-time = "2023-11-05T08:46:53.857Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/03/ff/7c0c86c43b3cbb927e0ccc0255cb4057ceba4799cd44ae95174ce8e8b5b2/vine-5.1.0-py3-none-any.whl", hash = "sha256:40fdf3c48b2cfe1c38a49e9ae2da6fda88e4794c810050a728bd7413811fb1dc", size = 9636, upload-time = "2023-11-05T08:46:51.205Z" },
]

[[package]]
name = "wcwidth"
version = "0.2.13"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6c/63/53559446a878410fc5a5974feb13d31d78d752eb18aeba59c7fef1af7598/wcwidth-0.2.13.tar.gz", hash = "sha256:72ea0c06399eb286d978fdedb6923a9eb47e1c486ce63e9b4e64fc18303972b5", size = 101301, upload-time = "2024-01-06T02:10:57.829Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl", hash = "sha256:3da69048e4540d84af32131829ff948f1e022c1c6bdb8d6102117aac784f6859", size = 34166, upload-time = "2024-01-06T02:10:55.763Z" },
]

[[package]]
name = "win32-setctime"
version = "1.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/b3/8f/705086c9d734d3b663af0e9bb3d4de6578d08f46b1b101c2442fd9aecaa2/win32_setctime-1.2.0.tar.gz", hash = "sha256:ae1fdf948f5640aae05c511ade119313fb6a30d7eabe25fef9764dca5873c4c0", size = 4867, upload-time = "2024-12-07T15:28:28.314Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/07/c6fe3ad3e685340704d314d765b7912993bcb8dc198f0e7a89382d37974b/win32_setctime-1.2.0-py3-none-any.whl", hash = "sha256:95d644c4e708aba81dc3704a116d8cbc974d70b3bdb8be1d150e36be6e9d1390", size = 4083, upload-time = "2024-12-07T15:28:26.465Z" },
]



================================================
FILE: v0_local_test.md
================================================
#   快速測試 本地端 task  <啟動 / 關閉>

## 設定環境
ENV=DEV python genenv.py

## 將 專案 套件化
uv pip install -e .

## 啟動資料庫
docker compose -f mysql-network.yml up -d
    
## project_104
@crawler/project_104/task_category_104.py @crawler/project_104/task_urls_104.py @crawler/project_104/task_jobs_104.py

```bash
python -m crawler.project_104.task_category_104
python -m crawler.project_104.task_urls_104
python -m crawler.project_104.task_jobs_104
```

## project_1111
@crawler/project_1111/task_category_1111.py @crawler/project_1111/task_urls_1111.py @crawler/project_1111/task_jobs_1111.py

```bash
python -m crawler.project_1111.task_category_1111
python -m crawler.project_1111.task_urls_1111
python -m crawler.project_1111.task_jobs_1111
```

## project_cakeresume
@crawler/project_cakeresume/task_category_cakeresume.py @crawler/project_cakeresume/task_urls_cakeresume.py @crawler/project_cakeresume/task_jobs_cakeresume.py

```bash
python -m crawler.project_cakeresume.task_category_cakeresume
python -m crawler.project_cakeresume.task_urls_cakeresume
python -m crawler.project_cakeresume.task_jobs_cakeresume
```

## project_yes123
@crawler/project_yes123/task_category_yes123.py @crawler/project_yes123/task_urls_yes123.py @crawler/project_yes123/task_jobs_yes123.py

```bash
python -m crawler.project_yes123.task_category_yes123
python -m crawler.project_yes123.task_urls_yes123
python -m crawler.project_yes123.task_jobs_yes123
```



================================================
FILE: .python-version
================================================
3.13



================================================
FILE: crawler/__init__.py
================================================




================================================
FILE: crawler/check_crawler_config.py
================================================
import structlog

from .logging_config import configure_logging
from .config import (
    RABBITMQ_HOST,
    RABBITMQ_PORT,
)

configure_logging()

logger = structlog.get_logger(__name__)  # Corrected: add __name__

logger.info(
    "RabbitMQ configuration check.",  # Improved log message
    rabbitmq_host=RABBITMQ_HOST,
    rabbitmq_port=RABBITMQ_PORT,
    worker_account="***masked***",  # Masked sensitive info
    worker_password="***masked***",  # Masked sensitive info
)



================================================
FILE: crawler/config.py
================================================
import os
import configparser
import structlog

logger = structlog.get_logger(__name__)

config = configparser.ConfigParser()
config_path = os.path.join(os.path.dirname(__file__), "..", "local.ini")

try:
    config.read(config_path)
except Exception as e:
    logger.critical(f"無法讀取 local.ini 設定檔: {e}", exc_info=True)
    raise RuntimeError("無法讀取設定檔。") from e

# Determine which section to use based on APP_ENV environment variable
# Default to 'DOCKER' if APP_ENV is not set or invalid
app_env = os.environ.get("APP_ENV", "DOCKER").upper()
if app_env not in config:
    logger.warning(
        f"環境變數 APP_ENV={app_env} 無效或未找到對應區塊，預設使用 [DOCKER] 設定。"
    )
    app_env = "DOCKER"

config_section = config[app_env]

WORKER_ACCOUNT = config_section.get("WORKER_ACCOUNT", "worker")
WORKER_PASSWORD = config_section.get("WORKER_PASSWORD", "worker")

RABBITMQ_HOST = config_section.get("RABBITMQ_HOST", "127.0.0.1")
RABBITMQ_PORT = int(config_section.get("RABBITMQ_PORT", "5672"))

MYSQL_HOST = config_section.get("MYSQL_HOST", "crawler_jobs_mysql")
MYSQL_PORT = int(config_section.get("MYSQL_PORT", "3306"))
MYSQL_ACCOUNT = config_section.get("MYSQL_ACCOUNT", "root")
MYSQL_ROOT_PASSWORD = config_section.get("MYSQL_ROOT_PASSWORD", "root_password")
MYSQL_PASSWORD = config_section.get("MYSQL_PASSWORD", "root_password")
MYSQL_DATABASE = os.environ.get('CRAWLER_DB_NAME') or config_section.get("MYSQL_DATABASE", "crawler_db")
LOG_LEVEL = config_section.get("LOG_LEVEL", "DEBUG").upper()
LOG_FORMATTER = config_section.get("LOG_FORMATTER", "json").lower()

PRODUCER_BATCH_SIZE = int(config_section.get("PRODUCER_BATCH_SIZE", "100"))
PRODUCER_DISPATCH_INTERVAL_SECONDS = float(
    config_section.get("PRODUCER_DISPATCH_INTERVAL_SECONDS", "1.0")
)

URL_CRAWLER_REQUEST_TIMEOUT_SECONDS = int(
    config_section.get("URL_CRAWLER_REQUEST_TIMEOUT_SECONDS", "30")
)
URL_CRAWLER_UPLOAD_BATCH_SIZE = int(
    config_section.get("URL_CRAWLER_UPLOAD_BATCH_SIZE", "30")
)
URL_CRAWLER_SLEEP_MIN_SECONDS = float(
    config_section.get("URL_CRAWLER_SLEEP_MIN_SECONDS", "0.5")
)
URL_CRAWLER_SLEEP_MAX_SECONDS = float(
    config_section.get("URL_CRAWLER_SLEEP_MAX_SECONDS", "1.5")
)



================================================
FILE: crawler/logging_config.py
================================================
import logging
import structlog
import sys

# 從集中的設定模組導入日誌級別和格式化工具
from crawler.config import LOG_LEVEL, LOG_FORMATTER


def configure_logging():
    """
    配置應用程式的日誌系統，整合 structlog 和標準 logging。
    日誌級別和格式化工具從 crawler.config 獲取。
    """
    numeric_log_level = getattr(logging, LOG_LEVEL, logging.INFO)

    # 檢查是否已經配置過，避免重複添加 handler
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        # 1. 配置 structlog 的處理器鏈
        #    - TimeStamper: 添加時間戳
        #    - add_logger_name, add_log_level: 添加日誌器名稱和級別
        #    - wrap_for_formatter: 為標準庫的 formatter 準備
        processors = [
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ]
        structlog.configure(
            processors=processors,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # 2. 根據 LOG_FORMATTER 選擇渲染器
        #    - console: 美觀、適合開發但效能較差
        #    - key_value: 結構化、易讀且效能好
        #    - json: 機器可讀、效能最佳，適合生產環境
        if LOG_FORMATTER == "console":
            renderer = structlog.dev.ConsoleRenderer()
        elif LOG_FORMATTER == "key_value":
            renderer = structlog.processors.KeyValueRenderer(key_order=['timestamp', 'level', 'event'])
        else:
            renderer = structlog.processors.JSONRenderer()

        # 3. 配置標準 logging 的 formatter 和 handler
        formatter = structlog.stdlib.ProcessorFormatter(
            processor=renderer,
            foreign_pre_chain=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.processors.TimeStamper(fmt="iso"),
            ],
        )

        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)

        # 4. 配置 root logger
        root_logger.addHandler(handler)
        root_logger.setLevel(numeric_log_level)

        logger = structlog.get_logger(__name__)
        logger.info(
            "日誌系統配置完成",
            configured_level=LOG_LEVEL,
            formatter=LOG_FORMATTER,
        )



================================================
FILE: crawler/worker.py
================================================
from celery import Celery
import structlog
import logging # Import logging module
import sys # Import sys module

from crawler.config import (
    RABBITMQ_HOST,
    RABBITMQ_PORT,
    WORKER_ACCOUNT,
    WORKER_PASSWORD,
    LOG_LEVEL, # Import LOG_LEVEL
)

# configure_logging() # Removed direct call
logger = structlog.get_logger(__name__)

logger.info(
    "RabbitMQ configuration",
    rabbitmq_host=RABBITMQ_HOST,
    rabbitmq_port=RABBITMQ_PORT,
    worker_account="***masked***",
    worker_password="***masked***",
)

app = Celery(
    "task",
    include=[
        "crawler.project_104.task_category_104",
        "crawler.project_104.task_jobs_104",
        "crawler.project_104.task_urls_104",
        "crawler.project_cakeresume.task_category_cakeresume",
        "crawler.project_cakeresume.task_jobs_cakeresume",
        "crawler.project_cakeresume.task_urls_cakeresume",
        "crawler.project_1111.task_category_1111",
        "crawler.project_1111.task_jobs_1111",
        "crawler.project_1111.task_urls_1111",
        "crawler.project_yes123.task_category_yes123",
        "crawler.project_yes123.task_jobs_yes123",
        "crawler.project_yes123.task_urls_yes123",
    ],
    # Configure broker connection settings for robustness
    broker_connection_retry_on_startup=True,
    broker_connection_max_retries=10,
    broker_connection_timeout=30,
)

# Set the broker URL using app.conf
app.conf.broker_url = (
    f"pyamqp://{WORKER_ACCOUNT}:{WORKER_PASSWORD}@{RABBITMQ_HOST}:{RABBITMQ_PORT}/"
)

# Initialize database when Celery app is ready
@app.on_after_configure.connect
def setup_database_connection(sender, **kwargs):
    from crawler.database.connection import initialize_database
    initialize_database()
    logger.info("Celery app configured and database initialized.")

# Configure Celery's logging to use structlog
@app.on_after_configure.connect
def setup_logging(sender, **kwargs):
    # 確保只配置一次
    if not logging.getLogger().handlers:
        # 配置 structlog 的處理器鏈
        structlog.configure(
            processors=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
            ],
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # 配置標準 logging 的 formatter 和 handler
        formatter = structlog.stdlib.ProcessorFormatter(
            processor=structlog.dev.ConsoleRenderer(level=getattr(logging, LOG_LEVEL, logging.INFO)),
            foreign_pre_chain=[
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
            ],
        )

        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(formatter)
        handler.setLevel(getattr(logging, LOG_LEVEL, logging.INFO)) # Set handler level

        # 配置 root logger
        root_logger = logging.getLogger()
        root_logger.addHandler(handler)
        root_logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

        # 確保 Celery 自己的日誌也通過 structlog 處理
        logging.getLogger('celery').addHandler(handler)
        logging.getLogger('celery').setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

        logger.info("Celery 日誌系統配置完成", configured_level=LOG_LEVEL)

app.conf.task_routes = {
    "crawler.project_104.task_jobs_104.fetch_url_data_104": {"queue": "producer_jobs_104"},
    "crawler.project_104.task_urls_104.crawl_and_store_category_urls": {
        "queue": "producer_urls_104"
    },
    "crawler.project_104.task_category_104.fetch_url_data_104": {"queue": "producer_category_104"},
    "crawler.project_cakeresume.task_jobs_cakeresume.fetch_url_data_cakeresume": {"queue": "producer_jobs_cakeresume"},
    "crawler.project_cakeresume.task_urls_cakeresume.crawl_and_store_cakeresume_category_urls": {
        "queue": "producer_urls_cakeresume"
    },
    "crawler.project_cakeresume.task_category_cakeresume.fetch_url_data_cakeresume": {"queue": "producer_category_cakeresume"},
    "crawler.project_1111.task_jobs_1111.fetch_url_data_1111": {"queue": "producer_jobs_1111"},
    "crawler.project_1111.task_urls_1111.crawl_and_store_1111_category_urls": {
        "queue": "producer_urls_1111"
    },
    "crawler.project_1111.task_category_1111.fetch_url_data_1111": {"queue": "producer_category_1111"},
    "crawler.project_yes123.task_jobs_yes123.fetch_url_data_yes123": {"queue": "producer_jobs_yes123"},
    "crawler.project_yes123.task_urls_yes123.crawl_and_store_yes123_category_urls": {
        "queue": "producer_urls_yes123"
    },
    "crawler.project_yes123.task_category_yes123.fetch_url_data_yes123": {"queue": "producer_category_yes123"},
}



================================================
FILE: crawler/database/connection.py
================================================
import os
import logging
import structlog
from contextlib import contextmanager

from tenacity import retry, stop_after_attempt, wait_exponential, before_log, RetryError
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from crawler.config import (
    MYSQL_HOST,
    MYSQL_PORT,
    MYSQL_ACCOUNT,
    MYSQL_PASSWORD,
    MYSQL_DATABASE as DEFAULT_DB_NAME,
)
from crawler.database.models import Base

logger = structlog.get_logger(__name__)
metadata = Base.metadata
_engine = None  # Singleton engine instance

SessionLocal = sessionmaker(autocommit=False, autoflush=False)


def get_db_name() -> str:
    """
    Determines the database name to use.
    It prioritizes the CRAWLER_DB_NAME environment variable for testing purposes.
    Otherwise, it falls back to the default database name from the config.
    """
    return os.environ.get('CRAWLER_DB_NAME', DEFAULT_DB_NAME)


@contextmanager
def get_session():
    """
    Provides a transactional database session via a context manager.
    Handles commit, rollback, and closing automatically.
    """
    engine = get_engine()  # Ensure engine is initialized
    SessionLocal.configure(bind=engine)
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        logger.error("Session encountered an error, performing rollback.", exc_info=True)
        session.rollback()
        raise
    finally:
        session.close()


def get_engine():
    """
    Retrieves the SQLAlchemy engine instance, creating it if it doesn't exist.
    This is a singleton to ensure one engine instance per application lifecycle.
    """
    global _engine
    if _engine is None:
        try:
            _engine = _connect_with_retry()
        except RetryError as e:
            logger.critical(
                "Database connection failed after multiple retries. Application cannot start.",
                error=e,
                exc_info=True,
            )
            raise RuntimeError("Database connection failed. Please check the database service.") from e
        except Exception as e:
            logger.critical(
                "An unexpected error occurred while creating the database engine.",
                error=e,
                exc_info=True,
            )
            raise RuntimeError("Fatal error creating the database engine.") from e
    return _engine


@retry(
    stop=stop_after_attempt(8),
    wait=wait_exponential(multiplier=1, min=2, max=20),
    before=before_log(logger, logging.INFO),
    reraise=True,
)
def _connect_with_retry() -> create_engine:
    """
    (Internal) Performs the actual database connection with retry logic.
    """
    db_name = get_db_name()
    logger.info(f"Attempting to connect to database: {db_name}@{MYSQL_HOST}:{MYSQL_PORT}")

    db_url = (
        f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{db_name}?charset=utf8mb4"
    )

    engine = create_engine(
        db_url,
        pool_recycle=3600,
        echo=False,
        connect_args={"connect_timeout": 10},
        isolation_level="READ COMMITTED",
    )

    # Test the connection; this will trigger tenacity's retry if it fails
    with engine.connect() as conn:
        conn.execute(text("SELECT 1"))

    logger.info("Database engine created successfully, connection test passed.")
    return engine


def initialize_database():
    """
    Initializes the database. If the target is 'test_db', it ensures
    the database exists before creating tables. For other databases,
    it simply creates tables based on the models.
    """
    db_name = get_db_name()
    logger.info(f"Initializing database: {db_name}")

    # If using the test database, ensure it exists first.
    if db_name == 'test_db':
        server_db_url = (
            f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
            f"{MYSQL_HOST}:{MYSQL_PORT}/?charset=utf8mb4"
        )
        server_engine = create_engine(server_db_url)
        try:
            with server_engine.connect() as connection:
                connection.execute(text(f"CREATE DATABASE IF NOT EXISTS {db_name};"))
                connection.commit()
            logger.info(f"Ensured database '{db_name}' exists.")
        finally:
            server_engine.dispose()

    # Now, connect to the specific database and create all tables
    try:
        engine = get_engine()
        metadata.create_all(engine)
        logger.info(f"Database tables for '{db_name}' initialized successfully.")
    except Exception as e:
        logger.critical(
            f"Failed to initialize tables for database '{db_name}'.",
            error=e,
            exc_info=True,
        )
        raise


================================================
FILE: crawler/database/models.py
================================================
from sqlalchemy import Column, Integer, String, Text, DateTime, Enum, ForeignKey, UniqueConstraint
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime, timezone

from crawler.database.schemas import (
    SourcePlatform,
    JobStatus,
    CrawlStatus,
    SalaryType,
    JobType,
)

Base = declarative_base()


# SQLAlchemy Models
class CategorySource(Base):
    __tablename__ = "tb_category_source"
    id = Column(Integer, primary_key=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False)
    source_category_id = Column(String(255), nullable=False, unique=True)
    source_category_name = Column(String(255), nullable=False)
    parent_source_id = Column(String(255))

    url_associations = relationship("UrlCategory", back_populates="category")


class Url(Base):
    __tablename__ = "tb_urls"
    source_url = Column(String(512), primary_key=True)
    source = Column(Enum(SourcePlatform), nullable=False, index=True)
    status = Column(
        Enum(JobStatus), nullable=False, index=True, default=JobStatus.ACTIVE
    )
    details_crawl_status = Column(
        String(20), nullable=False, index=True, default=CrawlStatus.PENDING.value
    )
    crawled_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    details_crawled_at = Column(DateTime)

    category_associations = relationship("UrlCategory", back_populates="url")


class UrlCategory(Base):
    __tablename__ = "tb_url_categories"
    source_url = Column(String(512), ForeignKey("tb_urls.source_url"), primary_key=True)
    source_category_id = Column(String(255), ForeignKey("tb_category_source.source_category_id"), primary_key=True)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc), nullable=False)

    url = relationship("Url", back_populates="category_associations")
    category = relationship("CategorySource", back_populates="url_associations")


class Job(Base):
    __tablename__ = "tb_jobs"
    id = Column(Integer, primary_key=True, autoincrement=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False, index=True)
    source_job_id = Column(String(255), index=True, nullable=False)
    url = Column(String(512), index=True, nullable=False)
    status = Column(Enum(JobStatus), nullable=False)
    title = Column(String(255), nullable=False)
    description = Column(Text)
    job_type = Column(Enum(JobType))
    location_text = Column(String(255))
    posted_at = Column(DateTime)
    salary_text = Column(String(255))
    salary_min = Column(Integer)
    salary_max = Column(Integer)
    salary_type = Column(Enum(SalaryType))
    experience_required_text = Column(String(255))
    education_required_text = Column(String(255))
    company_source_id = Column(String(255))
    company_name = Column(String(255))
    company_url = Column(String(512))
    created_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (UniqueConstraint('source_platform', 'url', name='_source_platform_url_uc'),)


class JobLocation(Base):
    __tablename__ = "tb_job_locations"
    id = Column(Integer, primary_key=True, autoincrement=True)
    source_platform = Column(Enum(SourcePlatform), nullable=False, index=True)
    source_job_id = Column(String(255), nullable=False, index=True)
    latitude = Column(String(255))
    longitude = Column(String(255))
    created_at = Column(
        DateTime, default=lambda: datetime.now(timezone.utc), nullable=False
    )
    updated_at = Column(
        DateTime,
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (UniqueConstraint('source_platform', 'source_job_id', name='_source_platform_job_id_uc'),)


================================================
FILE: crawler/database/repository.py
================================================
import structlog
from typing import List, Dict, Any, Optional, Set
from datetime import datetime, timezone, timedelta
import pandas as pd

from sqlalchemy import select, update, delete
from sqlalchemy.dialects.mysql import insert
from sqlalchemy.orm import DeclarativeBase

from crawler.database.connection import get_session

from crawler.database.models import (
    CategorySource,
    Url,
    Job,
    UrlCategory,
    JobLocation,
)
from crawler.database.schemas import (
    SourcePlatform,
    JobStatus,
    CrawlStatus,
    JobPydantic,
    CategorySourcePydantic,
    JobLocationPydantic,
)

logger = structlog.get_logger(__name__)


def _generic_upsert(
    model: DeclarativeBase, data_list: List[Dict[str, Any]], update_columns: List[str]
) -> int:
    """
    通用的 UPSERT 函式，用於將數據同步到資料庫。
    如果記錄已存在則更新指定欄位；否則插入新記錄。
    """
    if not data_list:
        return 0

    with get_session() as session:
        stmt = insert(model).values(data_list)
        
        if update_columns: # 只有當有需要更新的欄位時才構建 update_dict
            update_dict = {col: getattr(stmt.inserted, col) for col in update_columns}
            stmt = stmt.on_duplicate_key_update(**update_dict)
        else:
            # 對於沒有額外更新欄位的 UPSERT (例如只有複合主鍵的關聯表)，
            # 仍然需要呼叫 on_duplicate_key_update 以觸發 ON DUPLICATE KEY 行為
            # 這裡選擇更新第一個非主鍵欄位為其自身，以滿足語法要求但不實際更新數據
            # 如果沒有非主鍵欄位，則選擇第一個主鍵欄位
            first_column_name = next(iter(model.__table__.columns)).name
            stmt = stmt.on_duplicate_key_update(**{first_column_name: getattr(stmt.inserted, first_column_name)})

        result = session.execute(stmt)
        return result.rowcount # 返回受影響的行數


def clear_urls_and_categories() -> None:
    """
    清空 tb_urls 和 tb_url_categories 資料表。
    """
    with get_session() as session:
        session.execute(delete(UrlCategory))
        session.execute(delete(Url))
        session.commit()
        logger.info("已清空 tb_urls 和 tb_url_categories 資料表。")


def sync_source_categories(
    platform: SourcePlatform, flattened_data: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    將抓取到的職務分類數據同步到資料庫。
    執行 UPSERT 操作，如果分類已存在則更新，否則插入。
    """
    if not flattened_data:
        logger.info(
            "No flattened data to sync for categories.", platform=platform.value
        )
        return {"total": 0, "affected": 0}

    update_cols = ["source_category_name", "parent_source_id"]
    affected_rows = _generic_upsert(CategorySource, flattened_data, update_cols)

    logger.info(
        "Categories synced successfully.",
        platform=platform.value,
        total_categories=len(flattened_data),
        affected_rows=affected_rows,
    )
    return {"total": len(flattened_data), "affected": affected_rows}


def get_source_categories(
    platform: SourcePlatform, source_ids: Optional[List[str]] = None
) -> List[CategorySourcePydantic]:
    """
    從資料庫獲取指定平台和可選的 source_ids 的職務分類。
    返回 CategorySourcePydantic 實例列表，以便在 Session 關閉後安全使用。
    """
    with get_session() as session:
        stmt = select(CategorySource).where(CategorySource.source_platform == platform)
        if source_ids:
            stmt = stmt.where(CategorySource.source_category_id.in_(source_ids))

        categories = [
            CategorySourcePydantic.model_validate(cat)
            for cat in session.scalars(stmt).all()
        ]
        logger.debug(
            "Fetched source categories.",
            platform=platform.value,
            count=len(categories),
            source_ids=source_ids,
        )
        return categories


def get_all_categories_for_platform(
    platform: SourcePlatform,
) -> List[CategorySourcePydantic]:
    """
    從資料庫獲取指定平台的所有職務分類。
    返回 CategorySourcePydantic 實例列表。
    """
    with get_session() as session:
        stmt = select(CategorySource).where(CategorySource.source_platform == platform)
        categories = [
            CategorySourcePydantic.model_validate(cat)
            for cat in session.scalars(stmt).all()
        ]
        logger.debug(
            "Fetched all categories for platform.",
            platform=platform.value,
            count=len(categories),
        )
        return categories


def upsert_urls(platform: SourcePlatform, urls: List[str]) -> None:
    """
    Synchronizes a list of URLs for a given platform with the database。
    Performs an UPSERT operation. URLs are marked as ACTIVE and PENDING.
    """
    if not urls:
        logger.info("No URLs to upsert.", platform=platform.value)
        return

    from crawler.database.connection import get_db_name
    db_name = get_db_name()
    logger.info("Attempting to upsert URLs.", platform=platform.value, count=len(urls), db=db_name)

    now = datetime.now(timezone.utc)
    url_models_to_upsert = [
        {
            "source_url": url,
            "source": platform,
            "status": JobStatus.ACTIVE.value,
            "details_crawl_status": CrawlStatus.QUEUED.value,
            "crawled_at": now,
            "updated_at": now,
        }
        for url in urls
    ]

    update_cols = ["status", "updated_at", "details_crawl_status"]
    affected_rows = _generic_upsert(Url, url_models_to_upsert, update_cols)

    logger.info("URLs upserted successfully.", platform=platform.value, count=len(urls), affected_rows=affected_rows)


def get_urls_by_crawl_status(
    platform: SourcePlatform, statuses: List[CrawlStatus], limit: int
) -> List[str]:
    """
    從資料庫獲取指定平台和指定爬取狀態列表的 URL。
    返回 URL 字串列表。
    """
    with get_session() as session:
        statement = (
            select(Url.source_url)
            .where(Url.source == platform, Url.details_crawl_status.in_(statuses))
            .limit(limit)
        )
        urls = list(session.scalars(statement).all())
        logger.debug(
            "Fetched URLs by crawl status.",
            platform=platform.value,
            statuses=[s.value for s in statuses],
            count=len(urls),
            limit=limit,
        )
        return urls


def update_urls_status(urls: List[str], status: CrawlStatus) -> None:
    """
    批量更新一組 URL 的爬取狀態。
    """
    if not urls:
        logger.info("No URLs provided to update status.")
        return

    now = datetime.now(timezone.utc)
    with get_session() as session:
        stmt = (
            update(Url)
            .where(Url.source_url.in_(urls))
            .values(details_crawl_status=status, updated_at=now)
        )
        session.execute(stmt)
        logger.info(
            "Successfully updated URL statuses.",
            status=status.value,
            count=len(urls),
        )


def upsert_jobs(jobs: List[JobPydantic]) -> None:
    """
    將 Job 對象列表同步到資料庫。
    執行 UPSERT 操作，如果職位已存在則更新，否則插入。
    """
    if not jobs:
        logger.info("No jobs to upsert.", count=0)
        return

    now = datetime.now(timezone.utc)
    job_dicts_to_upsert = [
        {
            **job.model_dump(
                exclude_none=False,
                exclude={"extracted_skills", "extracted_languages", "extracted_licenses"}
            ),
            "updated_at": now,
            "created_at": job.created_at or now,
        }
        for job in jobs
    ]

    # --- DEBUG PRINT --- START
    for job_dict in job_dicts_to_upsert:
        logger.info("Job dict before upsert", job_id=job_dict.get("source_job_id"), salary_type=job_dict.get("salary_type"))
    # --- DEBUG PRINT --- END

    # 動態生成更新欄位列表，排除主鍵
    update_cols = [
        column.name for column in Job.__table__.columns if not column.primary_key
    ]
    affected_rows = _generic_upsert(Job, job_dicts_to_upsert, update_cols)

    logger.info("Jobs upserted successfully.", count=len(job_dicts_to_upsert), affected_rows=affected_rows)


def upsert_job_locations(job_locations: List[JobLocationPydantic]) -> None:
    """
    將 JobLocation 對象列表同步到資料庫。
    執行 UPSERT 操作，如果經緯度資訊已存在則更新，否則插入。
    """
    if not job_locations:
        logger.info("No job locations to upsert.", count=0)
        return

    now = datetime.now(timezone.utc)
    location_dicts_to_upsert = [
        {
            **loc.model_dump(exclude_none=False),
            "updated_at": now,
            "created_at": loc.created_at or now,
        }
        for loc in job_locations
    ]

    update_cols = ["latitude", "longitude", "updated_at"]
    affected_rows = _generic_upsert(JobLocation, location_dicts_to_upsert, update_cols)

    logger.info("Job locations upserted successfully.", count=len(location_dicts_to_upsert), affected_rows=affected_rows)


def upsert_url_categories(url_category_data: List[Dict[str, Any]]) -> None:
    """
    將 URL 與其所屬的分類關聯數據同步到資料庫。
    執行 UPSERT 操作，如果關聯已存在則更新，否則插入。
    """
    if not url_category_data:
        logger.info("No URL category data to upsert.")
        return

    from crawler.database.connection import get_db_name
    db_name = get_db_name()
    logger.info("Attempting to upsert URL categories.", count=len(url_category_data), db=db_name)

    affected_rows = _generic_upsert(UrlCategory, url_category_data, []) # UrlCategory has composite primary key, no update columns needed for UPSERT

    logger.info("URL categories upserted successfully.", count=len(url_category_data), affected_rows=affected_rows)


def mark_urls_as_crawled(processed_urls: Dict[CrawlStatus, List[str]]) -> None:
    """
    根據處理狀態標記 URL 為已爬取。
    """
    if not processed_urls:
        logger.info("No URLs to mark as crawled.")
        return

    now = datetime.now(timezone.utc)
    with get_session() as session:
        for status, urls in processed_urls.items():
            if urls:
                stmt = (
                    update(Url)
                    .where(Url.source_url.in_(urls))
                    .values(details_crawl_status=status, details_crawled_at=now)
                )
                session.execute(stmt)
                logger.info(
                    "URLs marked as crawled.", status=status.value, count=len(urls)
                )

def update_category_parent_id(
    platform: SourcePlatform, source_category_id: str, new_parent_source_id: Optional[str]
) -> None:
    """
    更新指定平台和 source_category_id 的職務分類的 parent_source_id。
    """
    with get_session() as session:
        stmt = (
            update(CategorySource)
            .where(
                CategorySource.source_platform == platform,
                CategorySource.source_category_id == source_category_id,
            )
            .values(parent_source_id=new_parent_source_id)
        )
        result = session.execute(stmt)
        if result.rowcount > 0:
            logger.info(
                "Updated parent_source_id for category.",
                platform=platform.value,
                source_category_id=source_category_id,
                new_parent_source_id=new_parent_source_id,
            )
        else:
            logger.warning(
                "Category not found for update.",
                platform=platform.value,
                source_category_id=source_category_id,
            )

def get_all_category_source_ids_pandas(platform: SourcePlatform) -> Set[str]:
    """
    使用 Pandas 獲取指定平台所有職務分類的 source_category_id。
    """
    with get_session() as session:
        query = select(CategorySource.source_category_id).where(CategorySource.source_platform == platform)
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


def get_all_crawled_category_ids_pandas(platform: SourcePlatform) -> Set[str]:
    """
    使用 Pandas 獲取指定平台所有已爬取 URL 的 source_category_id。
    """
    with get_session() as session:
        query = select(UrlCategory.source_category_id).join(Url, UrlCategory.source_url == Url.source_url).where(Url.source == platform)
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


def get_root_categories(platform: SourcePlatform) -> List[CategorySourcePydantic]:
    """
    從資料庫獲取指定平台所有 parent_source_id 為 NULL 的職務分類 (即根分類)。
    """
    with get_session() as session:
        stmt = select(CategorySource).where(
            CategorySource.source_platform == platform,
            CategorySource.parent_source_id.is_(None)  # Filter for NULL parent_source_id
        )
        categories = [
            CategorySourcePydantic.model_validate(cat)
            for cat in session.scalars(stmt).all()
        ]
        logger.debug(
            "Fetched root categories for platform.",
            platform=platform.value,
            count=len(categories),
        )
        return categories


def get_stale_crawled_category_ids_pandas(platform: SourcePlatform, n_days: int) -> Set[str]:
    """
    使用 Pandas 獲取指定平台中，上次爬取時間超過 n_days 的 source_category_id。
    """
    threshold_date = datetime.now(timezone.utc) - timedelta(days=n_days)
    with get_session() as session:
        query = (
            select(UrlCategory.source_category_id)
            .join(Url, UrlCategory.source_url == Url.source_url)
            .where(
                Url.source == platform,
                UrlCategory.created_at < threshold_date
            )
            .distinct()
        )
        df = pd.read_sql(query, session.bind)
        return set(df["source_category_id"].tolist())


================================================
FILE: crawler/database/schemas.py
================================================
from datetime import datetime, timezone
from typing import Optional
import enum

from pydantic import BaseModel, Field


class SourcePlatform(str, enum.Enum):
    """資料來源平台。用於在資料庫中標識數據的來源。"""

    PLATFORM_104 = "platform_104"
    PLATFORM_1111 = "platform_1111"
    PLATFORM_CAKERESUME = "platform_cakeresume"
    PLATFORM_YES123 = "platform_yes123"


class JobStatus(str, enum.Enum):
    """職缺或 URL 的活躍狀態。"""

    ACTIVE = "active"
    INACTIVE = "inactive"


class CrawlStatus(str, enum.Enum):
    """職缺詳情頁的抓取狀態。"""

    PENDING = "PENDING"
    QUEUED = "QUEUED"
    PROCESSING = "PROCESSING"
    SUCCESS = "SUCCESS"
    FAILED = "FAILED"


class SalaryType(str, enum.Enum):
    """標準化的薪資給付週期。"""

    MONTHLY = "MONTHLY"
    HOURLY = "HOURLY"
    YEARLY = "YEARLY"
    DAILY = "DAILY"
    BY_CASE = "BY_CASE"
    NEGOTIABLE = "NEGOTIABLE"


class JobType(str, enum.Enum):
    """標準化的工作類型。"""

    FULL_TIME = "FULL_TIME"
    PART_TIME = "PART_TIME"
    CONTRACT = "CONTRACT"
    INTERNSHIP = "INTERNSHIP"
    TEMPORARY = "TEMPORARY"
    OTHER = "OTHER"


class CategorySourcePydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_category_id: str
    source_category_name: str
    parent_source_id: Optional[str] = None

    class Config:
        from_attributes = True


class UrlPydantic(BaseModel):
    source_url: str
    source: SourcePlatform
    status: JobStatus = JobStatus.ACTIVE
    details_crawl_status: CrawlStatus = CrawlStatus.PENDING
    crawled_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    details_crawled_at: Optional[datetime] = None

    class Config:
        from_attributes = True


class UrlCategoryPydantic(BaseModel):
    source_url: str
    source_category_id: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

    class Config:
        from_attributes = True


class JobPydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_job_id: str
    url: str
    status: JobStatus
    title: str
    description: Optional[str] = None
    job_type: Optional[JobType] = None
    location_text: Optional[str] = None
    posted_at: Optional[datetime] = None
    salary_text: Optional[str] = None
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    salary_type: Optional[SalaryType] = None
    experience_required_text: Optional[str] = None
    education_required_text: Optional[str] = None
    company_source_id: Optional[str] = None
    company_name: Optional[str] = None
    company_url: Optional[str] = None
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    class Config:
        from_attributes = True


class JobLocationPydantic(BaseModel):
    id: Optional[int] = None
    source_platform: SourcePlatform
    source_job_id: str
    latitude: Optional[str] = None
    longitude: Optional[str] = None
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    class Config:
        from_attributes = True



================================================
FILE: crawler/database/category_classification_data/apply_classification.py
================================================
import os
import sys

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))

from crawler.database.schemas import SourcePlatform
from crawler.database.connection import initialize_database
from crawler.database.repository import get_root_categories, update_category_parent_id
import structlog
import json

# Configure logging for the script
structlog.configure(
    processors=[
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
)
logger = structlog.get_logger(__name__)

# 15 項大項目歸類定義
# 從 JSON 檔案載入 MAJOR_CATEGORIES
_major_categories_file_path = os.path.join(os.path.dirname(__file__), "major_categories.json")
with open(_major_categories_file_path, 'r', encoding='utf-8') as f:
    MAJOR_CATEGORIES = json.load(f)

# 映射關係：原始分類名稱 -> 大項目歸類 ID
# 從 JSON 檔案載入 MAPPING
_mapping_file_path = os.path.join(os.path.dirname(__file__), "mapping.json")
with open(_mapping_file_path, 'r', encoding='utf-8') as f:
    _raw_mapping = json.load(f)

MAPPING = {
    SourcePlatform(platform_name): platform_map
    for platform_name, platform_map in _raw_mapping.items()
}

def apply_category_classification(platform: SourcePlatform):
    logger.info(f"Applying classification for platform: {platform.value}")

    major_category_ids = {cat["source_category_id"] for cat in MAJOR_CATEGORIES}

    root_categories = get_root_categories(platform)
    
    for category in root_categories:
        # Skip updating the major categories themselves
        if category.source_category_id in major_category_ids:
            continue

        original_category_name = category.source_category_name.strip()
        new_parent_id = MAPPING[platform].get(original_category_name)

        if new_parent_id:
            logger.info(
                f"Updating parent_source_id for {platform.value} - {original_category_name} to {new_parent_id}"
            )
            update_category_parent_id(
                platform,
                category.source_category_id,
                new_parent_id
            )
        else:
            logger.warning(
                f"No mapping found for {platform.value} - {original_category_name}. Skipping update."
            )
    logger.info(f"Classification application complete for {platform.value}.")


if __name__ == "__main__":
    # This block is for standalone execution of classification for all platforms
    # It will initialize the database and apply classification for all defined platforms.
    logger.info("Initializing database connection for standalone classification application...")
    initialize_database()
    logger.info("Database initialized.")

    platforms_to_process = [
        SourcePlatform.PLATFORM_104,
        SourcePlatform.PLATFORM_1111,
        SourcePlatform.PLATFORM_CAKERESUME,
        SourcePlatform.PLATFORM_YES123,
    ]

    for platform in platforms_to_process:
        apply_category_classification(platform)


================================================
FILE: crawler/database/category_classification_data/check_all_category_parents.py
================================================
import os
import sys
import json

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))

from crawler.database.schemas import SourcePlatform
from crawler.database.connection import initialize_database
from crawler.database.repository import get_source_categories

from .apply_classification import MAPPING, MAJOR_CATEGORIES

def main():
    print("Initializing database connection...")
    initialize_database()
    print("Database initialized.")

    platforms_to_check = [
        SourcePlatform.PLATFORM_104,
        SourcePlatform.PLATFORM_1111,
        SourcePlatform.PLATFORM_CAKERESUME,
        SourcePlatform.PLATFORM_YES123,
    ]

    # Load original root categories from JSON for reference
    root_categories_json_path = os.path.join(os.path.dirname(__file__), "root_categories.json")
    with open(root_categories_json_path, 'r', encoding='utf-8') as f:
        original_root_categories_data = json.load(f)

    print("\n--- Detailed Category Parent ID Check ---")

    # Create a set of major category IDs for quick lookup
    major_category_ids = {cat["source_category_id"] for cat in MAJOR_CATEGORIES}

    for platform in platforms_to_check:
        print(f"\nPlatform: {platform.value}")
        all_platform_categories = get_source_categories(platform)
        
        if not all_platform_categories:
            print("  No categories found for this platform.")
            continue

        # Get original root category names for this platform from the JSON file
        original_names_for_platform = set(original_root_categories_data.get(platform.value, []))

        # Sort categories for consistent output
        sorted_categories = sorted(all_platform_categories, key=lambda x: x.source_category_name)

        for category in sorted_categories:
            status_message = ""
            
            # Check if it's one of our new major categories
            if category.source_category_id in major_category_ids:
                status_message = "(New Major Category)"

            if category.parent_source_id is None:
                if category.source_category_name in original_names_for_platform and category.source_category_id not in major_category_ids:
                    # This category was originally a root category and should have been mapped
                    expected_parent = MAPPING[platform].get(category.source_category_name)
                    if expected_parent:
                        status_message = f"(ERROR: Expected to map to {expected_parent} but parent is NULL)"
                    else:
                        status_message = "(WARNING: Original root, NULL parent, but no mapping found in MAPPING dict)"
                elif category.source_category_id not in major_category_ids:
                    status_message = "(WARNING: Unexpected NULL parent - not an original root or major category)"
            
            print(
                f"  Category: {category.source_category_name:<30} "
                f"Source ID: {category.source_category_id:<15} "
                f"Parent Source ID: {str(category.parent_source_id):<20} "
                f"{status_message}"
            )
    print("\n--- Check complete ---")

if __name__ == "__main__":
    main()



================================================
FILE: crawler/database/category_classification_data/get_root_categories_script.py
================================================
import os
import sys
import json

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))

from crawler.database.schemas import SourcePlatform
from crawler.database.connection import initialize_database
from crawler.database.repository import get_root_categories
import structlog

# Configure logging for the script
structlog.configure(
    processors=[
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
)
logger = structlog.get_logger(__name__)

def main():
    logger.info("Initializing database connection...")
    initialize_database()
    logger.info("Database initialized.")

    platforms = [
        SourcePlatform.PLATFORM_104,
        SourcePlatform.PLATFORM_1111,
        SourcePlatform.PLATFORM_CAKERESUME,
        SourcePlatform.PLATFORM_YES123,
    ]

    all_root_categories = {}

    for platform in platforms:
        logger.info(f"Fetching root categories for platform: {platform.value}")
        root_categories = get_root_categories(platform)
        all_root_categories[platform.value] = [
            cat.source_category_name for cat in root_categories
        ]
        logger.info(f"Found {len(root_categories)} root categories for {platform.value}")

    output_file = os.path.join(os.path.dirname(__file__), "root_categories.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_root_categories, f, ensure_ascii=False, indent=4)
    logger.info(f"Root categories saved to {output_file}")

if __name__ == "__main__":
    main()



================================================
FILE: crawler/database/category_classification_data/job_classification_scheme.md
================================================
# 職位大項目歸類方案 (15 項)

本文件定義了將來自不同求職平台（104、1111、Cakeresume、Yes123）的職位頂層分類，歸納為 15 個大項目歸類。

這些大項目歸類將作為新的父級分類，用於填補資料庫中 `tb_category_source` 表格裡 `parent_source_id` 為 `null` 的記錄。

---

## 15 項大項目歸類列表

### 1. 資訊/軟體/網路 (IT/Software/Network)
*   **描述:** 涵蓋軟體開發、系統管理、網路工程、資訊安全等相關職位。
*   **原始平台分類映射:**
    *   **104:** 資訊軟體系統類
    *   **1111:** 電腦系統／資訊／軟硬體, 光電IC／電子通訊
    *   **Cakeresume:** 科技
    *   **Yes123:** MIS／網管全部, 軟體／工程全部

### 2. 工程研發 (Engineering R&D)
*   **描述:** 專注於產品、技術、材料等領域的設計、開發與創新。
*   **原始平台分類映射:**
    *   **104:** 研發相關類 (主要指工程研發)
    *   **1111:** 光電IC／電子通訊 (部分), 機械模具／生產製程 (部分)
    *   **Yes123:** 工程研發全部, 化工／材料研發全部, 生技研發全部

### 3. 生產製造/品管 (Production/Manufacturing/Quality Control)
*   **描述:** 涉及產品的生產、製造流程管理、品質控制與環境安全。
*   **原始平台分類映射:**
    *   **104:** 生產製造／品管／環衛類
    *   **1111:** 機械模具／生產製程 (部分), 採購／物流／品質檢測 (部分)
    *   **Cakeresume:** 工業 (部分)
    *   **Yes123:** 生產管理全部, 品管／品保全部, 製程規劃全部, 環境安全衛生全部

### 4. 商業/管理/行政 (Business/Management/Admin)
*   **描述:** 包含企業經營管理、人力資源、行政總務等職位。
*   **原始平台分類映射:**
    *   **104:** 經營／人資類, 行政／總務／法務類
    *   **1111:** 管理幕僚／人資／行政
    *   **Cakeresume:** 公司服務, 公共行政
    *   **Yes123:** 行政／總務全部, 人力資源全部, 經營／幕僚全部

### 5. 財會/金融/法務 (Finance/Accounting/Legal)
*   **描述:** 涵蓋財務、會計、金融服務、保險、法律與智財權相關職位。
*   **原始平台分類映射:**
    *   **104:** 財會／金融專業類
    *   **1111:** 金融保險／財會／稽核, 法務專利／顧問諮詢
    *   **Cakeresume:** 銀行 / 保險 / 金融, 顧問 / 審計, 法律 / 法規
    *   **Yes123:** 金融／保險全部, 財會／稅務全部, 法務／智財全部

### 6. 行銷/企劃/專案 (Marketing/Planning/Project)
*   **描述:** 負責市場分析、品牌推廣、活動策劃與專案管理等職位。
*   **原始平台分類映射:**
    *   **104:** 行銷／企劃／專案管理類
    *   **1111:** 行銷／企劃／專案管理
    *   **Cakeresume:** 廣告 / 行銷 / 代理
    *   **Yes123:** 行銷／企劃全部, 專案管理全部

### 7. 業務/客服/貿易 (Sales/Customer Service/Trade)
*   **描述:** 涉及產品或服務的銷售、客戶關係維護、國際貿易等職位。
*   **原始平台分類映射:**
    *   **104:** 客服／門市／業務／貿易類
    *   **1111:** 業務／貿易／客服／門市
    *   **Cakeresume:** 分銷
    *   **Yes123:** 門市幹部全部, 客戶服務全部, 國際貿易全部, 業務銷售全部

### 8. 設計/媒體/藝術 (Design/Media/Arts)
*   **描述:** 包含視覺設計、多媒體製作、內容創作、傳播藝術等職位。
*   **原始平台分類映射:**
    *   **104:** 傳播藝術／設計類, 文字／傳媒工作類
    *   **1111:** 影視傳媒／出版翻譯, 美編設計
    *   **Cakeresume:** 文化 / 媒體 / 娛樂, 設計 / 藝術
    *   **Yes123:** 文字編譯全部, 採訪類人員全部, 設計人員全部, 傳播藝術全部

### 9. 營建/建築/空間 (Construction/Architecture/Space)
*   **描述:** 涵蓋建築設計、營建工程、空間規劃與測量等職位。
*   **原始平台分類映射:**
    *   **104:** 營建／製圖類
    *   **1111:** 營建／製圖／施作
    *   **Cakeresume:** 建築設計, 房地產
    *   **Yes123:** 製圖／測量全部, 營建施作全部, 營建規劃全部

### 10. 醫療/生技/健康 (Healthcare/Biotech/Health)
*   **描述:** 涉及醫療服務、生物科技研發、健康照護等職位。
*   **原始平台分類映射:**
    *   **104:** 醫療／保健服務類
    *   **1111:** 醫療／護理／保健, 生物科技／化學製藥
    *   **Cakeresume:** 生醫 / 醫療, 健康 / 社會 / 環境
    *   **Yes123:** 醫療／保健服務全部, 醫療專業全部

### 11. 教育/學術/輔導 (Education/Academic/Counseling)
*   **描述:** 包含教育教學、學術研究、專業輔導與培訓等職位。
*   **原始平台分類映射:**
    *   **104:** 學術／教育／輔導類
    *   **1111:** 學術研究／教育師資, 幼教才藝／補習進修
    *   **Cakeresume:** 教育 / 培訓 / 招聘
    *   **Yes123:** 才藝輔導全部, 教育／輔導全部, 學術研究全部

### 12. 餐飲/旅遊/生活服務 (Hospitality/Travel/Life Services)
*   **描述:** 涵蓋餐飲、飯店、旅遊、美容美髮及其他生活服務相關職位。
*   **原始平台分類映射:**
    *   **104:** 餐飲／旅遊 ／美容美髮類
    *   **1111:** 美容美髮／餐飲旅遊
    *   **Cakeresume:** 時尚 / 奢華 / 美膚 / 生活方式, 食品和飲料, 飯店 / 旅遊 / 休閒, 服務
    *   **Yes123:** 美容美髮全部, 旅遊休閒全部, 餐飲服務全部

### 13. 物流/倉儲/採購 (Logistics/Warehousing/Procurement)
*   **描述:** 負責供應鏈管理、倉儲、採購與運輸等職位。
*   **原始平台分類映射:**
    *   **104:** 資材／物流／運輸類
    *   **1111:** 採購／物流／品質檢測 (主要指採購/物流)
    *   **Cakeresume:** 移動 / 運輸
    *   **Yes123:** 倉管／資材／採購全部, 運輸物流全部

### 14. 操作/技術/維修 (Operations/Technical/Maintenance)
*   **描述:** 涉及設備操作、技術支援、機械維修與保養等職位。
*   **原始平台分類映射:**
    *   **104:** 操作／技術／維修類
    *   **1111:** 操作／維修／技術服務
    *   **Yes123:** 維修／技術服務全部, 操作／技術全部

### 15. 公共事務/農林漁牧/其他 (Public Affairs/Agriculture/Other)
*   **描述:** 包含軍警消、保全、農林漁牧及其他難以歸類或綜合性職位。
*   **原始平台分類映射:**
    *   **104:** 軍警消／保全類, 其他職類
    *   **1111:** 軍警消防／保全相關, 生活服務／農林漁牧
    *   **Cakeresume:** 農林漁牧業, 非營利 / 社團組織
    *   **Yes123:** 保全／樓管全部, 消防／軍警全部, 農林漁牧相關全部, 其他職務人員全部



================================================
FILE: crawler/database/category_classification_data/major_categories.json
================================================
[
    {"source_category_id": "MAJOR_IT_SOFTWARE", "source_category_name": "資訊/軟體/網路", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_ENGINEERING_RD", "source_category_name": "工程研發", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_PRODUCTION_QC", "source_category_name": "生產製造/品管", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_BUSINESS_ADMIN", "source_category_name": "商業/管理/行政", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_FINANCE_LEGAL", "source_category_name": "財會/金融/法務", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_MARKETING_PROJECT", "source_category_name": "行銷/企劃/專案", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_SALES_CS_TRADE", "source_category_name": "業務/客服/貿易", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_DESIGN_MEDIA_ARTS", "source_category_name": "設計/媒體/藝術", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_CONSTRUCTION_SPACE", "source_category_name": "營建/建築/空間", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_HEALTHCARE_BIOTECH", "source_category_name": "醫療/生技/健康", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_EDUCATION_ACADEMIC", "source_category_name": "教育/學術/輔導", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_HOSPITALITY_TRAVEL", "source_category_name": "餐飲/旅遊/生活服務", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_LOGISTICS_PROCUREMENT", "source_category_name": "物流/倉儲/採購", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_OPERATIONS_MAINTENANCE", "source_category_name": "操作/技術/維修", "parent_source_id": null, "source_platform": "platform_104"},
    {"source_category_id": "MAJOR_PUBLIC_AFFAIRS_OTHER", "source_category_name": "公共事務/農林漁牧/其他", "parent_source_id": null, "source_platform": "platform_104"}
]


================================================
FILE: crawler/database/category_classification_data/mapping.json
================================================
{
    "platform_104": {
        "資訊軟體系統類": "MAJOR_IT_SOFTWARE",
        "研發相關類": "MAJOR_ENGINEERING_RD",
        "生產製造／品管／環衛類": "MAJOR_PRODUCTION_QC",
        "經營／人資類": "MAJOR_BUSINESS_ADMIN",
        "行政／總務／法務類": "MAJOR_BUSINESS_ADMIN",
        "財會／金融專業類": "MAJOR_FINANCE_LEGAL",
        "行銷／企劃／專案管理類": "MAJOR_MARKETING_PROJECT",
        "客服／門市／業務／貿易類": "MAJOR_SALES_CS_TRADE",
        "傳播藝術／設計類": "MAJOR_DESIGN_MEDIA_ARTS",
        "文字／傳媒工作類": "MAJOR_DESIGN_MEDIA_ARTS",
        "營建／製圖類": "MAJOR_CONSTRUCTION_SPACE",
        "醫療／保健服務類": "MAJOR_HEALTHCARE_BIOTECH",
        "學術／教育／輔導類": "MAJOR_EDUCATION_ACADEMIC",
        "餐飲／旅遊 ／美容美髮類": "MAJOR_HOSPITALITY_TRAVEL",
        "資材／物流／運輸類": "MAJOR_LOGISTICS_PROCUREMENT",
        "操作／技術／維修類": "MAJOR_OPERATIONS_MAINTENANCE",
        "軍警消／保全類": "MAJOR_PUBLIC_AFFAIRS_OTHER",
        "其他職類": "MAJOR_PUBLIC_AFFAIRS_OTHER"
    },
    "platform_1111": {
        "電腦系統／資訊／軟硬體": "MAJOR_IT_SOFTWARE",
        "光電IC／電子通訊": "MAJOR_IT_SOFTWARE",
        "機械模具／生產製程": "MAJOR_PRODUCTION_QC",
        "管理幕僚／人資／行政": "MAJOR_BUSINESS_ADMIN",
        "金融保險／財會／稽核": "MAJOR_FINANCE_LEGAL",
        "法務專利／顧問諮詢": "MAJOR_FINANCE_LEGAL",
        "行銷／企劃／專案管理": "MAJOR_MARKETING_PROJECT",
        "業務／貿易／客服／門市": "MAJOR_SALES_CS_TRADE",
        "影視傳媒／出版翻譯": "MAJOR_DESIGN_MEDIA_ARTS",
        "美編設計": "MAJOR_DESIGN_MEDIA_ARTS",
        "營建／製圖／施作": "MAJOR_CONSTRUCTION_SPACE",
        "醫療／護理／保健": "MAJOR_HEALTHCARE_BIOTECH",
        "生物科技／化學製藥": "MAJOR_HEALTHCARE_BIOTECH",
        "學術研究／教育師資": "MAJOR_EDUCATION_ACADEMIC",
        "幼教才藝／補習進修": "MAJOR_EDUCATION_ACADEMIC",
        "美容美髮／餐飲旅遊": "MAJOR_HOSPITALITY_TRAVEL",
        "採購／物流／品質檢測": "MAJOR_LOGISTICS_PROCUREMENT",
        "操作／維修／技術服務": "MAJOR_OPERATIONS_MAINTENANCE",
        "軍警消防／保全相關": "MAJOR_PUBLIC_AFFAIRS_OTHER",
        "生活服務／農林漁牧": "MAJOR_PUBLIC_AFFAIRS_OTHER"
    },
    "platform_cakeresume": {
            "生物、醫藥": "MAJOR_HEALTHCARE_BIOTECH",
            "建設": "MAJOR_CONSTRUCTION_SPACE",
            "客戶服務": "MAJOR_SALES_CS_TRADE",
            "設計": "MAJOR_DESIGN_MEDIA_ARTS",
            "教育": "MAJOR_EDUCATION_ACADEMIC",
            "工程研發": "MAJOR_ENGINEERING_RD",
            "金融": "MAJOR_FINANCE_LEGAL",
            "餐飲服務 / 食品相關": "MAJOR_HOSPITALITY_TRAVEL",
            "遊戲製作": "MAJOR_DESIGN_MEDIA_ARTS",
            "人資": "MAJOR_BUSINESS_ADMIN",
            "軟體": "MAJOR_IT_SOFTWARE",
            "法律": "MAJOR_FINANCE_LEGAL",
            "物流、貿易": "MAJOR_LOGISTICS_PROCUREMENT",
            "經營、管理、商務": "MAJOR_BUSINESS_ADMIN",
            "製造": "MAJOR_PRODUCTION_QC",
            "行銷": "MAJOR_MARKETING_PROJECT",
            "文字編輯、新聞採訪、藝術演藝": "MAJOR_DESIGN_MEDIA_ARTS",
            "其他": "MAJOR_PUBLIC_AFFAIRS_OTHER",
            "政府機關": "MAJOR_PUBLIC_AFFAIRS_OTHER",
            "業務": "MAJOR_SALES_CS_TRADE"
        },
    "platform_yes123": {
        "MIS／網管全部": "MAJOR_IT_SOFTWARE",
        "軟體／工程全部": "MAJOR_IT_SOFTWARE",
        "工程研發全部": "MAJOR_ENGINEERING_RD",
        "化工／材料研發全部": "MAJOR_ENGINEERING_RD",
        "生技研發全部": "MAJOR_ENGINEERING_RD",
        "生產管理全部": "MAJOR_PRODUCTION_QC",
        "品管／品保全部": "MAJOR_QC",
        "製程規劃全部": "MAJOR_PRODUCTION_QC",
        "環境安全衛生全部": "MAJOR_PRODUCTION_QC",
        "行政／總務全部": "MAJOR_BUSINESS_ADMIN",
        "人力資源全部": "MAJOR_BUSINESS_ADMIN",
        "經營／幕僚全部": "MAJOR_BUSINESS_ADMIN",
        "金融／保險全部": "MAJOR_FINANCE_LEGAL",
        "財會／稅務全部": "MAJOR_FINANCE_LEGAL",
        "法務／智財全部": "MAJOR_FINANCE_LEGAL",
        "行銷／企劃全部": "MAJOR_MARKETING_PROJECT",
        "專案管理全部": "MAJOR_MARKETING_PROJECT",
        "門市幹部全部": "MAJOR_SALES_CS_TRADE",
        "客戶服務全部": "MAJOR_SALES_CS_TRADE",
        "國際貿易全部": "MAJOR_SALES_CS_TRADE",
        "業務銷售全部": "MAJOR_SALES_CS_TRADE",
        "文字編譯全部": "MAJOR_DESIGN_MEDIA_ARTS",
        "採訪類人員全部": "MAJOR_DESIGN_MEDIA_ARTS",
        "設計人員全部": "MAJOR_DESIGN_MEDIA_ARTS",
        "傳播藝術全部": "MAJOR_DESIGN_MEDIA_ARTS",
        "製圖／測量全部": "MAJOR_CONSTRUCTION_SPACE",
        "營建施作全部": "MAJOR_CONSTRUCTION_SPACE",
        "營建規劃全部": "MAJOR_CONSTRUCTION_SPACE",
        "醫療／保健服務全部": "MAJOR_HEALTHCARE_BIOTECH",
        "醫療專業全部": "MAJOR_HEALTHCARE_BIOTECH",
        "才藝輔導全部": "MAJOR_EDUCATION_ACADEMIC",
        "教育／輔導全部": "MAJOR_EDUCATION_ACADEMIC",
        "學術研究全部": "MAJOR_EDUCATION_ACADEMIC",
        "美容美髮全部": "MAJOR_HOSPITALITY_TRAVEL",
        "旅遊休閒全部": "MAJOR_HOSPITALITY_TRAVEL",
        "餐飲服務全部": "MAJOR_HOSPITALITY_TRAVEL",
        "倉管／資材／採購全部": "MAJOR_LOGISTICS_PROCUREMENT",
        "運輸物流全部": "MAJOR_LOGISTICS_PROCUREMENT",
        "維修／技術服務全部": "MAJOR_OPERATIONS_MAINTENANCE",
        "操作／技術全部": "MAJOR_OPERATIONS_MAINTENANCE",
        "保全／樓管全部": "MAJOR_PUBLIC_AFFAIRS_OTHER",
        "消防／軍警全部": "MAJOR_PUBLIC_AFFAIRS_OTHER",
        "農林漁牧相關全部": "MAJOR_PUBLIC_AFFAIRS_OTHER",
        "其他職務人員全部": "MAJOR_PUBLIC_AFFAIRS_OTHER"
    }
}


================================================
FILE: crawler/database/category_classification_data/root_categories.json
================================================
[Binary file]


================================================
FILE: crawler/database/category_classification_data/verify_classification.py
================================================
import os
import sys

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))

from crawler.database.schemas import SourcePlatform
from crawler.database.connection import initialize_database
from crawler.database.repository import get_source_categories
import structlog

# Configure logging for the script (can be removed if only using print)
structlog.configure(
    processors=[
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
)
logger = structlog.get_logger(__name__)

def main():
    print("Initializing database connection...")
    initialize_database()
    print("Database initialized.")

    platform_to_check = SourcePlatform.PLATFORM_104
    categories_to_check = [
        "資訊軟體系統類",
        "經營／人資類",
        "財會／金融專業類",
        "其他職類",
    ]

    print(f"Verifying parent_source_id for {platform_to_check.value} categories...")

    # Fetch all categories for the platform and then filter by name
    all_platform_categories = get_source_categories(platform_to_check)
    
    found_categories = {}
    for cat in all_platform_categories:
        found_categories[cat.source_category_name] = cat

    for category_name in categories_to_check:
        category = found_categories.get(category_name)
        if category:
            print(
                f"Category: {category.source_category_name}, "
                f"Source ID: {category.source_category_id}, "
                f"Parent Source ID: {category.parent_source_id}"
            )
        else:
            print(f"Category '{category_name}' not found for {platform_to_check.value}.")

    print("Verification complete.")

if __name__ == "__main__":
    main()


================================================
FILE: crawler/database/scripts/fix_salary_data.py
================================================
from crawler.database.connection import get_session
from crawler.database.models import Job
from crawler.database.schemas import SourcePlatform
from crawler.utils.salary_parser import parse_salary_text
from crawler.project_1111.parser_apidata_1111 import derive_salary_type

try:
    with get_session() as session:
        # Query for jobs that match the original criteria
        # You can modify this filter as needed for actual data updates
        jobs_to_update = session.query(Job).filter(
            Job.source_platform == SourcePlatform.PLATFORM_1111,
            Job.salary_text.isnot(None) # Ensure salary_text is not null
        ).all()

        print(f"Found {len(jobs_to_update)} jobs to process.")

        for job in jobs_to_update:
            min_salary, max_salary = parse_salary_text(job.salary_text)
            # Assuming job.job_type is already populated from the crawler
            salary_type = derive_salary_type(job.salary_text, min_salary, job.job_type)

            if min_salary is not None or salary_type is not None: # Update if either min_salary is parsed or type is derived
                job.salary_min = min_salary
                job.salary_max = max_salary
                job.salary_type = salary_type
                print(f"Updating job_id: {job.id} - salary_text: '{job.salary_text}' - salary_min: {min_salary}, salary_max: {max_salary}, salary_type: {salary_type}")
            else:
                print(f"Skipping job_id: {job.id} - Could not parse salary from '{job.salary_text}'")

        print("Successfully processed salary information for matching jobs.")

except Exception as e:
    print(f"An error occurred: {e}")



================================================
FILE: crawler/database/scripts/get_category_ids.py
================================================
import os
import sys
import structlog
import pandas as pd

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.database.connection import get_session, initialize_database
from crawler.database.models import CategorySource
from crawler.logging_config import configure_logging

# Configure logging at the script level
configure_logging()
logger = structlog.get_logger(__name__)


def get_source_category_ids():
    """
    Fetches all category IDs and names from the database and returns them as a Pandas DataFrame.
    """
    try:
        with get_session() as session:
            categories = session.query(CategorySource).all()
            data = [
                {
                    "parent_source_id": cat.parent_source_id,
                    "source_category_id": cat.source_category_id,
                    "source_category_name": cat.source_category_name,
                }
                for cat in categories
            ]
            df = pd.DataFrame(data)
            logger.info("Successfully fetched source category IDs.", count=len(df))
            return df
    except Exception as e:
        logger.error("Error fetching source_category_ids with ORM.", error=e, exc_info=True)
        return pd.DataFrame()  # Return an empty DataFrame on error


if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.get_category_ids
    
    # Ensure the database is initialized before fetching data
    initialize_database()

    ids_df = get_source_category_ids()
    if not ids_df.empty:
        logger.info(
            "Source Category IDs fetched successfully.",
            dataframe_head=ids_df.head().to_dict("records")
        )
    else:
        logger.warning("Could not fetch any source category IDs.")


================================================
FILE: crawler/database/scripts/pandas_sql_config.py
================================================
import os
import sys
import pandas as pd
import structlog
from sqlalchemy import create_engine

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.logging_config import configure_logging
from crawler.config import (
    MYSQL_HOST,
    MYSQL_PORT,
    MYSQL_ACCOUNT,
    MYSQL_PASSWORD,
    MYSQL_DATABASE,
)

configure_logging()
logger = structlog.get_logger(__name__)


def main():
    """
    Demonstrates connecting to the database and reading data using Pandas.
    """
    db_url = (
        f"mysql+mysqlconnector://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}"
    )

    engine = create_engine(db_url)

    try:
        table_name = "tb_category_source"
        logger.info(
            "Attempting to read data from database using Pandas.", table=table_name
        )
        df = pd.read_sql(f"SELECT * FROM {table_name} LIMIT 5", engine)

        logger.info(
            "Successfully read data from database.", table=table_name, rows_read=len(df)
        )
        logger.info("DataFrame head:", dataframe_head=df.head().to_dict("records"))

    except Exception as e:
        logger.error(
            "An error occurred during database connection or query.",
            error=e,
            exc_info=True,
        )

    finally:
        engine.dispose()
        logger.info("Database engine disposed.")


if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.pandas_sql_config
    main()


================================================
FILE: crawler/database/scripts/temp_count_db.py
================================================
import os
import sys
import structlog
from sqlalchemy import text

# Add project root to the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from crawler.database.connection import get_session, initialize_database
from crawler.logging_config import configure_logging

# Configure logging at the script level
configure_logging()
logger = structlog.get_logger(__name__)


def main():
    """
    Connects to the database and counts the records in key tables.
    """
    logger.info("Starting database count check.")
    try:
        # Ensure the database and tables exist before counting
        initialize_database()
        
        with get_session() as session:
            category_count = session.execute(
                text("SELECT COUNT(*) FROM tb_category_source")
            ).scalar_one_or_none() or 0
            url_count = session.execute(text("SELECT COUNT(*) FROM tb_urls")).scalar_one_or_none() or 0

            logger.info(
                "Database record counts retrieved successfully.",
                category_count=category_count,
                url_count=url_count,
            )

    except Exception as e:
        logger.error(
            "An error occurred while counting database records.", error=e, exc_info=True
        )

if __name__ == "__main__":
    # To run this script for the test database, set the environment variable:
    # CRAWLER_DB_NAME=test_db python -m crawler.database.scripts.temp_count_db
    main()


================================================
FILE: crawler/geocoding/client.py
================================================
import structlog
from typing import Optional, Dict
import random
import time

from crawler.config import (
    URL_CRAWLER_SLEEP_MIN_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
)
from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

def geocode_address(address: str) -> Optional[Dict[str, float]]:
    """
    模擬地理編碼服務，將地址轉換為經緯度。
    在實際應用中，這裡會呼叫第三方地理編碼 API (例如 Google Maps Geocoding API)。
    """
    logger.info("Attempting to geocode address.", address=address)
    
    # 模擬網路延遲
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    time.sleep(sleep_time)

    # 簡單的模擬經緯度，實際應根據地址返回真實數據
    if "台北" in address:
        return {"latitude": 25.032969, "longitude": 121.564559} # 台北市政府
    elif "台中" in address:
        return {"latitude": 24.137135, "longitude": 120.687138} # 台中市政府
    elif "高雄" in address:
        return {"latitude": 22.627278, "longitude": 120.301435} # 高雄市政府
    else:
        logger.warning("Address not found in mock geocoding service.", address=address)
        return None



================================================
FILE: crawler/geocoding/task.py
================================================
import structlog

from crawler.worker import app
from crawler.database.models import SourcePlatform, JobLocationPydantic
from crawler.database.repository import upsert_job_locations
from crawler.geocoding.client import geocode_address

logger = structlog.get_logger(__name__)


@app.task()
def geocode_job_location(
    source_platform: str, source_job_id: str, location_text: str
) -> None:
    """
    Celery 任務：接收職缺的地理位置文字，呼叫地理編碼服務，
    並將經緯度資訊儲存到資料庫。
    """
    logger.info(
        "Starting geocoding for job location.",
        source_platform=source_platform,
        source_job_id=source_job_id,
        location_text=location_text,
    )

    try:
        geocoded_data = geocode_address(location_text)

        if geocoded_data:
            # 只有當任務不是在 eager 模式下執行時才寫入資料庫
            if not app.conf.task_always_eager:
                job_location_pydantic = JobLocationPydantic(
                    source_platform=SourcePlatform(source_platform),
                    source_job_id=source_job_id,
                    latitude=str(geocoded_data["latitude"]),
                    longitude=str(geocoded_data["longitude"]),
                )
                upsert_job_locations([job_location_pydantic])
                logger.info(
                    "Geocoding successful and data upserted.",
                    source_platform=source_platform,
                    source_job_id=source_job_id,
                    latitude=geocoded_data["latitude"],
                    longitude=geocoded_data["longitude"],
                )
            else:
                logger.info(
                    "Geocoding successful (eager mode), skipping database upsert.",
                    source_platform=source_platform,
                    source_job_id=source_job_id,
                    latitude=geocoded_data["latitude"],
                    longitude=geocoded_data["longitude"],
                )
        else:
            logger.warning(
                "Geocoding failed for location.",
                source_platform=source_platform,
                source_job_id=source_job_id,
                location_text=location_text,
            )

    except Exception as e:
        logger.error(
            "An unexpected error occurred during geocoding task.",
            source_platform=source_platform,
            source_job_id=source_job_id,
            location_text=location_text,
            error=e,
            exc_info=True,
        )


if __name__ == "__main__":
    from crawler.database.connection import initialize_database
    initialize_database()

    # 測試範例
    geocode_job_location("platform_104", "test_job_1", "台北市")
    geocode_job_location("platform_1111", "test_job_2", "台中市")
    geocode_job_location("platform_cakeresume", "test_job_3", "高雄市")
    geocode_job_location("platform_yes123", "test_job_4", "未知地點")



================================================
FILE: crawler/project_104/104人力銀行_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import random
import json
import requests
from tqdm import tqdm
import pandas as pd
from collections import deque

WEB_NAME = '104_人力銀行'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Referer': 'https://www.104.com.tw/jobs/search',
    }


## 取得網站所有職業總覽

# 1. 取得 JSON 資料
# jobcat 檔案名稱
file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
url_JobCat = "https://static.104.com.tw/category-tool/json/JobCat.json"

response_jobcat = requests.get(url_JobCat, headers=HEADERS, timeout=10)
response_jobcat.raise_for_status()
jobcat_data = response_jobcat.json()
with open(file_jobcat_json, "w", encoding="utf-8") as f:
    json.dump(jobcat_data, f, ensure_ascii=False, indent=4)
print(f"職業總覽資料已儲存為 {file_jobcat_json}")


# 2. 直接將 requests 取得的資料傳入遞迴函式
def flatten_jobcat_recursive(node_list, parent_des=None, parent_no=None):
    flat_list = []
    for node in node_list:
        row = {
            "parent_code": parent_no,
            "parent_name": parent_des,
            "job_code": node.get("no"),
            "job_name": node.get("des"),
        }
        flat_list.append(row)
        if "n" in node and node["n"]:
            children_list = flatten_jobcat_recursive(
                node_list=node["n"],
                parent_des=node.get("des"),
                parent_no=node.get("no"),
            )
            flat_list.extend(children_list)
    return flat_list


# 3. 執行結果轉為 DataFrame
flattened_data = flatten_jobcat_recursive(jobcat_data)
df_jobcat = pd.DataFrame(flattened_data)
df_jobcat = df_jobcat[df_jobcat["parent_code"].notnull()]
df_jobcat_sorted = df_jobcat.sort_values(by="job_code")
df_jobcat_sorted.to_excel(f"{WEB_NAME}_category.xlsx", index=False)
print(f"職業總覽資料已轉換為 '{WEB_NAME}_category.xlsx'")

# 篩選出 IT 相關的工作
# 篩選出 job_code 以 '140' 開頭的行
mask = df_jobcat_sorted["job_code"].astype(str).str.startswith("2007")
df_it_jobs = df_jobcat_sorted[mask]
# df_it_jobs
# Output:
#   職業總覽資料已儲存為 104_人力銀行_jobcat_json.txt

#   職業總覽資料已轉換為 '104_人力銀行_category.xlsx'


# 產生 104 人力銀行網址 https://www.104.com.tw 根據提供的 (關鍵字和職缺類別) 轉換為職缺網址

def catch_104_url (KEYWORDS, CATEGORY, ORDER=None):
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (int, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。
    """

    BASE_URL = "https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s"

    param_fragments = []
    if ORDER is not None:
        param_fragments.append(f"order={ORDER}")
    if KEYWORDS:
        param_fragments.append(f"keyword={KEYWORDS}")
    if CATEGORY:
        param_fragments.append(f"jobcat={CATEGORY}")
    query_string = "&".join(param_fragments)

    return f"{BASE_URL}{query_string}&page="


KEYWORDS_STR = "雲端工程師"
JOBCAT_CODE = "2007000000"
ORDER_SETTING = 16     # 15 (符合度高)、  16 (最近更新)


# # 測試範例
# url_1 = catch_104_url (KEYWORDS_STR, JOBCAT_CODE, ORDER_SETTING)
# print (url_1)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&order=15&keyword=雲端工程師&jobcat=2007000000&page=

# url_2 = catch_104_url (KEYWORDS_STR, "")
# print (url_2)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&keyword=雲端工程師&page=

# url_3 = catch_104_url ("", JOBCAT_CODE, ORDER_SETTING)
# print (url_3)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&order=15&jobcat=2007000000&page=

# url_4 = catch_104_url ("","")
# print (url_4)  # https://www.104.com.tw/jobs/search/?jobsource=joblist_search&mode=s&page=

#  從 api 網址獲取工作職缺的網址

def fetch_104_job_url (_CODE, KEYWORD):
    """
    這個函數會遍歷多個頁面，並從每個頁面中提取工作職缺的網址，將其存儲在一個集合中以避免重複。
    使用 tqdm 顯示進度條，並在每次請求之間隨機延遲以避免過於頻繁的請求。

    參數:
    _CODE (str): 職缺類別的代碼。
    KEYWORD (str): 搜尋的關鍵字。

    返回:
    list: 包含所有獲取到的工作職缺網址的列表。

    """

    BASE_URL = "https://www.104.com.tw/jobs/search/api/jobs"

    PAGE = 1
    MAX_PAGE = 10
    PAGE_SIZE = 30
    ORDER_SETTING = 15   # 15 (符合度高)、  16 (最近更新)
    
    MAX_LENGTH = 4
    recent_counts = deque (maxlen=MAX_LENGTH)
    job_url_set = set ()  # 用於存儲唯一的職缺網址

    with requests.Session () as session, tqdm (total=MAX_PAGE, desc="104 職缺列表", unit="PAGE", leave=True) as pbar:
        while True:
            params = {
                'jobsource': 'm_joblist_search',
                'page': PAGE,
                'pagesize': PAGE_SIZE,
                'order': ORDER_SETTING, 
                'jobcat': _CODE,
                'keyword': KEYWORD,
            }

            response = requests.get (BASE_URL, headers=HEADERS, params=params, timeout=20)
            api_job_urls = response.json ()['data']
            for job_url in api_job_urls:
                job_url_set.add (job_url ['link']['job'])

            # 檢查是否有新資料
            total_jobs = len (job_url_set) 
            recent_counts.append (total_jobs)
            if len (recent_counts) == MAX_LENGTH and len (set (recent_counts)) == 1:
                print (f"連續 {MAX_LENGTH} 次沒有新資料，提前結束。")
                break
            
            time.sleep (random.uniform (0.5, 1.5))
            
            pbar.set_postfix_str (f"目前頁面 {PAGE}, 最大頁數: {MAX_PAGE}")
            pbar.update (1)

            PAGE = PAGE + 1  # 更新頁碼
            if PAGE >= MAX_PAGE:
                MAX_PAGE = PAGE + 1 
                pbar.total = MAX_PAGE

    modified_job_url_set = {f"https://www.104.com.tw/job/ajax/content/{url.split ('/')[-1]}" for url in job_url_set}
    print (f"共獲取到 {len (job_url_set)} 筆職缺資料。")
    return list (modified_job_url_set)


# 測試範例
# JOBCAT_CODE = "2007000000"
# KEYWORDS = "雲端工程師"
# jobs_url = fetch_104_job_url(JOBCAT_CODE, KEYWORDS)
# jobs_url [0]

# 從指定的職缺網址獲取職缺的相關數據

def fetch_104_job_data(job_url):
    """
    這個函數會發送 GET 請求到提供的職缺網址，並使用 BeautifulSoup 解析返回的 HTML 文檔。
    它會從頁面中的 JavaScript 代碼中提取職缺的元數據，並將其轉換為 Pandas DataFrame 格式。

    參數:
    job_url (str): 職缺的網址。

    返回:
    pd.DataFrame: 包含職缺詳細信息的 DataFrame，包括職缺網址、公司名稱、公司網址及其他職缺網址。
    """
    response = requests.get(job_url, headers=HEADERS)
    jobMetaData = response.json()["data"]
    df = pd.json_normalize(jobMetaData)
    return df


# 測試範例
# job_url = "https://www.104.com.tw/job/ajax/content/8k4lp"   # jobs_url [0]
# job_data = fetch_104_job_data(job_url)
# job_data

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime ('%Y-%m-%d', time.localtime (time.time ()))
JOBCAT_CODE = "2007000000"
KEYWORDS = "雲端工程師"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{JOBCAT_CODE}"

print ( f"開始執行 {FILE_NAME}" )
job_data_list = []
job_urls = fetch_104_job_url(JOBCAT_CODE, KEYWORDS)        # 列出 104 人力銀行 - 職缺網址列表 


all_jobs_df = pd.DataFrame ()  # 初始化一個空的 DataFrame

for url in tqdm (job_urls, desc="Fetching job data", unit="job"):
    df_job_data = fetch_104_job_data(url)
    all_jobs_df = pd.concat ([all_jobs_df, df_job_data], ignore_index=True)


print(all_jobs_df.shape)

all_jobs_df.head(1)
# Output:
#   開始執行 (2025-06-16)_104_人力銀行_雲端工程師_2007000000

#   104 職缺列表:  95%|█████████▌| 38/40 [01:00<00:03,  1.59s/PAGE, 目前頁面 38, 最大頁數: 39]

#   連續 4 次沒有新資料，提前結束。

#   共獲取到 1034 筆職缺資料。

#   Fetching job data: 100%|██████████| 1034/1034 [03:30<00:00,  4.91job/s]
#   (1034, 79)

#   

#     switch                                           custLogo postalCode  \

#   0     on  https://static.104.com.tw/b_profile/cust_pictu...        105   

#   

#     closeDate industry       custNo  \

#   0             人力仲介代徵  11111119000   

#   

#                                              reportUrl  industryNo employees  \

#   0  https://www.104.com.tw/feedback?category=2&cus...  1009001001      820人   

#   

#      chinaCorp  ... jobDetail.startWorkingDay jobDetail.hireType  \

#   0      False  ...                        不限                  0   

#   

#     jobDetail.delegatedRecruit jobDetail.needEmp jobDetail.landmark  \

#   0                                           1人                      

#   

#     jobDetail.remoteWork interactionRecord.lastProcessedResumeAtTime  \

#   0                 None                                        None   

#   

#     interactionRecord.nowTimestamp  jobDetail.remoteWork.type  \

#   0                     1750026676                        NaN   

#   

#     jobDetail.remoteWork.description  

#   0                              NaN  

#   

#   [1 rows x 79 columns]

# all_jobs_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
# Output:
#   已將所有職缺資料儲存到 (2025-06-16)_104_人力銀行_雲端工程師_2007000000.xlsx


all_jobs_df.columns
# Output:
#   Index(['switch', 'custLogo', 'postalCode', 'closeDate', 'industry', 'custNo',

#          'reportUrl', 'industryNo', 'employees', 'chinaCorp',

#          'corpImageRight.corpImageRight.imageUrl',

#          'corpImageRight.corpImageRight.link', 'header.corpImageTop.imageUrl',

#          'header.corpImageTop.link', 'header.jobName', 'header.appearDate',

#          'header.custName', 'header.custUrl', 'header.analysisType',

#          'header.analysisUrl', 'header.isSaved', 'header.isApplied',

#          'header.applyDate', 'header.userApplyCount', 'header.isActivelyHiring',

#          'contact.hrName', 'contact.email', 'contact.visit', 'contact.phone',

#          'contact.other', 'contact.reply', 'environmentPic.environmentPic',

#          'environmentPic.corpImageBottom.imageUrl',

#          'environmentPic.corpImageBottom.link', 'condition.acceptRole.role',

#          'condition.acceptRole.disRole.needHandicapCompendium',

#          'condition.acceptRole.disRole.disability', 'condition.workExp',

#          'condition.edu', 'condition.major', 'condition.language',

#          'condition.localLanguage', 'condition.specialty', 'condition.skill',

#          'condition.certificate', 'condition.driverLicense', 'condition.other',

#          'welfare.tag', 'welfare.welfare', 'welfare.legalTag',

#          'jobDetail.jobDescription', 'jobDetail.jobCategory', 'jobDetail.salary',

#          'jobDetail.salaryMin', 'jobDetail.salaryMax', 'jobDetail.salaryType',

#          'jobDetail.jobType', 'jobDetail.workType', 'jobDetail.addressNo',

#          'jobDetail.addressRegion', 'jobDetail.addressArea',

#          'jobDetail.addressDetail', 'jobDetail.industryArea',

#          'jobDetail.longitude', 'jobDetail.latitude', 'jobDetail.manageResp',

#          'jobDetail.businessTrip', 'jobDetail.workPeriod',

#          'jobDetail.vacationPolicy', 'jobDetail.startWorkingDay',

#          'jobDetail.hireType', 'jobDetail.delegatedRecruit', 'jobDetail.needEmp',

#          'jobDetail.landmark', 'jobDetail.remoteWork',

#          'interactionRecord.lastProcessedResumeAtTime',

#          'interactionRecord.nowTimestamp', 'jobDetail.remoteWork.type',

#          'jobDetail.remoteWork.description'],

#         dtype='object')

column_names = [
    {"序號": 1, "英文": "switch", "中文": "內部切換/開關"},
    {"序號": 2, "英文": "custLogo", "中文": "公司Logo"},
    {"序號": 3, "英文": "postalCode", "中文": "郵遞區號"},
    {"序號": 4, "英文": "closeDate", "中文": "截止日期"},
    {"序號": 5, "英文": "industry", "中文": "產業類別"},
    {"序號": 6, "英文": "custNo", "中文": "公司代號"},
    {"序號": 7, "英文": "reportUrl", "中文": "檢舉職務網址"},
    {"序號": 8, "英文": "industryNo", "中文": "產業代號"},
    {"序號": 9, "英文": "employees", "中文": "員工人數"},
    {"序號": 10, "英文": "chinaCorp", "中文": "中國大陸關係企業"},
    {
        "序號": 11,
        "英文": "corpImageRight.corpImageRight.imageUrl",
        "中文": "右側公司圖片網址",
    },
    {
        "序號": 12,
        "英文": "corpImageRight.corpImageRight.link",
        "中文": "右側公司圖片連結",
    },
    {"序號": 13, "英文": "header.corpImageTop.imageUrl", "中文": "頂部公司圖片網址"},
    {"序號": 14, "英文": "header.corpImageTop.link", "中文": "頂部公司圖片連結"},
    {"序號": 15, "英文": "header.jobName", "中文": "職務名稱"},
    {"序號": 16, "英文": "header.appearDate", "中文": "更新日期"},
    {"序號": 17, "英文": "header.custName", "中文": "公司名稱"},
    {"序號": 18, "英文": "header.custUrl", "中文": "公司頁面網址"},
    {"序號": 19, "英文": "header.analysisType", "中文": "應徵分析類型"},
    {"序號": 20, "英文": "header.analysisUrl", "中文": "應徵分析網址"},
    {"序號": 21, "英文": "header.isSaved", "中文": "是否已儲存"},
    {"序號": 22, "英文": "header.isApplied", "中文": "是否已應徵"},
    {"序號": 23, "英文": "header.applyDate", "中文": "應徵日期"},
    {"序號": 24, "英文": "header.userApplyCount", "中文": "使用者應徵次數"},
    {"序號": 25, "英文": "header.isActivelyHiring", "中文": "是否為積極徵才"},
    {"序號": 26, "英文": "contact.hrName", "中文": "聯絡人"},
    {"序號": 27, "英文": "contact.email", "中文": "聯絡E-mail"},
    {"序號": 28, "英文": "contact.visit", "中文": "親洽地址"},
    {"序號": 29, "英文": "contact.phone", "中文": "聯絡電話"},
    {"序號": 30, "英文": "contact.other", "中文": "其他聯絡方式"},
    {"序號": 31, "英文": "contact.reply", "中文": "應徵回覆率/時間"},
    {"序號": 32, "英文": "environmentPic.environmentPic", "中文": "公司環境照片"},
    {
        "序號": 33,
        "英文": "environmentPic.corpImageBottom.imageUrl",
        "中文": "底部公司圖片網址",
    },
    {
        "序號": 34,
        "英文": "environmentPic.corpImageBottom.link",
        "中文": "底部公司圖片連結",
    },
    {"序號": 35, "英文": "condition.acceptRole.role", "中文": "接受身份"},
    {
        "序號": 36,
        "英文": "condition.acceptRole.disRole.needHandicapCompendium",
        "中文": "需附身心障礙證明",
    },
    {
        "序號": 37,
        "英文": "condition.acceptRole.disRole.disability",
        "中文": "身心障礙類別",
    },
    {"序號": 38, "英文": "condition.workExp", "中文": "工作經歷"},
    {"序號": 39, "英文": "condition.edu", "中文": "學歷要求"},
    {"序號": 40, "英文": "condition.major", "中文": "科系要求"},
    {"序號": 41, "英文": "condition.language", "中文": "語文條件"},
    {"序號": 42, "英文": "condition.localLanguage", "中文": "本國語言條件"},
    {"序號": 43, "英文": "condition.specialty", "中文": "工作技能"},
    {"序號": 44, "英文": "condition.skill", "中文": "擅長工具"},
    {"序號": 45, "英文": "condition.certificate", "中文": "具備證照"},
    {"序號": 46, "英文": "condition.driverLicense", "中文": "具備駕照"},
    {"序號": 47, "英文": "condition.other", "中文": "其他條件"},
    {"序號": 48, "英文": "welfare.tag", "中文": "福利標籤"},
    {"序號": 49, "英文": "welfare.welfare", "中文": "公司福利(詳細說明)"},
    {"序號": 50, "英文": "welfare.legalTag", "中文": "法定福利標籤"},
    {"序號": 51, "英文": "jobDetail.jobDescription", "中文": "工作內容"},
    {"序號": 52, "英文": "jobDetail.jobCategory", "中文": "職務類別"},
    {"序號": 53, "英文": "jobDetail.salary", "中文": "薪資待遇(文字描述)"},
    {"序號": 54, "英文": "jobDetail.salaryMin", "中文": "最低薪資"},
    {"序號": 55, "英文": "jobDetail.salaryMax", "中文": "最高薪資"},
    {"序號": 56, "英文": "jobDetail.salaryType", "中文": "薪資類型(月薪/面議)"},
    {"序號": 57, "英文": "jobDetail.jobType", "中文": "工作性質(全職/兼職)"},
    {"序號": 58, "英文": "jobDetail.workType", "中文": "工作型態"},
    {"序號": 59, "英文": "jobDetail.addressNo", "中文": "地址郵遞區號"},
    {"序號": 60, "英文": "jobDetail.addressRegion", "中文": "上班地點(縣市)"},
    {"序號": 61, "英文": "jobDetail.addressArea", "中文": "上班地點(鄉鎮市區)"},
    {"序號": 62, "英文": "jobDetail.addressDetail", "中文": "上班地點(詳細地址)"},
    {"序號": 63, "英文": "jobDetail.industryArea", "中文": "工作地點/工業區"},
    {"序號": 64, "英文": "jobDetail.longitude", "中文": "經度"},
    {"序號": 65, "英文": "jobDetail.latitude", "中文": "緯度"},
    {"序號": 66, "英文": "jobDetail.manageResp", "中文": "管理責任"},
    {"序號": 67, "英文": "jobDetail.businessTrip", "中文": "出差外派"},
    {"序號": 68, "英文": "jobDetail.workPeriod", "中文": "上班時段"},
    {"序號": 69, "英文": "jobDetail.vacationPolicy", "中文": "休假制度"},
    {"序號": 70, "英文": "jobDetail.startWorkingDay", "中文": "可上班日"},
    {"序號": 71, "英文": "jobDetail.hireType", "中文": "聘僱類型"},
    {"序號": 72, "英文": "jobDetail.delegatedRecruit", "中文": "是否為派遣工作"},
    {"序號": 73, "英文": "jobDetail.needEmp", "中文": "需求人數"},
    {"序號": 74, "英文": "jobDetail.landmark", "中文": "地標"},
    {"序號": 75, "英文": "jobDetail.remoteWork.type", "中文": "遠端工作類型"},
    {"序號": 76, "英文": "jobDetail.remoteWork.description", "中文": "遠端工作描述"},
    {
        "序號": 77,
        "英文": "interactionRecord.lastProcessedResumeAtTime",
        "中文": "上次處理履歷時間",
    },
    {"序號": 78, "英文": "interactionRecord.nowTimestamp", "中文": "當前時間戳"},
    {"序號": 79, "英文": "jobDetail.remoteWork", "中文": "遠端工作(物件)"},
]

df_new = pd.json_normalize(column_names)
df_new.columns = ["序號", "104_英文", "104_中文"]
df_new
# Output:
#       序號                                       104_英文    104_中文

#   0    1                                       switch   內部切換/開關

#   1    2                                     custLogo    公司Logo

#   2    3                                   postalCode      郵遞區號

#   3    4                                    closeDate      截止日期

#   4    5                                     industry      產業類別

#   ..  ..                                          ...       ...

#   74  75                    jobDetail.remoteWork.type    遠端工作類型

#   75  76             jobDetail.remoteWork.description    遠端工作描述

#   76  77  interactionRecord.lastProcessedResumeAtTime  上次處理履歷時間

#   77  78               interactionRecord.nowTimestamp     當前時間戳

#   78  79                         jobDetail.remoteWork  遠端工作(物件)

#   

#   [79 rows x 3 columns]



================================================
FILE: crawler/project_104/client_104.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import (
    HEADERS_104_JOB_API,
    JOB_API_BASE_URL_104,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_api_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[Dict[str, Any]]:
    """
    通用的 API 請求函式，處理隨機延遲、請求發送、JSON 解析和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before API request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        data = response.json()
        return data
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except json.JSONDecodeError:
        logger.error(
            "Failed to parse JSON response from API.",
            url=url,
            exc_info=True,
            **log_context,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None


def fetch_job_data_from_104_api(job_id: str) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取單一職缺的原始數據。
    """
    api_url = f"{JOB_API_BASE_URL_104}{job_id}"
    return _make_api_request(
        "GET",
        api_url,
        headers=HEADERS_104_JOB_API,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,  # 加上這行
        log_context={"job_id": job_id, "api_type": "job_data"},
    )


def fetch_category_data_from_104_api(
    api_url: str, headers: Dict[str, str]
) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取職務分類的原始數據。
    """
    return _make_api_request(
        "GET",
        api_url,
        headers=headers,
        log_context={"api_type": "category_data"},
    )


def fetch_job_urls_from_104_api(
    base_url: str,
    headers: Dict[str, str],
    params: Dict[str, Any],
    timeout: int,
    verify: bool = True,
) -> Optional[Dict[str, Any]]:
    """
    從 104 API 獲取職缺 URL 列表的原始數據。
    """
    return _make_api_request(
        "GET",
        base_url,
        headers=headers,
        params=params,
        timeout=timeout,
        verify=verify,
        log_context={"api_type": "job_urls"},
    )



================================================
FILE: crawler/project_104/config_104.py
================================================
# crawler/project_104/config.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# 104 平台相關設定
JOB_CAT_URL_104 = config_section.get(
    "JOB_CAT_URL_104", "https://static.104.com.tw/category-tool/json/JobCat.json"
)
JOB_API_BASE_URL_104 = config_section.get(
    "JOB_API_BASE_URL_104", "https://www.104.com.tw/job/ajax/content/"
)
WEB_NAME_104 = config_section.get("WEB_NAME_104", "104_人力銀行")

HEADERS_104 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/jobs/search",
}

HEADERS_104_JOB_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/job/",
}

HEADERS_104_URL_CRAWLER = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
    "Referer": "https://www.104.com.tw/jobs/search/",
}

URL_CRAWLER_BASE_URL_104 = config_section.get(
    "URL_CRAWLER_BASE_URL_104", "https://www.104.com.tw/jobs/search/api/jobs"
)
URL_CRAWLER_PAGE_SIZE_104 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_104", "20"))
URL_CRAWLER_ORDER_BY_104 = int(
    config_section.get("URL_CRAWLER_ORDER_BY_104", "16")
)  # 16 (最近更新)



================================================
FILE: crawler/project_104/local_fetch_104_url_data.py
================================================
import requests
import sys
from requests.exceptions import HTTPError, JSONDecodeError
import structlog

from crawler.worker import app
from crawler.logging_config import configure_logging  # Import configure_logging
from crawler.config import JOB_API_BASE_URL_104  # Import the base URL from config

configure_logging()  # Call configure_logging at the beginning
logger = structlog.get_logger(__name__)


# 註冊 task, 有註冊的 task 才可以變成任務發送給 rabbitmq
@app.task()
def get_job_api_data(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "referer": "https://www.104.com.tw/",
    }

    job_id = url.split("/")[-1].split("?")[0]
    # Use the configured base URL
    url_api = f"{JOB_API_BASE_URL_104}{job_id}"

    try:
        response = requests.get(url_api, headers=headers)
        response.raise_for_status()
        data = response.json()
    except (HTTPError, JSONDecodeError) as err:
        logger.error(
            "Failed to fetch job API data", url=url_api, error=err
        )  # Improved log message
        return {}

    job_data = data.get("data", {})
    if not job_data or job_data.get("custSwitch", {}) == "off":
        logger.info(
            "Job content does not exist or is closed", job_id=job_id
        )  # Improved log message
        return {}

    extracted_info = {
        "job_id": job_id,
        "update_date": job_data.get("header", {}).get("appearDate"),
        "title": job_data.get("header", {}).get("jobName"),
        "description": job_data.get("jobDetail", {}).get("jobDescription"),
        "salary": job_data.get("jobDetail", {}).get("salary"),
        "work_type": job_data.get("jobDetail", {}).get("workType"),
        "work_time": job_data.get("jobDetail", {}).get("workPeriod"),
        "location": job_data.get("jobDetail", {}).get("addressRegion"),
        "degree": job_data.get("condition", {}).get("edu"),
        "department": job_data.get("jobDetail", {}).get("department"),
        "working_experience": job_data.get("condition", {}).get("workExp"),
        "qualification_required": job_data.get("condition", {}).get("other"),
        "qualification_bonus": job_data.get("welfare", {}).get("welfare"),
        "company_id": job_data.get("header", {}).get("custNo"),
        "company_name": job_data.get("header", {}).get("custName"),
        "company_address": job_data.get("company", {}).get("address"),
        "contact_person": job_data.get("contact", {}).get("hrName"),
        "contact_phone": job_data.get("contact", {}).get("email", "未提供"),
    }

    logger.info(
        "Extracted job information", job_id=job_id, extracted_info=extracted_info
    )  # Improved log message
    return extracted_info  # Return the extracted info


if __name__ == "__main__":
    if len(sys.argv) != 2:
        logger.info(
            "Usage: python local_fetch_104_url_data.py <job_url>"
        )  # Updated usage message
        sys.exit(1)

    job_url = sys.argv[1]
    logger.info("Dispatching job API data task", job_url=job_url)
    get_job_api_data.delay(job_url)



================================================
FILE: crawler/project_104/page_api_data_104.txt
================================================
{
  "data": [
    {
      "appearDate": "20250730",
      "applyCnt": 0,
      "coIndustry": 1011001003,
      "coIndustryDesc": "其他營造業",
      "custName": "統上開發建設股份有限公司",
      "custNo": "130000000194576",
      "description": "工作內容:\n1.市場趨勢研究及同業競品分析。\n2.專案執行(新型態商機專案研究、公司中長期年度計畫)。\n3.專案進度追蹤。\n4.專案分析管控。\n",
      "descSnippet": "工作內容:\n1.市場趨勢研究及同業競品分析。\n2.專案執行(新型態商機專案研究、公司中長期年度計畫)。\n3.專案進度追蹤。\n4.專案分析管控。\n",
      "mrtDist": 0,
      "jobAddress": "永安路165巷155號",
      "jobAddrNo": 6001014007,
      "jobAddrNoDesc": "台南市永康區",
      "jobName": "經營企劃人員(需不動產相關科系)",
      "jobNameSnippet": "經營企劃人員(需不動產相關科系)",
      "jobNo": "14570953",
      "jobRo": 1,
      "jobType": 1,
      "lat": 23.0445358,
      "lon": 120.2425509,
      "link": {
        "job": "https://www.104.com.tw/job/8ob0p",
        "cust": "https://www.104.com.tw/company/1a2x6bm5g0",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8ob0p?channel=104rpt"
      },
      "major": [
        "其他相關科系"
      ],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 35000,
      "salaryLow": 30000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf32": {
          "desc": "",
          "param": "wf32"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf23": {
          "desc": "",
          "param": "wf23"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "8:00~17:00",
      "hrBehaviorPR": 0,
      "jobCat": [
        2001001003,
        2004001005,
        2005003004
      ],
      "labels": [
        "c@wf7",
        "c@wf18",
        "c@wf31",
        "c@wf9",
        "c@wf2",
        "c@wf32",
        "c@wf10",
        "c@wf17",
        "c@wf1",
        "c@wf23",
        "c@wf4",
        "c@wf28",
        "c@wf3"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        },
        {
          "language": 18,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 15,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": null,
        "lastCustReplyTimestamp": null,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250727",
      "applyCnt": 4,
      "coIndustry": 1009004002,
      "coIndustryDesc": "汽機車維修業",
      "custName": "Gogoro_渝城車業有限公司",
      "custNo": "130000000113203",
      "description": "只要你是《積極、樂觀、細膩、善溝通、配合度高 》，歡迎加入我們！\n歡迎尋求長期工作穩定、心思細膩、能跟夥伴互助合作、同甘共苦者，一同加入我們的行列。 \n\n工作內容： \n\n1. Gogoro智慧雙輪 銷售 交車\n2. Gogoro智慧雙輪 品牌價值認同 推廣\n3. 售後服務\n4. 客戶關係維繫 維修預約進廠安排\n\n- 本薪、伙食津貼、業績達成獎金\n- 具銷售經驗1年以上、具備說話藝術者尤佳\n- Gogoro認證培訓課程(帶薪)\n- 需配合門市夥伴排班排假\n- 需配合公司政策及人力需求而調動服務門市\n\n在你的工作中，你願意提供自己的經驗，讓你與你的團隊追求卓越表現。完成公司賦予的目標，包含業績目標、客戶滿意度、行政流程等。此外，在你的工作中，能夠達到甚至超越自己設定的工作目標與表現，讓你與你的團隊追求卓越。",
      "descSnippet": "只要你是《積極、樂觀、細膩、善溝通、配合度高 》，歡迎加入我們！\n歡迎尋求長期工作穩定、心思細膩、能跟夥伴互助合作、同甘共苦者，一同加入我們的行列。 \n\n工作內容： \n\n1. Gogoro智慧雙輪 銷售 交車\n2. Gogoro智慧雙輪 品牌價值認同 推廣\n3. 售後服務\n4. 客戶關係維繫 維修預約進廠安排\n\n- 本薪、伙食津貼、業績達成獎金\n- 具銷售經驗1年以上、具備說話藝術者尤佳\n- Gogoro認證培訓課程(帶薪)\n- 需配合門市夥伴排班排假\n- 需配合公司政策及人力需求而調動服務門市\n\n在你的工作中，你願意提供自己的經驗，讓你與你的團隊追求卓越表現。完成公司賦予的目標，包含業績目標、客戶滿意度、行政流程等。此外，在你的工作中，能夠達到甚至超越自己設定的工作目標與表現，讓你與你的團隊追求卓越。",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6001002021,
      "jobAddrNoDesc": "新北市新莊區",
      "jobName": "GOGORO新莊民安【帶薪培訓】銷售業務&儲備幹部 Sales Specialist [本薪+績效獎金]",
      "jobNameSnippet": "GOGORO新莊民安【帶薪培訓】銷售業務&儲備幹部 Sales Specialist [本薪+績效獎金]",
      "jobNo": "13040563",
      "jobRo": 1,
      "jobType": 1,
      "lat": 25.0265985,
      "lon": 121.4178347,
      "link": {
        "job": "https://www.104.com.tw/job/7ri5v",
        "cust": "https://www.104.com.tw/company/1a2x6bkenn",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7ri5v?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 32400,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "11:30-20:30, 12:30-21:30",
      "hrBehaviorPR": 0.8558028490931103,
      "jobCat": [
        2005002004,
        2001001002,
        2005003004
      ],
      "labels": [
        "college@student_invited",
        "c@wf7"
      ],
      "languageRequirements": [
        {
          "language": 18,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 35,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753613395,
        "lastCustReplyTimestamp": 1753692175,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 28,
      "coIndustry": 1003001015,
      "coIndustryDesc": "綜合商品批發代理業",
      "custName": "京麗國際股份有限公司",
      "custNo": "130000000211566",
      "description": "1.通路主管拜訪、合作評估規劃與開發\n2.熟悉公司產品、規章作業流程\n3.現有客戶之關係維護與溝通\n4.對各類營銷數據具高度敏感分析能力\n5.節慶促銷活動提案與檢討\n6.需具跨部門協商之溝通表達能力\n7.善於安排管理與引領部門所屬工作方向\n8.積極完成上級主管其他交辦事項",
      "descSnippet": "1.通路主管拜訪、合作評估規劃與開發\n2.熟悉公司產品、規章作業流程\n3.現有客戶之關係維護與溝通\n4.對各類營銷數據具高度敏感分析能力\n5.節慶促銷活動提案與檢討\n6.需具跨部門協商之溝通表達能力\n7.善於安排管理與引領部門所屬工作方向\n8.積極完成上級主管其他交辦事項",
      "mrtDist": 0.5,
      "jobAddress": "重陽路477號",
      "jobAddrNo": 6001001011,
      "jobAddrNoDesc": "台北市南港區",
      "jobName": "業務部門主管",
      "jobNameSnippet": "業務部門主管",
      "jobNo": "13636052",
      "jobRo": 3,
      "jobType": 0,
      "lat": 25.06001,
      "lon": 121.61094,
      "link": {
        "job": "https://www.104.com.tw/job/849n8",
        "cust": "https://www.104.com.tw/company/1a2x6bmijy",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/849n8?channel=104rpt"
      },
      "major": [
        "企業管理相關",
        "市場行銷相關",
        "其他商業及管理相關"
      ],
      "mrt": "99001001023",
      "mrtDesc": "捷運南港軟體園區站",
      "optionEdu": [
        4,
        5
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf25": {
          "desc": "",
          "param": "wf25"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "landmark": {
          "desc": "距捷運南港軟體園區站約500公尺"
        }
      },
      "s9": [],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9405876961721676,
      "jobCat": [
        9009002000,
        9009004000,
        9001001000
      ],
      "labels": [
        "foreigners@full_fc_em_jbs",
        "c@wf9",
        "45plus@45invited",
        "c@wf29",
        "c@wf7",
        "c@wf28",
        "senior@senior_job_I",
        "c@wf1",
        "c@wf2",
        "c@wf34",
        "c@wf10",
        "c@wf25",
        "c@wf26",
        "college@student_invited",
        "c@wf3"
      ],
      "languageRequirements": [],
      "acceptRole": [],
      "employeeCount": 30,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861291,
        "lastCustReplyTimestamp": 1753863289,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 12,
      "coIndustry": 1009005002,
      "coIndustryDesc": "美容／美體業",
      "custName": "享淨顏_佳登國際有限公司",
      "custNo": "130000000232919",
      "description": "我們有完整的員工訓練且注重工作氣氛和員工福利對人才極為重視，薪水方面隨著年資增長未來只會更高。\r\n\r\n\r\n【福利】\r\n*績效獎金、每月業績獎金\r\n*享勞健保及6%勞退\r\n*不定期部門聚餐\r\n*每月一次免費潔顏福利\r\n*提供員工工作圍裙\r\n*良好升遷管道(培訓幹部、店長等...)\r\n*三節禮金（春節禮金加碼）\r\n*全額補助考取美容證照\r\n*已有美容證照給予專業加給\r\n丙級500/乙級1000（每月給予）\r\n*久任獎金（滿兩年2000、滿三年5000每年給予）\r\n*每年按考績調升薪資\r\n\r\n【工作內容】\r\n1.基本臉部清潔、美容課程操作(洗卸、粉刺清潔、儀器導入等)、相關知識解說。\r\n2.客戶肌膚檢測分析、建議課程資訊。\r\n3.客戶服務管理、行政庶務及系統操作。\r\n4.店務處理(盤點進貨、環境整潔、日報回覆...等)。\r\n5.無需推銷產品\r\n\r\n【所需特質】\r\n1.無經驗可(乙、丙級美容證照or美容銷售經驗佳)\r\n2.熱愛學習，能保持彈性吸收新鮮事物\r\n3.喜歡與顧客互動，口條流暢善溝通，不害怕主動與人接觸\r\n4.具有高度向心力，具團隊合作精神\r\n\r\n【薪獎制度】\r\n1. 當月激勵：當月作滿105人次以上，每多一人獎金200元(月結)\r\n2. 當月儲值：第一名2000、第二名1500、第三名1000\r\n3. 當月個人營業額獎勵(不含儲值金)： (1)9萬1.5% (2)10萬2% （3）15萬3%\r\n4.全勤：1000元",
      "descSnippet": "我們有完整的員工訓練且注重工作氣氛和員工福利對人才極為重視，薪水方面隨著年資增長未來只會更高。\r\n\r\n\r\n【福利】\r\n*績效獎金、每月業績獎金\r\n*享勞健保及6%勞退\r\n*不定期部門聚餐\r\n*每月一次免費潔顏福利\r\n*提供員工工作圍裙\r\n*良好升遷管道(培訓幹部、店長等...)\r\n*三節禮金（春節禮金加碼）\r\n*全額補助考取美容證照\r\n*已有美容證照給予專業加給\r\n丙級500/乙級1000（每月給予）\r\n*久任獎金（滿兩年2000、滿三年5000每年給予）\r\n*每年按考績調升薪資\r\n\r\n【工作內容】\r\n1.基本臉部清潔、美容課程操作(洗卸、粉刺清潔、儀器導入等)、相關知識解說。\r\n2.客戶肌膚檢測分析、建議課程資訊。\r\n3.客戶服務管理、行政庶務及系統操作。\r\n4.店務處理(盤點進貨、環境整潔、日報回覆...等)。\r\n5.無需推銷產品\r\n\r\n【所需特質】\r\n1.無經驗可(乙、丙級美容證照or美容銷售經驗佳)\r\n2.熱愛學習，能保持彈性吸收新鮮事物\r\n3.喜歡與顧客互動，口條流暢善溝通，不害怕主動與人接觸\r\n4.具有高度向心力，具團隊合作精神\r\n\r\n【薪獎制度】\r\n1. 當月激勵：當月作滿105人次以上，每多一人獎金200元(月結)\r\n2. 當月儲值：第一名2000、第二名1500、第三名1000\r\n3. 當月個人營業額獎勵(不含儲值金)： (1)9萬1.5% (2)10萬2% （3）15萬3%\r\n4.全勤：1000元",
      "mrtDist": 0.09,
      "jobAddress": "和平街19號",
      "jobAddrNo": 6001002015,
      "jobAddrNoDesc": "新北市中和區",
      "jobName": "美容師/潔顏師（享淨顏-新北中和店）",
      "jobNameSnippet": "美容師/潔顏師（享淨顏-新北中和店）",
      "jobNo": "14051744",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.9906111,
      "lon": 121.5085711,
      "link": {
        "job": "https://www.104.com.tw/job/8d6e8",
        "cust": "https://www.104.com.tw/company/1a2x6bmz13",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8d6e8?channel=104rpt"
      },
      "major": [],
      "mrt": "99001004001",
      "mrtDesc": "捷運南勢角站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 32000,
      "tags": {
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運南勢角站約90公尺"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "日班(10:30~19:30)晚班(11:00~20:00)",
      "hrBehaviorPR": 0.8974587323723998,
      "jobCat": [
        2006003001,
        2006003008,
        2001001002
      ],
      "labels": [
        "college@student_invited",
        "c@wf29",
        "c@wf26",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [],
      "employeeCount": 10,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753865431,
        "lastCustReplyTimestamp": 1753865505,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1008003001,
      "coIndustryDesc": "工商顧問服務業",
      "custName": "泰紅心國際有限公司",
      "custNo": "130000000242963",
      "description": "工作內容：\n\n我們正在尋找一位充滿熱情、具有領導能力的門市店長，加入米蘭廚房的大家庭！\n作為店長，您將負責帶領團隊打造高效、溫馨的用餐環境，並確保門市營運順暢，讓每一位顧客都感受到我們的用心與專業。\n\n主要工作職責包含：\n\t•\t門市營運管理：人員排班、進出貨管理、庫存控管\n\t•\t銷售目標設定與達成：分析業績數據、擬定促銷策略\n\t•\t團隊管理與訓練：帶領團隊提升服務品質與工作效率\n\t•\t顧客服務與客訴處理\n\t•\t品牌形象維護與標準SOP執行\n\n工作地點：\n\n台中市西屯區（鄰近逢甲商圈／交通便利）\n\n薪資福利：\n\t•\t月薪 通常 50,000 （績效獎金、營收分紅、年終獎金可能更多）\n\t•\t員工餐飲優惠、完整培訓制度、升遷管道明確\n\n上班時間：\n\n排班制，月休8天（可彈性協調）\n\n我們希望你具備：\n\t•\t具門市管理經驗者優先（如餐飲、零售、美業等）\n\t•\t擅長溝通協調，能激勵團隊士氣\n\t•\t目標導向，具執行力\n\t•\t對顧客服務有熱忱，有責任感與抗壓性",
      "descSnippet": "工作內容：\n\n我們正在尋找一位充滿熱情、具有領導能力的門市店長，加入米蘭廚房的大家庭！\n作為店長，您將負責帶領團隊打造高效、溫馨的用餐環境，並確保門市營運順暢，讓每一位顧客都感受到我們的用心與專業。\n\n主要工作職責包含：\n\t•\t門市營運管理：人員排班、進出貨管理、庫存控管\n\t•\t銷售目標設定與達成：分析業績數據、擬定促銷策略\n\t•\t團隊管理與訓練：帶領團隊提升服務品質與工作效率\n\t•\t顧客服務與客訴處理\n\t•\t品牌形象維護與標準SOP執行\n\n工作地點：\n\n台中市西屯區（鄰近逢甲商圈／交通便利）\n\n薪資福利：\n\t•\t月薪 通常 50,000 （績效獎金、營收分紅、年終獎金可能更多）\n\t•\t員工餐飲優惠、完整培訓制度、升遷管道明確\n\n上班時間：\n\n排班制，月休8天（可彈性協調）\n\n我們希望你具備：\n\t•\t具門市管理經驗者優先（如餐飲、零售、美業等）\n\t•\t擅長溝通協調，能激勵團隊士氣\n\t•\t目標導向，具執行力\n\t•\t對顧客服務有熱忱，有責任感與抗壓性",
      "mrtDist": 0,
      "jobAddress": "寶慶街50巷51號",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "高底薪＋獎金｜門市店長｜西屯區｜米蘭廚房親子餐廳",
      "jobNameSnippet": "高底薪＋獎金｜門市店長｜西屯區｜米蘭廚房親子餐廳",
      "jobNo": "14751928",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1722414,
      "lon": 120.6475223,
      "link": {
        "job": "https://www.104.com.tw/job/8s6ns",
        "cust": "https://www.104.com.tw/company/1a2x6bn6s3",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8s6ns?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 9999999,
      "salaryLow": 50000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf11": {
          "desc": "",
          "param": "wf11"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.64529391953745,
      "jobCat": [
        2005002002,
        2005002001,
        2001001002
      ],
      "labels": [
        "senior@senior_job_I",
        "c@wf2",
        "c@wf4",
        "c@wf11",
        "foreigners@foreigners_tick",
        "foreigners@overseasStudents_tick"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 10,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753849823,
        "lastCustReplyTimestamp": null,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 5,
      "coIndustry": 1004001006,
      "coIndustryDesc": "其他金融及輔助業",
      "custName": "宏旺當舖",
      "custNo": "130000000165552",
      "description": "新型態當鋪徵才中！（無經驗可）\r\n業務專員 / 儲備幹部\r\n\r\n工作內容：\r\n辦公室夏天吹冷氣冬天吹暖氣\r\n只需會操作使用手機及電腦\r\n不需曬太陽淋雨路邊發傳單\r\n與客戶聊天中了解客戶需求\r\n學習金融借款審核模式\r\n與主管學習觀念及管理\r\n升遷管道暢通晉升無阻礙\r\n\r\n工作時間：\r\n0900-1700、1300-2100（輪班制，靈活調整）\r\n依照業績調整工作時數，從8小時可降至6小時，最低上班只要4小時，賺錢輕鬆、時間自由！\r\n\r\n薪資待遇：\r\n保障底薪+業績獎金+每月分紅\r\n半年後月薪：$55,000以上\r\n兩年後月薪：$100,000以上，挑戰無上限！\r\n\r\n工作福利：\r\n分紅獎金、激勵獎金、年終獎金、招募獎金\r\n三節禮品、生日禮金、業績獎金、特殊獎金\r\n不定期員工聚餐，工作氛圍良好，無惡性競爭\r\n\r\n休假制度：\r\n月休共8天每週日固定公休\r\n其餘天數自由排休\r\n\r\n為什麼選我們？\r\n零經驗可，教到會\r\n室內上班，不外跑\r\n獎金無上限，在職越久薪水越多\r\n我們是創新當鋪，主打內勤業務＋高收入。\r\n完整培訓制度，無經驗也能快速上手。\r\n保障基本底薪＋高獎金，輕鬆十萬月薪不是夢！\r\n歡迎銀行、融資、保險、代書、金主、車行、\r\n房仲、保經、電銷、代辦、各行各業業務轉行。\r\n想賺錢，來就對了！",
      "descSnippet": "新型態當鋪徵才中！（無經驗可）\r\n業務專員 / 儲備幹部\r\n\r\n工作內容：\r\n辦公室夏天吹冷氣冬天吹暖氣\r\n只需會操作使用手機及電腦\r\n不需曬太陽淋雨路邊發傳單\r\n與客戶聊天中了解客戶需求\r\n學習金融借款審核模式\r\n與主管學習觀念及管理\r\n升遷管道暢通晉升無阻礙\r\n\r\n工作時間：\r\n0900-1700、1300-2100（輪班制，靈活調整）\r\n依照業績調整工作時數，從8小時可降至6小時，最低上班只要4小時，賺錢輕鬆、時間自由！\r\n\r\n薪資待遇：\r\n保障底薪+業績獎金+每月分紅\r\n半年後月薪：$55,000以上\r\n兩年後月薪：$100,000以上，挑戰無上限！\r\n\r\n工作福利：\r\n分紅獎金、激勵獎金、年終獎金、招募獎金\r\n三節禮品、生日禮金、業績獎金、特殊獎金\r\n不定期員工聚餐，工作氛圍良好，無惡性競爭\r\n\r\n休假制度：\r\n月休共8天每週日固定公休\r\n其餘天數自由排休\r\n\r\n為什麼選我們？\r\n零經驗可，教到會\r\n室內上班，不外跑\r\n獎金無上限，在職越久薪水越多\r\n我們是創新當鋪，主打內勤業務＋高收入。\r\n完整培訓制度，無經驗也能快速上手。\r\n保障基本底薪＋高獎金，輕鬆十萬月薪不是夢！\r\n歡迎銀行、融資、保險、代書、金主、車行、\r\n房仲、保經、電銷、代辦、各行各業業務轉行。\r\n想賺錢，來就對了！",
      "mrtDist": 0.28,
      "jobAddress": "幸福路66號1樓",
      "jobAddrNo": 6001002021,
      "jobAddrNoDesc": "新北市新莊區",
      "jobName": "新手業務第一首選，我們一起成長！",
      "jobNameSnippet": "新手業務第一首選，我們一起成長！",
      "jobNo": "14505866",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0495205,
      "lon": 121.457265,
      "link": {
        "job": "https://www.104.com.tw/job/8mwsq",
        "cust": "https://www.104.com.tw/company/1a2x6blj1s",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8mwsq?channel=104rpt"
      },
      "major": [],
      "mrt": "99001006013",
      "mrtDesc": "捷運幸福站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 100000,
      "salaryLow": 28590,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "landmark": {
          "desc": "距捷運幸福站約280公尺"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "09:00-17:00/13:00-21:00（依業績調整上班時間，最低上班時數四小時）",
      "hrBehaviorPR": 0.9961574713449076,
      "jobCat": [
        2005003016,
        2001001002,
        2005003015
      ],
      "labels": [
        "c@wf1",
        "c@wf4",
        "foreigners@foreigners",
        "c@wf2",
        "college@student_invited",
        "c@wf29",
        "c@wf7",
        "foreigners@chineseDiasporas",
        "c@wf10"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        64,
        2048
      ],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753748167,
        "lastCustReplyTimestamp": 1753893623,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 18,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "1.規劃、執行與協調行政事務作業\n2.協助集團推動相關專案，包含資訊蒐集與分析、數據彙整與報告、進度掌握等\n3.參與跨單位會議與追蹤決議事項 \n4.報表分析與會議簡報製作\n5.廠區人事作業執行(海外幹部到職及入境事宜)\n6.廠區大型活動與會議籌備\n7.執行越南總管理處有關財務、關務、庶務…等工作配合事宜\n8.其他主管交辦事項",
      "descSnippet": "1.規劃、執行與協調行政事務作業\n2.協助集團推動相關專案，包含資訊蒐集與分析、數據彙整與報告、進度掌握等\n3.參與跨單位會議與追蹤決議事項 \n4.報表分析與會議簡報製作\n5.廠區人事作業執行(海外幹部到職及入境事宜)\n6.廠區大型活動與會議籌備\n7.執行越南總管理處有關財務、關務、庶務…等工作配合事宜\n8.其他主管交辦事項",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6003002001,
      "jobAddrNoDesc": "越南",
      "jobName": "【行政/幕僚】廠區主管室主管/人員(越南)",
      "jobNameSnippet": "【行政/幕僚】廠區主管室主管/人員(越南)",
      "jobNo": "14290959",
      "jobRo": 1,
      "jobType": 2,
      "lat": 14.058324,
      "lon": 108.277199,
      "link": {
        "job": "https://www.104.com.tw/job/8iaz3",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8iaz3?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4,
        5
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.8372079340114733,
      "jobCat": [
        2001001003,
        2002001003,
        2001001001
      ],
      "labels": [
        "c@wf16",
        "c@wf9",
        "senior@senior_job_B",
        "c@wf10",
        "foreigners@foreigners",
        "c@wf27",
        "c@wf1",
        "c@wf34",
        "45plus@45invited"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861478,
        "lastCustReplyTimestamp": 1753335411,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 9,
      "coIndustry": 1006003001,
      "coIndustryDesc": "廣告行銷公關業",
      "custName": "時代社群行銷有限公司",
      "custNo": "130000000149039",
      "description": "We Need You Can Do\n①專案控管\n - 專案管理：負責專案規劃、執行與進度追蹤，確保項目目標達成\n - 策略執行：根據營運長決策，協助策略落地並監督執行情況\n - 成本與資源控管：協助優化營運成本，提高工作流程效率\n - 跨部門協作：與全公司營運部門協調合作，確保營運順暢\n - 內外部關係經營：負責與客戶、合作夥伴、供應商的溝通協調\n - 會議與行程管理：協助營運長行程、內外部會議，並製作簡報、會議紀錄與追蹤決議事項\n②行銷策略輔助，協助規劃行銷活動、品牌推廣與執行、執行活動時程控管\n③應變與問題解決，處理突發狀況，提供高效解決方案\n④時間與細節管理，有效安排工作優先順序，確保各項事務順利進行\n⑤稅務作業流程(各類所得及二代健保之扣繳/營業稅)\n⑥配合財務團隊及會計師查帳作業\n⑦其他營運長交辦事項\n\nWe Expect You\n①具備出色的組織和協調能力,能夠有效處理多項任務。\n②擁有優秀的溝通技巧,能夠舆不同層級的人員有效溝通。\n③具備保密性和敏感性,能夠處理機密信息。\n④具有良好的時間管理能力和工作細心的態度。\n⑤學習能力佳，有上進心，願意與公司一同發展成長。\n ＊薪資待遇依照工作能力與經驗而定＊\n\n媒體網站：https://cyclingtime.com/tw.html\n粉絲專頁：https://www.facebook.com/cyclingtime.tw\n公司官網：https://timedia.tw/\n\n了解執行長：www.youtube.com/@heynick2024\n---\n我們是時代社群行銷團隊，公司大多是U35的夥伴\n專門協助傳統產業(ex.自行車)數位轉型、整合行銷...\n我們還擁有一個將近17萬追蹤的粉絲專頁「單車時代CYCLINGTIME」，\n是目前全亞洲最大的自行車媒體，\n更有一個超過4萬訂閱的Youtube頻道，\n讓你的創意可以透過實際數據反饋更加了解市場需求，\n而且我們還舉辦了亞洲最大自行車百K挑戰活動「時代騎輪節」，\n邀請具有創造力，有主見有想法的夥伴一起加入我們的行列\n\n公司文化與福利\n我們創造一個開放式、鼓勵夥伴互相碰撞想法的工作環境\n團隊注重創新與成長，定期安排專家講座激發大家的想像力\n因為我們認為想像是你能力的唯一限制～\n更是非常重視團隊個人的意見與想法，熱於傾聽每個人的聲音\n---\nTIMEDIA\nTendency 趨勢＊Innovation 創新＊Marvel 驚奇＊Enthusiasm 熱情＊Desire 渴望＊Influence 影響力＊Ambition 野心\n\n我們是一個關注「趨勢」擁抱「創新」不斷創造「驚奇」並且充滿「熱情」的團隊，我們是由一群對成長極度「渴望」的Y+Z世代組成，立志於拼搏的過程持續創造對社會正面的「影響力」，「野心」早已刻在心中...",
      "descSnippet": "We Need You Can Do\n①專案控管\n - 專案管理：負責專案規劃、執行與進度追蹤，確保項目目標達成\n - 策略執行：根據營運長決策，協助策略落地並監督執行情況\n - 成本與資源控管：協助優化營運成本，提高工作流程效率\n - 跨部門協作：與全公司營運部門協調合作，確保營運順暢\n - 內外部關係經營：負責與客戶、合作夥伴、供應商的溝通協調\n - 會議與行程管理：協助營運長行程、內外部會議，並製作簡報、會議紀錄與追蹤決議事項\n②行銷策略輔助，協助規劃行銷活動、品牌推廣與執行、執行活動時程控管\n③應變與問題解決，處理突發狀況，提供高效解決方案\n④時間與細節管理，有效安排工作優先順序，確保各項事務順利進行\n⑤稅務作業流程(各類所得及二代健保之扣繳/營業稅)\n⑥配合財務團隊及會計師查帳作業\n⑦其他營運長交辦事項\n\nWe Expect You\n①具備出色的組織和協調能力,能夠有效處理多項任務。\n②擁有優秀的溝通技巧,能夠舆不同層級的人員有效溝通。\n③具備保密性和敏感性,能夠處理機密信息。\n④具有良好的時間管理能力和工作細心的態度。\n⑤學習能力佳，有上進心，願意與公司一同發展成長。\n ＊薪資待遇依照工作能力與經驗而定＊\n\n媒體網站：https://cyclingtime.com/tw.html\n粉絲專頁：https://www.facebook.com/cyclingtime.tw\n公司官網：https://timedia.tw/\n\n了解執行長：www.youtube.com/@heynick2024\n---\n我們是時代社群行銷團隊，公司大多是U35的夥伴\n專門協助傳統產業(ex.自行車)數位轉型、整合行銷...\n我們還擁有一個將近17萬追蹤的粉絲專頁「單車時代CYCLINGTIME」，\n是目前全亞洲最大的自行車媒體，\n更有一個超過4萬訂閱的Youtube頻道，\n讓你的創意可以透過實際數據反饋更加了解市場需求，\n而且我們還舉辦了亞洲最大自行車百K挑戰活動「時代騎輪節」，\n邀請具有創造力，有主見有想法的夥伴一起加入我們的行列\n\n公司文化與福利\n我們創造一個開放式、鼓勵夥伴互相碰撞想法的工作環境\n團隊注重創新與成長，定期安排專家講座激發大家的想像力\n因為我們認為想像是你能力的唯一限制～\n更是非常重視團隊個人的意見與想法，熱於傾聽每個人的聲音\n---\nTIMEDIA\nTendency 趨勢＊Innovation 創新＊Marvel 驚奇＊Enthusiasm 熱情＊Desire 渴望＊Influence 影響力＊Ambition 野心\n\n我們是一個關注「趨勢」擁抱「創新」不斷創造「驚奇」並且充滿「熱情」的團隊，我們是由一群對成長極度「渴望」的Y+Z世代組成，立志於拼搏的過程持續創造對社會正面的「影響力」，「野心」早已刻在心中...",
      "mrtDist": 0.24,
      "jobAddress": "文心路三段241號11樓之五",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "營運長特助",
      "jobNameSnippet": "營運長特助",
      "jobNo": "14545891",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1722076,
      "lon": 120.6629809,
      "link": {
        "job": "https://www.104.com.tw/job/8nroj",
        "cust": "https://www.104.com.tw/company/1a2x6bl6b3",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nroj?channel=104rpt"
      },
      "major": [],
      "mrt": "99003001007",
      "mrtDesc": "捷運文華高中站",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 38000,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "landmark": {
          "desc": "距捷運文華高中站約240公尺"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "09:00-18:00",
      "hrBehaviorPR": 0.7026488192896503,
      "jobCat": [
        2001001003,
        2003001010,
        2001001001
      ],
      "labels": [
        "c@wf1",
        "c@wf2",
        "c@wf29",
        "c@wf3",
        "c@wf26",
        "c@wf7"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2048
      ],
      "employeeCount": 12,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753894774,
        "lastCustReplyTimestamp": 1752222882,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1003001001,
      "coIndustryDesc": "農／畜／水產品批發業",
      "custName": "擎燁有限公司",
      "custNo": "130000000215386",
      "description": "依照固定路線配送蔬果至餐廳、學校、企業等熟客據點\n\n負責每日出車、回場卸貨、整理冷藏區作業（須搬運10～30公斤）\n\n協助車輛保養、載貨安全確認\n\n學習司機排班、新手訓練、路線交接與客訴處理\n\n表現穩定者可晉升儲備幹部，參與跨部門管理會議",
      "descSnippet": "依照固定路線配送蔬果至餐廳、學校、企業等熟客據點\n\n負責每日出車、回場卸貨、整理冷藏區作業（須搬運10～30公斤）\n\n協助車輛保養、載貨安全確認\n\n學習司機排班、新手訓練、路線交接與客訴處理\n\n表現穩定者可晉升儲備幹部，參與跨部門管理會議",
      "mrtDist": 0,
      "jobAddress": "民生路一段202號",
      "jobAddrNo": 6001008020,
      "jobAddrNoDesc": "台中市大雅區",
      "jobName": "【儲備司機幹部｜固定早班】可升主管／熟客路線／穩定長期",
      "jobNameSnippet": "【儲備司機幹部｜固定早班】可升主管／熟客路線／穩定長期",
      "jobNo": "13529865",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.2274529,
      "lon": 120.6509676,
      "link": {
        "job": "https://www.104.com.tw/job/81zpl",
        "cust": "https://www.104.com.tw/company/1a2x6bmli2",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/81zpl?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 60000,
      "salaryLow": 50000,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "0430-1430",
      "hrBehaviorPR": 0.8116430373731316,
      "jobCat": [
        2001001002,
        2011002001,
        2011002003
      ],
      "labels": [
        "c@wf1"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048
      ],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753357347,
        "lastCustReplyTimestamp": 1752072390,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 2,
      "coIndustry": 1002001001,
      "coIndustryDesc": "菸草製造業",
      "custName": "104測試_APP公司(請勿應徵)",
      "custNo": "130000000005797",
      "description": "[img] https://upload.cc/i1/2023/10/20/z7f6QZ.png [/img]\n1.規劃測試計畫與測試案例（test case）。\n2.測試與驗證系統功能、相容性、效能、壓力承載、可靠度等。\n3.建立測試環境，撰寫、維護、改善測試程式。\n4.分析測試資料、釐清問題、提出改善建議，並撰寫測試報告。\n5.維護相關測試設備。",
      "descSnippet": "[img] https://upload.cc/i1/2023/10/20/z7f6QZ.png [/img]\n1.規劃測試計畫與測試案例（test case）。\n2.測試與驗證系統功能、相容性、效能、壓力承載、可靠度等。\n3.建立測試環境，撰寫、維護、改善測試程式。\n4.分析測試資料、釐清問題、提出改善建議，並撰寫測試報告。\n5.維護相關測試設備。",
      "mrtDist": 0,
      "jobAddress": "大安路66號",
      "jobAddrNo": 6007001005,
      "jobAddrNoDesc": "冰島",
      "jobName": "測試職務_請勿應徵_web_qa_e2e_主動應徵",
      "jobNameSnippet": "測試職務_請勿應徵_web_qa_e2e_主動應徵",
      "jobNo": "12201294",
      "jobRo": 1,
      "jobType": 0,
      "lat": 64.963051,
      "lon": -19.020835,
      "link": {
        "job": "https://www.104.com.tw/job/79iku",
        "cust": "https://www.104.com.tw/company/1a2x6bi3s5",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/79iku?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": [],
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9951323278154313,
      "jobCat": [
        2001002005,
        2018002002,
        2001001002
      ],
      "labels": [],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 1234,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753829249,
        "lastCustReplyTimestamp": 1753913252,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "老王炸烤_三匠國際興業股份有限公司",
      "custNo": "130000000206725",
      "description": "老王炸烤/信義旗艦店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：臺北市信義區忠孝東路5段486號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "descSnippet": "老王炸烤/信義旗艦店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：臺北市信義區忠孝東路5段486號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "mrtDist": 0.21,
      "jobAddress": "忠孝東路五段486號",
      "jobAddrNo": 6001001007,
      "jobAddrNoDesc": "台北市信義區",
      "jobName": "門市店長(信義店)",
      "jobNameSnippet": "門市店長(信義店)",
      "jobNo": "13425601",
      "jobRo": 1,
      "jobType": 2,
      "lat": 25.04071,
      "lon": 121.578065,
      "link": {
        "job": "https://www.104.com.tw/job/7zr9d",
        "cust": "https://www.104.com.tw/company/1a2x6bmeth",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7zr9d?channel=104rpt"
      },
      "major": [],
      "mrt": "99001005019",
      "mrtDesc": "捷運永春站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 50000,
      "tags": {
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運永春站約210公尺"
        }
      },
      "s9": [
        1,
        2,
        8
      ],
      "s5": 0,
      "d3": "(依公司規定排班)",
      "hrBehaviorPR": 0.9897622738433338,
      "jobCat": [
        2006002002,
        2001001002,
        2005002001
      ],
      "labels": [
        "c@wf3",
        "c@wf31",
        "c@wf7",
        "foreigners@foreigners_tick",
        "c@wf1",
        "c@wf4",
        "foreigners@overseasStudents_tick",
        "c@wf27",
        "foreigners@foreignStudents",
        "college@student_invited",
        "c@wf28",
        "senior@senior_job_A",
        "c@wf29",
        "c@wf10",
        "c@wf17",
        "c@wf26",
        "foreigners@chineseDiasporas",
        "foreigners@overseasStudents",
        "foreigners@foreigners",
        "45plus@45invited",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 50,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753866167,
        "lastCustReplyTimestamp": 1753852939,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "八和和牛燒肉專門店/黑武士老火鍋/SSAM韓式燒肉專門店_八和餐飲有限公司",
      "custNo": "130000000162856",
      "description": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "descSnippet": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "mrtDist": 0.08,
      "jobAddress": "安和路1段102巷4號",
      "jobAddrNo": 6001001005,
      "jobAddrNoDesc": "台北市大安區",
      "jobName": "【八和和牛燒肉-安和店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNameSnippet": "【八和和牛燒肉-安和店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNo": "14524766",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0329786,
      "lon": 121.5521903,
      "link": {
        "job": "https://www.104.com.tw/job/8nbdq",
        "cust": "https://www.104.com.tw/company/1a2x6blgyw",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nbdq?channel=104rpt"
      },
      "major": [],
      "mrt": "99001002003",
      "mrtDesc": "捷運信義安和站",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf20": {
          "desc": "",
          "param": "wf20"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "landmark": {
          "desc": "距捷運信義安和站約80公尺"
        }
      },
      "s9": [
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.8844506427194253,
      "jobCat": [
        2001001001,
        2001001002,
        2006002002
      ],
      "labels": [
        "c@wf20",
        "c@wf17",
        "foreigners@foreigners_tick",
        "c@wf29",
        "c@wf3",
        "foreigners@foreigners",
        "c@wf1",
        "foreigners@chineseDiasporas",
        "c@wf7",
        "c@wf2",
        "c@wf9",
        "c@wf28",
        "foreigners@overseasStudents_tick",
        "c@wf4",
        "c@wf14"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        65536,
        131072
      ],
      "employeeCount": 130,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753859626,
        "lastCustReplyTimestamp": 1753859315,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 0,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "老王炸烤_三匠國際興業股份有限公司",
      "custNo": "130000000206725",
      "description": "老王炸烤/南京店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：台北市松山區健康路220號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "descSnippet": "老王炸烤/南京店-誠徵門市主管\n\n基本主管底薪：50000 起\n工作時間：16：00-01：00 (到打洋收完)\n\n上班地點：台北市松山區健康路220號\n具餐飲門市三年以上管理經驗 優先錄取\n\n工作內容【門市主管】 \n1.\t負責門市營業業績目標達成\n2.\t維持門市環境的清潔衛生\n3.\t客戶關係管理維護\n4.\t管理店內員工的招募、出缺勤、業績、訓練、升遷。\n5.\t掌管物料的進出貨數量和品質。\n6.\t協助當地商圈廣告活動規劃\n7.\t協調及促銷宣傳、商圈市場調查\n8.\t商品銷售與流程品質控管\n9.\t解決較複雜的顧客抱怨\n10.\t店舖行政工作管理與帳務處理\n11.\t門市人力調配。\n12.\t制訂餐廳作業標準流程。\n13.\t監督食物之製作流程、份量及擺盤等事務。\n14.\t親自品嚐餐食以確保菜色品質穩定。\n15.\t協調外場點餐與內場廚師料理的順序與速度。\n16.\t主動詢問顧客需求與對服務之滿意度。\n17.\t建立員工表現及顧客滿意度的標準。\n18.\t進行門市營運成本之管控。\n\n工作態度：積極是基本「禮貌尊重」最為重要\n\n月休8天(排班)、勞、健保、勞退\n另有全勤獎金、月獎金、年資獎金、考核獎金\n\n試用期2~3個月 (除個人特殊狀況)\n試用期門市獎金（入職第一個月沒有）\n\n上班非常忙碌，有心理準備再來，不缺人，缺人才\n看過履歷覺得合適者會通知面試，老王感謝您～",
      "mrtDist": 0.36,
      "jobAddress": "健康路220號",
      "jobAddrNo": 6001001004,
      "jobAddrNoDesc": "台北市松山區",
      "jobName": "門市主管(南京店)",
      "jobNameSnippet": "門市主管(南京店)",
      "jobNo": "13425603",
      "jobRo": 1,
      "jobType": 2,
      "lat": 25.0538007,
      "lon": 121.5614413,
      "link": {
        "job": "https://www.104.com.tw/job/7zr9f",
        "cust": "https://www.104.com.tw/company/1a2x6bmeth",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7zr9f?channel=104rpt"
      },
      "major": [],
      "mrt": "99001003019",
      "mrtDesc": "捷運南京三民站",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 70000,
      "salaryLow": 50000,
      "tags": {
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "landmark": {
          "desc": "距捷運南京三民站約360公尺"
        }
      },
      "s9": [
        1,
        2,
        8
      ],
      "s5": 0,
      "d3": "(依公司規定排班)",
      "hrBehaviorPR": 0.9179669378472262,
      "jobCat": [
        2006002002,
        2001001002,
        2005002001
      ],
      "labels": [
        "c@wf26",
        "c@wf28",
        "c@wf2",
        "senior@senior_job_A",
        "45plus@45invited",
        "c@wf17",
        "foreigners@overseasStudents_tick",
        "c@wf4",
        "c@wf3",
        "c@wf1",
        "foreigners@chineseDiasporas",
        "c@wf31",
        "c@wf27",
        "foreigners@foreignStudents",
        "c@wf10",
        "foreigners@foreigners_tick",
        "foreigners@foreigners",
        "college@student_invited",
        "c@wf7",
        "c@wf29"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        16384,
        32768,
        65536,
        131072
      ],
      "employeeCount": 50,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753851307,
        "lastCustReplyTimestamp": 1753860040,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 4,
      "coIndustry": 1009006001,
      "coIndustryDesc": "保全樓管相關業",
      "custName": "齊家公寓大廈管理維護股份有限公司",
      "custNo": "84760406000",
      "description": "1. 提供管委會全方位的整體性大樓綜合管理服務，及住戶生活服務等業務規劃執行。\n2. 召開會議並執行會議決議事項。\n3. 警衛安全管理、監督廠商設備維護、清潔環保工作。\n4. 處理社區行政事務、規畫財務、安全、環境、設備等管理事項規劃與執行。\n5. 處理住戶客訴，並規勸喧嘩及濫用公物等行為。\n6. 執行公共設施的各項使用管理辦法。\n7. 其他交辦事項。",
      "descSnippet": "1. 提供管委會全方位的整體性大樓綜合管理服務，及住戶生活服務等業務規劃執行。\n2. 召開會議並執行會議決議事項。\n3. 警衛安全管理、監督廠商設備維護、清潔環保工作。\n4. 處理社區行政事務、規畫財務、安全、環境、設備等管理事項規劃與執行。\n5. 處理住戶客訴，並規勸喧嘩及濫用公物等行為。\n6. 執行公共設施的各項使用管理辦法。\n7. 其他交辦事項。",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6001005007,
      "jobAddrNoDesc": "桃園市桃園區",
      "jobName": "社區經理-桃園區-桃園齊家",
      "jobNameSnippet": "社區經理-桃園區-桃園齊家",
      "jobNo": "12262130",
      "jobRo": 1,
      "jobType": 2,
      "lat": 24.9934099,
      "lon": 121.2969674,
      "link": {
        "job": "https://www.104.com.tw/job/7atiq",
        "cust": "https://www.104.com.tw/company/12xs5nu8",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7atiq?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 47000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.7433507960035756,
      "jobCat": [
        2017002005,
        2001001002,
        2002001001
      ],
      "labels": [
        "senior@senior_job_A",
        "c@wf7",
        "45plus@45invited",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048,
        4096,
        32768
      ],
      "employeeCount": 750,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753775815,
        "lastCustReplyTimestamp": 1752584612,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002003,
      "coIndustryDesc": "其他餐飲業",
      "custName": "花磚甜點有限公司",
      "custNo": "130000000237825",
      "description": "1.負責商品介紹銷售與諮詢、收銀結帳包裝等。\r\n2.協助商品訂貨、庫存盤點管理，商品排面整理。\r\n3.協助器具清洗、門市清潔、商品陳列佈置等。\r\n4.協助門市進行活動規劃，並提出營運建議。\r\n5.學習人力調度與人員管理。\r\n\r\n\r\n門市儲備幹部是本公司培養未來銷售管理人才的重要崗位，透過訓練與實務操作，讓您成為專業的銷售幹部，展現您的銷售潛力。\r\n歡迎有興趣的您加入我們的團隊，一同成長與發展！",
      "descSnippet": "1.負責商品介紹銷售與諮詢、收銀結帳包裝等。\r\n2.協助商品訂貨、庫存盤點管理，商品排面整理。\r\n3.協助器具清洗、門市清潔、商品陳列佈置等。\r\n4.協助門市進行活動規劃，並提出營運建議。\r\n5.學習人力調度與人員管理。\r\n\r\n\r\n門市儲備幹部是本公司培養未來銷售管理人才的重要崗位，透過訓練與實務操作，讓您成為專業的銷售幹部，展現您的銷售潛力。\r\n歡迎有興趣的您加入我們的團隊，一同成長與發展！",
      "mrtDist": 0,
      "jobAddress": "忠誠路二段154巷7號1樓",
      "jobAddrNo": 6001001008,
      "jobAddrNoDesc": "台北市士林區",
      "jobName": "店員門市",
      "jobNameSnippet": "店員門市",
      "jobNo": "14198781",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.115113,
      "lon": 121.5323589,
      "link": {
        "job": "https://www.104.com.tw/job/8gbul",
        "cust": "https://www.104.com.tw/company/1a2x6bn2td",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8gbul?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 40000,
      "salaryLow": 34000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "10:00-19:00",
      "hrBehaviorPR": 0.6176480560195472,
      "jobCat": [
        2005002001,
        2005002004,
        2001001002
      ],
      "labels": [
        "c@wf2",
        "c@wf29",
        "c@wf1",
        "c@wf3",
        "c@wf7"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 0,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753864415,
        "lastCustReplyTimestamp": 1753890399,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 5,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "八和和牛燒肉專門店/黑武士老火鍋/SSAM韓式燒肉專門店_八和餐飲有限公司",
      "custNo": "130000000162856",
      "description": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "descSnippet": "我們在找這樣的你  \n-學歷不限，對餐飲充滿熱忱\n-具備不同程度的餐飲經驗，並渴望發展長期職涯  \n*儲備店長 ：2 年以上餐飲服務業經驗，具管理或營運經驗  \n*儲備副店長 / 儲備副料理長：2 年以上餐飲經驗，曾負責營運值班  \n*儲備值班幹部：一年以上餐飲經驗，對未來充滿企圖心  \n\n●你的工作舞台  \n營運管理 & 顧客服務  \n- 確保門市順利運作，帶領團隊達成營運目標  \n- 安排人力調度、培訓夥伴，打造高效團隊  \n- 提供優質的餐點與服務，讓顧客滿意而歸  \n\n廚房管理 & 出餐品質  \n- 確保烹飪流程符合食品安全標準  \n- 協助餐點製作，維持穩定的高品質  \n- 與外場緊密配合，提升出餐流暢度  \n\n\n我們提供完善的培訓計畫，讓你快速成長、晉升，透明且完整的調薪制度，固定每半年考核，能力強調薪快，餐飲業高薪不是夢! 開啟餐飲職涯新篇章！\n (1) 每季各店損益表之稅後淨利，可獲得額外激勵獎金\n (2) You Can & We Pay，工作所獲得的報酬源自於【信任】，因為相信你/妳可以勝任未來的職務所以我們願意投資。\n\n薪資 (以上薪資不含加班費，加班費另計)\n1. 儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)\n2. 副店長    ，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n3. 副料理長，薪資 $52,000~65,000 + (另享每季分紅獎金$6000~39000)\n4. 店長        ，薪資 $65,000~92,000 + (另享每季分紅獎金$6000~51000)\n5. 計時社員，薪資 $230元/HR(含全勤獎金$20/每小時) + (另享每季分紅獎金$3000~12000)\n\n公司福利\n季獎金：\n1. 儲備幹部，每季分紅獎金$6000~27000\n2. 副店長    ，每季分紅獎金$6000~39000\n3. 副料理長，每季分紅獎金$6000~39000\n4. 店長        ，每季分紅獎金$6000~51000\n5. 計時社員，每季分紅獎金$3000~12000\n介紹獎金 $8000(過試用期3000，滿半年再加5000元)申請者及被介紹者都需再職。\n生日禮金+(正職人員另享有1天生日假)\n特殊技能津貼:(切肉津貼，通過考核於每月發放)\n三節禮金：春節、端午、中秋。\n\n享食好福利 \n\n員工餐 - 每日供應美味員工餐，讓你吃飽喝好。\n店鋪用餐 - 員工用餐折扣7折。\n店鋪聚餐 - 每月同仁定期聚餐/娛樂歡聚活動\n國內外員工旅遊(滿一年享有全額補助)。\n健康檢查 - 到職滿一年，免費的年度身體健康檢查\n\n基本的不能少 \n勞保、健保、每月提撥6%勞退 是基本的喔！",
      "mrtDist": 0.38,
      "jobAddress": "敦化北路100號",
      "jobAddrNo": 6001001004,
      "jobAddrNoDesc": "台北市松山區",
      "jobName": "【八和和牛燒肉-敦北店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNameSnippet": "【八和和牛燒肉-敦北店】一頭班 外場儲備幹部，薪資 $40,000~55,000 + (另享每季分紅獎金$6000~27000)",
      "jobNo": "14524772",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.053227,
      "lon": 121.5485558,
      "link": {
        "job": "https://www.104.com.tw/job/8nbdw",
        "cust": "https://www.104.com.tw/company/1a2x6blgyw",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8nbdw?channel=104rpt"
      },
      "major": [],
      "mrt": "99001003018",
      "mrtDesc": "捷運台北小巨蛋站",
      "optionEdu": [
        2,
        3,
        4,
        5,
        6
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf17": {
          "desc": "",
          "param": "wf17"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "wf20": {
          "desc": "",
          "param": "wf20"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "landmark": {
          "desc": "距捷運台北小巨蛋站約380公尺"
        }
      },
      "s9": [
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.8895634661416718,
      "jobCat": [
        2001001002,
        2001001001,
        2006002002
      ],
      "labels": [
        "college@student_invited",
        "foreigners@overseasStudents_tick",
        "c@wf7",
        "c@wf9",
        "c@wf17",
        "c@wf4",
        "c@wf28",
        "c@wf1",
        "c@wf3",
        "c@wf14",
        "c@wf20",
        "foreigners@foreigners_tick",
        "c@wf29",
        "foreigners@foreigners",
        "c@wf2"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        16,
        32,
        64,
        1024,
        2048,
        65536,
        131072
      ],
      "employeeCount": 130,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753677685,
        "lastCustReplyTimestamp": 1753859339,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 6,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "1.負責產線作業及人員管理，達成生產目標\n2.執行生管排程計劃，反饋並解決生產異常\n3.推動精實專案，落實各項專案計劃並提供反饋\n4.負責生產安全，減少工傷發生\n5.配合內外部環安衛稽查及改善\n\n本職位負責負責廠務管理和產線作業等相關工作，為公司製造類的核心職務，具有重要的意義和發揮空間。\n如果您符合以上要求且對此職位有興趣，請提交您的履歷表並聯繫我們，期待您加入我們的專業團隊，謝謝！",
      "descSnippet": "1.負責產線作業及人員管理，達成生產目標\n2.執行生管排程計劃，反饋並解決生產異常\n3.推動精實專案，落實各項專案計劃並提供反饋\n4.負責生產安全，減少工傷發生\n5.配合內外部環安衛稽查及改善\n\n本職位負責負責廠務管理和產線作業等相關工作，為公司製造類的核心職務，具有重要的意義和發揮空間。\n如果您符合以上要求且對此職位有興趣，請提交您的履歷表並聯繫我們，期待您加入我們的專業團隊，謝謝！",
      "mrtDist": 0,
      "jobAddress": "",
      "jobAddrNo": 6003002010,
      "jobAddrNoDesc": "印尼",
      "jobName": "【生產製造】產品製造部培訓幹部(印尼)",
      "jobNameSnippet": "【生產製造】產品製造部培訓幹部(印尼)",
      "jobNo": "12435145",
      "jobRo": 1,
      "jobType": 2,
      "lat": -0.789275,
      "lon": 113.921327,
      "link": {
        "job": "https://www.104.com.tw/job/7ej0p",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7ej0p?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4,
        5
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.8037339186910918,
      "jobCat": [
        2001001002,
        2009002001,
        2009001001
      ],
      "labels": [
        "c@wf34",
        "c@wf10",
        "c@wf16",
        "c@wf27",
        "c@wf1",
        "c@wf9",
        "foreigners@foreigners"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753864419,
        "lastCustReplyTimestamp": 1753064041,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 1,
      "coIndustry": 1016002001,
      "coIndustryDesc": "餐館業",
      "custName": "永芯茶檔有限公司",
      "custNo": "130000000229814",
      "description": "\n準備食材並烹調菜餚\n掌控菜餚的擺盤與出菜的順序\n測量食材的容量與重量並協助處理廚房事務完成主廚指派工作\n\n【中壢內壢店】桃園市中壢區成章四街61號\n\n工作時段:(工作時段依現場營運狀況調整)\n10:00-21:30\n\n一天工作八小時+1至2小時休息時間\n月休8-10天(按法定休假日排休)\n\n其他福利項目 員工勞健保並額外替員工承擔「團保職災險」。\n每日供餐、定期聚餐。\n\n年終獎金、分紅獎金、月獎金、季獎金\n讓我們一起朝目標衝刺！\n\n工作環境極佳\n培訓優秀幹部，員工的成長我們看得見\n如果你對我們的職缺有興趣，想在一個公平且友善的環境工作，歡迎你跟我們聯絡。\n\n★起薪依學經歷而定★\n★獎金依個人績效能力而定★\n★依個人工作能力不定期調薪★",
      "descSnippet": "\n準備食材並烹調菜餚\n掌控菜餚的擺盤與出菜的順序\n測量食材的容量與重量並協助處理廚房事務完成主廚指派工作\n\n【中壢內壢店】桃園市中壢區成章四街61號\n\n工作時段:(工作時段依現場營運狀況調整)\n10:00-21:30\n\n一天工作八小時+1至2小時休息時間\n月休8-10天(按法定休假日排休)\n\n其他福利項目 員工勞健保並額外替員工承擔「團保職災險」。\n每日供餐、定期聚餐。\n\n年終獎金、分紅獎金、月獎金、季獎金\n讓我們一起朝目標衝刺！\n\n工作環境極佳\n培訓優秀幹部，員工的成長我們看得見\n如果你對我們的職缺有興趣，想在一個公平且友善的環境工作，歡迎你跟我們聯絡。\n\n★起薪依學經歷而定★\n★獎金依個人績效能力而定★\n★依個人工作能力不定期調薪★",
      "mrtDist": 0,
      "jobAddress": "成章四街61號",
      "jobAddrNo": 6001005001,
      "jobAddrNoDesc": "桃園市中壢區",
      "jobName": "桃園內壢店【內場儲備幹部】熱忱內場廚房【薪資37,000元至45,000元(含全勤)】",
      "jobNameSnippet": "桃園內壢店【內場儲備幹部】熱忱內場廚房【薪資37,000元至45,000元(含全勤)】",
      "jobNo": "14045037",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.974788,
      "lon": 121.2567711,
      "link": {
        "job": "https://www.104.com.tw/job/8d17x",
        "cust": "https://www.104.com.tw/company/1a2x6bmwmu",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/8d17x?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        1,
        2,
        3,
        4,
        5
      ],
      "period": 4,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 45000,
      "salaryLow": 38000,
      "tags": {
        "wf26": {
          "desc": "",
          "param": "wf26"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf30": {
          "desc": "",
          "param": "wf30"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf12": {
          "desc": "",
          "param": "wf12"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf3": {
          "desc": "",
          "param": "wf3"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.9666626734506133,
      "jobCat": [
        2006001002,
        2006001004,
        2001001002
      ],
      "labels": [
        "senior@senior_job_E",
        "c@wf26",
        "c@wf10",
        "foreigners@overseasStudents",
        "45plus@45invited",
        "c@wf18",
        "college@student_invited",
        "c@wf30",
        "c@wf2",
        "c@wf4",
        "c@wf12",
        "c@wf1",
        "foreigners@foreigners",
        "foreigners@chineseDiasporas",
        "foreigners@foreignStudents",
        "c@wf29",
        "c@wf3"
      ],
      "languageRequirements": [],
      "acceptRole": [
        2,
        4,
        8,
        2048
      ],
      "employeeCount": 20,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753875874,
        "lastCustReplyTimestamp": 1753867603,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 3,
      "coIndustry": 1002003001,
      "coIndustryDesc": "鞋類製造業",
      "custName": "賜昌集團_英屬維京群島商賜昌有限公司",
      "custNo": "130000000116851",
      "description": "．學習各類鞋材知識及系統開單流程\n．學習工作計畫擬定及任務安排方法\n．供應商管理、交期追蹤\n．協助主管建構當地團隊及與越南區對應\n．協助主管處理異常與索賠",
      "descSnippet": "．學習各類鞋材知識及系統開單流程\n．學習工作計畫擬定及任務安排方法\n．供應商管理、交期追蹤\n．協助主管建構當地團隊及與越南區對應\n．協助主管處理異常與索賠",
      "mrtDist": 0,
      "jobAddress": "Cirebon",
      "jobAddrNo": 6003002010,
      "jobAddrNoDesc": "印尼",
      "jobName": "【行政/幕僚】採購培訓幹部(印尼)",
      "jobNameSnippet": "【行政/幕僚】採購培訓幹部(印尼)",
      "jobNo": "13315358",
      "jobRo": 1,
      "jobType": 2,
      "lat": -6.6898876,
      "lon": 108.4750846,
      "link": {
        "job": "https://www.104.com.tw/job/7xe72",
        "cust": "https://www.104.com.tw/company/1a2x6bkhgz",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7xe72?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        4
      ],
      "period": 0,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "wf34": {
          "desc": "",
          "param": "wf34"
        },
        "wf16": {
          "desc": "",
          "param": "wf16"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf9": {
          "desc": "",
          "param": "wf9"
        },
        "wf27": {
          "desc": "",
          "param": "wf27"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "",
      "hrBehaviorPR": 0.6509645956618654,
      "jobCat": [
        2001001002,
        2011001003
      ],
      "labels": [
        "c@wf1",
        "c@wf34",
        "c@wf16",
        "c@wf10",
        "c@wf9",
        "c@wf27"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 8,
            "speaking": 8,
            "reading": 8,
            "writing": 8
          }
        }
      ],
      "acceptRole": [
        2
      ],
      "employeeCount": 120000,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753861718,
        "lastCustReplyTimestamp": 1751964746,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 7,
      "coIndustry": 1003001015,
      "coIndustryDesc": "綜合商品批發代理業",
      "custName": "馨昌股份有限公司",
      "custNo": "22946706000",
      "description": "\r\n1.負責主管交辦專案業務計劃擬定、推動、執行，隨時掌控進度。\r\n2.負責行政事務流程之溝通、整合及規劃。\r\n3.配合公司營運策略，協助進行企劃與專案管理、進行跨技術的統合。\r\n4.協助市場分析，專案成效追蹤及費用相關報告。\r\n5.可配合主管假日專案業務行程。",
      "descSnippet": "\r\n1.負責主管交辦專案業務計劃擬定、推動、執行，隨時掌控進度。\r\n2.負責行政事務流程之溝通、整合及規劃。\r\n3.配合公司營運策略，協助進行企劃與專案管理、進行跨技術的統合。\r\n4.協助市場分析，專案成效追蹤及費用相關報告。\r\n5.可配合主管假日專案業務行程。",
      "mrtDist": 0.49,
      "jobAddress": "基隆路二段51號6樓之6(110)",
      "jobAddrNo": 6001001007,
      "jobAddrNoDesc": "台北市信義區",
      "jobName": "董事長室特助",
      "jobNameSnippet": "董事長室特助",
      "jobNo": "10229416",
      "jobRo": 1,
      "jobType": 0,
      "lat": 25.0309321,
      "lon": 121.558469,
      "link": {
        "job": "https://www.104.com.tw/job/6392g",
        "cust": "https://www.104.com.tw/company/ajhvjo0",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/6392g?channel=104rpt"
      },
      "major": [
        "商業及管理學科類",
        "企業管理相關",
        "國際貿易相關"
      ],
      "mrt": "99001002002",
      "mrtDesc": "捷運台北101/世貿站",
      "optionEdu": [
        4,
        5,
        6
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 10,
      "salaryHigh": 0,
      "salaryLow": 0,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf10": {
          "desc": "",
          "param": "wf10"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        },
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf18": {
          "desc": "",
          "param": "wf18"
        },
        "wf32": {
          "desc": "",
          "param": "wf32"
        },
        "wf1": {
          "desc": "",
          "param": "wf1"
        },
        "landmark": {
          "desc": "距捷運台北101/世貿站約490公尺"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "9:00~18:00",
      "hrBehaviorPR": 0.6778261804274771,
      "jobCat": [
        2001001003
      ],
      "labels": [
        "c@wf7",
        "c@wf10",
        "c@wf28",
        "c@wf31",
        "c@wf2",
        "c@wf18",
        "c@wf32",
        "c@wf1"
      ],
      "languageRequirements": [
        {
          "language": 1,
          "ability": {
            "listening": 2,
            "speaking": 2,
            "reading": 2,
            "writing": 2
          }
        }
      ],
      "acceptRole": [],
      "employeeCount": 45,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753677543,
        "lastCustReplyTimestamp": 1753777038,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 7,
      "coIndustry": 1009006001,
      "coIndustryDesc": "保全樓管相關業",
      "custName": "台灣國際物業_台灣國際公寓大廈管理維護有限公司",
      "custNo": "70654740000",
      "description": "1.本職務為擔任高級社區物業管理工作。\r\n2.具飯店俱樂休閒事業或相關社區實務經驗佳。\r\n3.具有物業經理之相關證照及經驗優先錄取。\r\n4.需具一般電腦文書作業系統及會計基礎概念。\r\n5.本公司有強大之溝通能力、工作支援性與人員教育訓練。\r\n6.免費培訓升為物業經理之專業知識。\r\n7.公司定期參與工會舉辦團體物業經理之報考相關證照與獎勵。\r\n8.公司之薪資福利待遇佳，值得您選擇！\r\n9.若有相關之證照﹝防火管理人及總幹事證照﹞請影印一份攜帶至本公司。\r\n（一）深入物業組織學習能力、專業教育訓練，有效累積物業管理的專業知識及經驗。\r\n（二）不與同業惡性競爭，培養具有產業差異化且互補的核心專長業務。\r\n（三）研訂長期發展策略，以勞力密集的建築物管理維護業務為基礎，朝資訊化、人文涵養方面多角化發展，拓展高附加價值的商業支援、資產管理及生活服務的業務。\r\n（四）提升業界形象，改善工作條件，以吸引更多優秀人才投入服務、提升企業優質文化與同仁工作尊嚴。\r\n（五）培育專業物業經理人以儲備未來業務發展的需要，與國際品牌競爭。",
      "descSnippet": "1.本職務為擔任高級社區物業管理工作。\r\n2.具飯店俱樂休閒事業或相關社區實務經驗佳。\r\n3.具有物業經理之相關證照及經驗優先錄取。\r\n4.需具一般電腦文書作業系統及會計基礎概念。\r\n5.本公司有強大之溝通能力、工作支援性與人員教育訓練。\r\n6.免費培訓升為物業經理之專業知識。\r\n7.公司定期參與工會舉辦團體物業經理之報考相關證照與獎勵。\r\n8.公司之薪資福利待遇佳，值得您選擇！\r\n9.若有相關之證照﹝防火管理人及總幹事證照﹞請影印一份攜帶至本公司。\r\n（一）深入物業組織學習能力、專業教育訓練，有效累積物業管理的專業知識及經驗。\r\n（二）不與同業惡性競爭，培養具有產業差異化且互補的核心專長業務。\r\n（三）研訂長期發展策略，以勞力密集的建築物管理維護業務為基礎，朝資訊化、人文涵養方面多角化發展，拓展高附加價值的商業支援、資產管理及生活服務的業務。\r\n（四）提升業界形象，改善工作條件，以吸引更多優秀人才投入服務、提升企業優質文化與同仁工作尊嚴。\r\n（五）培育專業物業經理人以儲備未來業務發展的需要，與國際品牌競爭。",
      "mrtDist": 0,
      "jobAddress": "（依照公司規定分派）",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "台中市豪宅物業經理",
      "jobNameSnippet": "台中市豪宅物業經理",
      "jobNo": "4855867",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.162568013470807,
      "lon": 120.64123167036131,
      "link": {
        "job": "https://www.104.com.tw/job/2w2t7",
        "cust": "https://www.104.com.tw/company/wgi0aao",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/2w2t7?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        3,
        4,
        5
      ],
      "period": 3,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 50000,
      "salaryLow": 38000,
      "tags": {
        "wf7": {
          "desc": "",
          "param": "wf7"
        },
        "wf31": {
          "desc": "",
          "param": "wf31"
        }
      },
      "s9": [
        1
      ],
      "s5": 0,
      "d3": "09:00~18:00",
      "hrBehaviorPR": 0.6610461761278131,
      "jobCat": [
        2001001001,
        2005001001,
        2017002005
      ],
      "labels": [
        "senior@senior_job_G",
        "c@wf7",
        "45plus@45invited",
        "c@wf31"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048,
        4096,
        32768
      ],
      "employeeCount": 800,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": null,
        "lastCustReplyTimestamp": 1752044986,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    },
    {
      "appearDate": "20250731",
      "applyCnt": 8,
      "coIndustry": 1003002003,
      "coIndustryDesc": "鞋類／布類／服飾品零售業",
      "custName": "Nab_Shop女裝_自然堂國際時尚有限公司",
      "custNo": "24350667000",
      "description": "工作內容： \n1. 門市管理及推動業績目標達成。 \n2. 商品介紹及銷售服務。 \n3. 排班及人員填補。 \n4. 獨立性高、具備帶人及管理經驗。 \n5.負責員工的招募、訓練、稽核與管理與調度。\n6. 可支援台中區門市巡店及人員調度。\n\n\n公司好康: \n1. 月休8-9天、 滿半年特休3天、滿一年特休7天。\n2. 享有勞、健保與6%勞退新制 。\n3. 全勤獎金、伙食津貼、業績獎金、全區月達標獎金。\n4. 員購優惠 。\n5.生日、中秋、端午禮金(品)。\n6. 補助津貼(結婚、生育) 。\n7. 公司聚餐，培養姊妹滔同事們的情誼。\n\n公司悄悄話: \n只要是人才，有自信心，業績表現優異，公司待遇優於其他同業。 \nNab_Shop在台中、新竹已設有9家門市，陸續在拓展中，如果屬於你的感覺跟 \n風格，趕快加入我們行列吧 。\n有耐心,善於溝通,並喜歡流行服飾者,我們歡迎你。\n具備一年以上相關門市管理經驗者優先面試。\n\n相關資料請參考: \n官網: www.nab.com.tw\nIG: nabshop.ig\n",
      "descSnippet": "工作內容： \n1. 門市管理及推動業績目標達成。 \n2. 商品介紹及銷售服務。 \n3. 排班及人員填補。 \n4. 獨立性高、具備帶人及管理經驗。 \n5.負責員工的招募、訓練、稽核與管理與調度。\n6. 可支援台中區門市巡店及人員調度。\n\n\n公司好康: \n1. 月休8-9天、 滿半年特休3天、滿一年特休7天。\n2. 享有勞、健保與6%勞退新制 。\n3. 全勤獎金、伙食津貼、業績獎金、全區月達標獎金。\n4. 員購優惠 。\n5.生日、中秋、端午禮金(品)。\n6. 補助津貼(結婚、生育) 。\n7. 公司聚餐，培養姊妹滔同事們的情誼。\n\n公司悄悄話: \n只要是人才，有自信心，業績表現優異，公司待遇優於其他同業。 \nNab_Shop在台中、新竹已設有9家門市，陸續在拓展中，如果屬於你的感覺跟 \n風格，趕快加入我們行列吧 。\n有耐心,善於溝通,並喜歡流行服飾者,我們歡迎你。\n具備一年以上相關門市管理經驗者優先面試。\n\n相關資料請參考: \n官網: www.nab.com.tw\nIG: nabshop.ig\n",
      "mrtDist": 0,
      "jobAddress": "文華路45號",
      "jobAddrNo": 6001008007,
      "jobAddrNoDesc": "台中市西屯區",
      "jobName": "Nab 女裝-【台中區】儲備幹部",
      "jobNameSnippet": "Nab 女裝-【台中區】儲備幹部",
      "jobNo": "12364218",
      "jobRo": 1,
      "jobType": 0,
      "lat": 24.1772192,
      "lon": 120.6464504,
      "link": {
        "job": "https://www.104.com.tw/job/7d0ai",
        "cust": "https://www.104.com.tw/company/b6prawo",
        "applyAnalyze": "https://www.104.com.tw/jobs/apply/analysis/7d0ai?channel=104rpt"
      },
      "major": [],
      "mrt": "",
      "mrtDesc": "",
      "optionEdu": [
        2,
        3,
        4,
        5
      ],
      "period": 2,
      "remoteWorkType": 0,
      "s10": 50,
      "salaryHigh": 55000,
      "salaryLow": 40000,
      "tags": {
        "wf2": {
          "desc": "",
          "param": "wf2"
        },
        "wf14": {
          "desc": "",
          "param": "wf14"
        },
        "wf28": {
          "desc": "",
          "param": "wf28"
        },
        "wf29": {
          "desc": "",
          "param": "wf29"
        },
        "wf4": {
          "desc": "",
          "param": "wf4"
        },
        "wf7": {
          "desc": "",
          "param": "wf7"
        }
      },
      "s9": [
        1,
        2
      ],
      "s5": 256,
      "d3": "",
      "hrBehaviorPR": 0.925921316149764,
      "jobCat": [
        2005002002,
        2005002001,
        2001001002
      ],
      "labels": [
        "senior@senior_job_E",
        "c@wf2",
        "c@wf14",
        "45plus@45invited",
        "c@wf28",
        "c@wf29",
        "c@wf4",
        "c@wf7"
      ],
      "languageRequirements": [],
      "acceptRole": [
        64,
        2048
      ],
      "employeeCount": 20,
      "isSave": null,
      "interactionRecord": {
        "lastProcessedResumeAtTime": 1753867143,
        "lastCustReplyTimestamp": 1753694861,
        "nowTimestamp": 1753913482
      },
      "isApplied": null,
      "applyDate": null,
      "userApplyCount": null
    }
  ],
  "metadata": {
    "pagination": {
      "count": 22,
      "currentPage": 2,
      "lastPage": 150,
      "total": 21314
    },
    "isPreciseHotJob": false,
    "filterQuery": {
      "order": 11,
      "asc": 0,
      "jobcat": [
        "2001001000"
      ],
      "ro": [
        0
      ],
      "scstrict": 0,
      "scneg": 0,
      "page": 2,
      "pagesize": 20
    },
    "personalBoost": 0
  }
}


================================================
FILE: crawler/project_104/parser_apidata_104.py
================================================
import re
from datetime import datetime
from typing import Optional
import structlog

from crawler.database.schemas import (
    JobPydantic,
    SourcePlatform,
    JobStatus,
    JobType,
    SalaryType,
)

logger = structlog.get_logger(__name__)

# 104 API 的 jobType 到我們內部 JobType Enum 的映射
JOB_TYPE_MAPPING = {
    0: JobType.FULL_TIME, # 0 也代表全職
    1: JobType.FULL_TIME,
    2: JobType.PART_TIME,
    3: JobType.INTERNSHIP,
    4: JobType.CONTRACT,  # 派遣
    5: JobType.TEMPORARY,  # 兼職/計時
}

# 104 API 的教育程度 (optionEdu) 映射
EDUCATION_MAPPING_104 = {
    1: "不拘",
    2: "國中",
    3: "高中",
    4: "專科",
    5: "大學",
    6: "碩士",
    7: "博士",
}

# 104 API 的工作經驗 (period) 映射
EXPERIENCE_MAPPING_104 = {
    0: "不拘",
    1: "1年以下",
    2: "1-3年",
    3: "3-5年",
    4: "5-10年",
    5: "10年以上",
}


def parse_salary(
    salary_text: str,
) -> (Optional[int], Optional[int], Optional[SalaryType]):
    salary_min, salary_max, salary_type = None, None, None
    text = salary_text.replace(",", "").lower()

    # 月薪 (範圍)
    match_monthly_range = re.search(r"月薪([0-9]+)(?:[至~])([0-9]+)元", text)
    if match_monthly_range:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_range.group(1))
        salary_max = int(match_monthly_range.group(2))
        return salary_min, salary_max, salary_type

    # 月薪 (單一數值)
    match_monthly_single = re.search(r"月薪([0-9]+)元", text)
    if match_monthly_single:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_single.group(1))
        salary_max = int(match_monthly_single.group(1))
        return salary_min, salary_max, salary_type

    # 月薪 (以上)
    match_monthly_above = re.search(r"月薪([0-9]+)元以上", text)
    if match_monthly_above:
        salary_type = SalaryType.MONTHLY
        salary_min = int(match_monthly_above.group(1))
        salary_max = 9999999 # 設定一個足夠大的上限值
        return salary_min, salary_max, salary_type

    # 年薪
    match_yearly = re.search(r"年薪([0-9]+)萬(?:[至~])([0-9]+)萬", text) or re.search(
        r"年薪([0-9]+)萬以上", text
    )
    if match_yearly:
        salary_type = SalaryType.YEARLY
        salary_min = int(match_yearly.group(1)) * 10000
        if len(match_yearly.groups()) > 1 and match_yearly.group(2):
            salary_max = int(match_yearly.group(2)) * 10000
        return salary_min, salary_max, salary_type

    # 時薪
    match_hourly = re.search(r"時薪([0-9]+)元", text)
    if match_hourly:
        salary_type = SalaryType.HOURLY
        salary_min = int(match_hourly.group(1))
        salary_max = int(match_hourly.group(1))
        return salary_min, salary_max, salary_type

    # 日薪
    match_daily = re.search(r"日薪([0-9]+)元", text)
    if match_daily:
        salary_type = SalaryType.DAILY
        salary_min = int(match_daily.group(1))
        salary_max = int(match_daily.group(1))
        return salary_min, salary_max, salary_type

    # 論件計酬
    if "論件計酬" in text:
        salary_type = SalaryType.BY_CASE
        return None, None, salary_type

    # 面議
    if "面議" in text:
        salary_type = SalaryType.NEGOTIABLE
        return None, None, salary_type

    return salary_min, salary_max, salary_type


def parse_job_item_to_pydantic(job_item: dict) -> Optional[JobPydantic]:
    """
    從 104 API 的單一職缺項目(dict)解析並轉換為 JobPydantic 物件。
    此函式可處理來自「列表頁 API」和「單一職缺 API」的回應。
    """
    try:
        # 判斷資料來源是列表頁 API 還是單一職缺 API
        # 列表頁 API 的特徵是 jobName, jobNo, link, description, appearDate, custName 直接在 job_item 頂層
        # 單一職缺 API 的特徵是包含 'header' 和 'jobDetail' 鍵
        is_single_job_api = "header" in job_item and "jobDetail" in job_item

        if is_single_job_api:  # 來自單一職缺 API (data.data)
            header = job_item.get("header", {})
            job_detail = job_item.get("jobDetail", {})
            condition = job_item.get("condition", {})

            job_id_match = re.search(r'/analysis/([a-zA-Z0-9]+)', header.get("analysisUrl", ""))
            job_id = job_id_match.group(1) if job_id_match else None
            url = header.get("analysisUrl", "")
            cust_no = header.get("custNo")
            cust_name = header.get("custName")
            cust_url = header.get("custUrl", "")
            title = header.get("jobName")
            description = job_detail.get("jobDescription")
            raw_job_type = job_detail.get("jobType")
            job_addr_region = job_detail.get("addressRegion", "")
            job_address_detail = job_detail.get("addressDetail", "")
            appear_date_str = header.get("appearDate")
            salary_text_raw = job_detail.get("salary", "")
            experience_required_text = condition.get("workExp")
            education_required_text = condition.get("edu")

        else:  # 來自列表頁 API
            job_id = job_item.get("jobNo")
            url = job_item.get("link", {}).get("job", "")
            cust_no = job_item.get("custNo")
            cust_name = job_item.get("custName")
            cust_url = job_item.get("link", {}).get("cust", "")
            title = job_item.get("jobName")
            description = job_item.get("description")
            raw_job_type = job_item.get("jobType")
            job_addr_region = job_item.get("jobAddrNoDesc", "")
            job_address_detail = job_item.get("jobAddress", "")
            appear_date_str = job_item.get("appearDate")

            # 列表頁 API 的薪資處理
            salary_text_raw = ""
            salary_low = job_item.get("salaryLow")
            salary_high = job_item.get("salaryHigh")

            if salary_low is not None and salary_high is not None:
                if salary_low == 0 and salary_high == 0:
                    salary_text_raw = "面議"  # 假設 0,0 表示面議
                elif salary_low == salary_high:
                    salary_text_raw = f"月薪{salary_low}元"
                else:
                    salary_text_raw = f"月薪{salary_low}至{salary_high}元"
            elif salary_low is not None:
                salary_text_raw = f"月薪{salary_low}元以上"

            # 從列表頁 API 獲取教育程度和工作經驗，並進行映射
            raw_education = job_item.get("optionEdu")
            if raw_education and isinstance(raw_education, list):
                if 1 in raw_education: # If "不拘" is an option
                    education_required_text = "不拘"
                else:
                    # Find the lowest education level required
                    min_edu_code = min(raw_education)
                    education_required_text = EDUCATION_MAPPING_104.get(min_edu_code, "不拘")
                    # Check if there are higher education levels also accepted
                    if any(code > min_edu_code for code in raw_education):
                        education_required_text += "以上"
            else:
                education_required_text = "不拘"

            raw_experience = job_item.get("period")
            experience_required_text = EXPERIENCE_MAPPING_104.get(raw_experience, "不拘")

        if not job_id:
            logger.warning("Missing job_id in job_item.", job_item_keys=job_item.keys(), full_job_item=job_item)
            return None

        # 組合地址
        location_text = (job_addr_region + job_address_detail).strip()
        if not location_text:
            location_text = None

        # 解析發布日期
        posted_at = None
        if appear_date_str:
            try:
                # 嘗試 YYYY/MM/DD 格式 (單一職缺 API)
                posted_at = datetime.strptime(appear_date_str, "%Y/%m/%d")
            except ValueError:
                try:
                    # 嘗試 YYYYMMDD 格式 (列表頁 API)
                    posted_at = datetime.strptime(appear_date_str, "%Y%m%d")
                except ValueError:
                    logger.warning(
                        "Could not parse posted_at date format.",
                        appear_date=appear_date_str,
                        job_id=job_id,
                    )

        # 解析薪資
        salary_min, salary_max, salary_type = parse_salary(salary_text_raw)

        # 處理 job_type 轉換
        job_type = JOB_TYPE_MAPPING.get(raw_job_type)
        if job_type is None: # 如果映射後為 None，則設置為 OTHER 或保持 None
            job_type = JobType.OTHER # 假設 JobType 中有 OTHER，如果沒有，請自行定義或保持 None

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_104,
            source_job_id=str(job_id),
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text_raw,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=str(cust_no),
            company_name=cust_name,
            company_url=cust_url,
        )
        return job_pydantic_data

    except (AttributeError, KeyError) as e:
        logger.error(
            "Missing key fields when parsing job_item.",
            error=e,
            job_id=job_item.get("jobNo", "N/A"),
            exc_info=True,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error when parsing job_item.",
            error=e,
            job_id=job_item.get("jobNo", "N/A"),
            exc_info=True,
        )
        return None



================================================
FILE: crawler/project_104/producer_category_104.py
================================================
from .task_category_104 import fetch_url_data_104
import structlog

from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import JOB_CAT_URL_104  # Changed import path

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_url_data_104.s(JOB_CAT_URL_104).apply_async(queue='producer_category_104')
logger.info("send task_category_104 url", url=JOB_CAT_URL_104, queue='producer_category_104')





================================================
FILE: crawler/project_104/producer_jobs_104.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_104.task_jobs_104 import fetch_url_data_104
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_job_urls():
    """
    從資料庫讀取待處理或失敗的職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_104,
            statuses=statuses_to_fetch,  # 傳入狀態列表
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_104.s(url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_104")

        logger.info(
            "已成功分發一批職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_104"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))




================================================
FILE: crawler/project_104/producer_urls_104.py
================================================
from crawler.database.repository import get_all_categories_for_platform, get_all_crawled_category_ids_pandas, get_stale_crawled_category_ids_pandas
from crawler.project_104.task_urls_104 import crawl_and_store_category_urls
from crawler.database.models import SourcePlatform, CategorySourcePydantic
import structlog
from typing import Optional, Set, List


from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

def dispatch_urls_for_all_categories(sort_key: Optional[str] = None, limit: int = 0, url_limit: int = 0, n_days: int = 7) -> None:
    """
    分發所有 104 職務類別的 URL 抓取任務。

    從資料庫中獲取所有 104 平台的類別，並為每個類別分發一個 Celery 任務，
    由 `crawl_and_store_category_urls` 任務負責實際的 URL 抓取。

    :param sort_key: 用於排序類別的鍵 (例如 'source_category_id', 'source_category_name')。
    :param limit: 限制分發的類別數量。0 表示無限制。
    :param url_limit: 限制每個分類任務抓取的 URL 數量。0 表示無限制。
    :param n_days: 判斷類別是否需要重新爬取的時間間隔（天）。
    :return: 無。
    :rtype: None
    """
    logger.info("Starting URL task distribution for all 104 categories.",  sort_key=sort_key, limit=limit, url_limit=url_limit, n_days=n_days)

    # 1. 取出所有類別 A
    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_104)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}

    # 2. 取出 tb_url_categories.source_category_id B
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104)

    # 3. B 超過7天 視為 C
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104, n_days)

    # 4. D = (A - B) | C
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids

    # 5. 分發任務
    # Filter the full pydantic objects for dispatch
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    if categories_to_dispatch:
        logger.info("Found categories to dispatch.", count=len(categories_to_dispatch))
        
        if limit > 0:
            categories_to_dispatch = categories_to_dispatch[:limit]
            logger.info(
                "Applying category limit for dispatch.",
                limit=limit,
                actual_count=len(categories_to_dispatch),
            )

        for category_info in categories_to_dispatch:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id, url_limit=url_limit)
            # 在直接執行模式下，我們直接調用函數
            crawl_and_store_category_urls(category_info.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for URL crawling.")



================================================
FILE: crawler/project_104/single_url_api_data_104.py
================================================
import requests
import sys
import structlog
import json

from requests.exceptions import (
    HTTPError,
    ConnectionError,
    Timeout,
    RequestException,
    JSONDecodeError,
)

from crawler.logging_config import configure_logging
from crawler.project_104.config_104 import (
    HEADERS_104_JOB_API,
    JOB_API_BASE_URL_104,
)  # Changed import path

configure_logging()
logger = structlog.get_logger(__name__)


def fetch_url_data_104(url: str) -> dict:
    """
    從 104 職缺 API 抓取單一 URL 的資料。
    """
    job_id = url.split("/")[-1].split("?")[0]
    url_api = f"{JOB_API_BASE_URL_104}{job_id}"

    logger.info(
        "Fetching data for single URL.", url=url, job_id=job_id, api_url=url_api
    )

    try:
        response = requests.get(url_api, headers=HEADERS_104_JOB_API, timeout=10)
        response.raise_for_status()
        data = response.json()
        logger.info(
            "Successfully fetched data.", job_id=job_id, data_keys=list(data.keys())
        )
        return data
    except (HTTPError, ConnectionError, Timeout, RequestException) as e:
        logger.error(
            "Network or request error occurred.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}
    except JSONDecodeError as e:
        logger.error(
            "Failed to decode JSON response.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}
    except Exception as e:
        logger.error(
            "An unexpected error occurred.",
            url=url,
            api_url=url_api,
            error=e,
            exc_info=True,
        )
        return {}


if __name__ == "__main__":
    if len(sys.argv) != 2:
        logger.info(
            "Usage: python -m crawler.project_104.single_url_api_data_104 <job_url>"
        )
        sys.exit(1)

    job_url = sys.argv[1]
    data = fetch_url_data_104(job_url)
    if data:
        logger.info(
            "Fetched data content (sample).",
            job_url=job_url,
            data_sample=json.dumps(data, indent=2, ensure_ascii=False)[:500],
        )
    else:
        logger.warning("No data fetched for the given URL.", job_url=job_url)



================================================
FILE: crawler/project_104/single_url_api_data_104.txt
================================================
{'data': {'corpImageRight': {'corpImageRight': {'imageUrl': '', 'link': ''}}, 'header': {'corpImageTop': {'imageUrl': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/custintroduce/image2.jpg?v=20241107162243', 'link': ''}, 'jobName': '軟體自動化測試工程師(新竹)', 'appearDate': '2025/07/29', 'custName': '全景軟體股份有限公司', 'custUrl': 'https://www.104.com.tw/company/7hzjbag', 'analysisType': 1, 'analysisUrl': '//www.104.com.tw/jobs/apply/analysis/2ews9', 'isSaved': False, 'isFollowed': False, 'isApplied': False, 'applyDate': '', 'userApplyCount': 0, 'hrBehaviorPR': 0.016831555955287163}, 'contact': {'hrName': '温先生', 'email': '', 'visit': '', 'phone': [], 'other': '', 'reply': ''}, 'environmentPic': {'environmentPic': [{'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303088.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303088.jpg?v=20241107162243', 'description': '25周年運動會'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_964488071933812501.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_964488071933812501.jpg?v=20241107162243', 'description': 'Lobby-1'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_964488071933812502.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_964488071933812502.jpg?v=20241107162243', 'description': 'Lobby-2'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303089.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303089.jpg?v=20241107162243', 'description': '員工交誼廳'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303090.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303090.jpg?v=20241107162243', 'description': '尾牙活動'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_892440736021303091.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_892440736021303091.jpg?v=20241107162243', 'description': '社團活動-羽球社'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_893506157025877465.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_893506157025877465.jpg?v=20241107162243', 'description': '春聯DIY'}, {'thumbnailLink': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/s_893506157025877466.jpg?v=20241107162243', 'link': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/env/l_893506157025877466.jpg?v=20241107162243', 'description': '員工旅遊'}], 'corpImageBottom': {'imageUrl': '', 'link': ''}}, 'condition': {'acceptRole': {'role': [{'code': 2, 'description': '應屆畢業生'}, {'code': 64, 'description': '原住民'}], 'disRole': {'needHandicapCompendium': False, 'disability': []}}, 'workExp': '不拘', 'edu': '大學以上', 'major': ['資訊工程相關', '資訊管理相關'], 'language': [{'code': 1, 'language': '英文', 'ability': {'listening': '中等', 'speaking': '中等', 'reading': '中等', 'writing': '中等'}}], 'localLanguage': [], 'specialty': [], 'skill': [], 'certificate': [], 'driverLicense': [], 'other': '1.熟悉 Windows及Linux系統。\n2.具備自動化測試程式開發相關經驗者佳。(JUnit/Selenium/Sikulix/Jmeter/Postman等)\n3.具備RESTful API測試經驗、基本SQL指令操作者佳。\n4.具備DevOps相關經驗者佳。\n5.規劃產品測試相關流程，評估測試結果。\n6.有密碼學或網路安全相關經驗者佳。\n\n人格特質：\n1.發想靈活，具邏輯分析能力。\n2.具掌握時程的能力，有效控管、細心、負責任。\n3.具解決問題及應變能力，有耐心及抗壓性。\n4.具團隊合作精神，能接受指導及良好的溝通能力。\n5.熱情正面，積極追求新知與接受考驗。'}, 'welfare': {'tag': [], 'welfare': '【努力工作也要充電休息】\n· 新人入職當天，即享有特休假配套福利\n· 全年彈性放假不用補班，別人要上班，我們悠閒吃早午餐\n· 給薪活力假，充電完畢，活力滿滿回崗位\n· 彈性上下班時間，讓你更能兼顧家庭生活\n· 年度休假日數優於勞基法\n\n【優於同業的各項獎勵】\n· 年終獎金、績效獎金、員工分紅\n· 介紹獎金、專利獎金\n· 生日禮金、佳節禮金\n· 完善的調薪制度\n每位員工皆是全景軟體的重要資產，我們鼓勵員工努力的付出，並期許能與全景互相成長。\n\n【員工照顧】\n· 年度健檢及團保，我們在乎每一位同仁的健康\n· 員工婚喪喜慶補助、生育津貼等，你的家人就是我們的家人\n\n【在全景的生活多采多姿】\n· 小當家社、燃脂社、壘球社、桌遊社等多元社團活動\n· 定期下午茶、零食點心，在上班時刻撫慰你的身心靈\n· 電影包場欣賞，不用人擠人排隊買票\n· 國內外旅遊補助，想去哪就去哪\n· 聖誕節派對、年度尾牙等活動，讓我們一起共度重要節慶\n\n【各項技能的訓練及提升】\n· 新進同仁訓：職前訓練、制訂個別培訓計畫\n· 在職同仁訓：e-learning線上學習平台、專業技術分享、多樣化課程\n· 各階主管訓：激發領導統御力，提升跨部門合作效率\n', 'legalTag': []}, 'jobDetail': {'jobDescription': '1.建立測試環境，各項產品運作架構熟悉。\n2.負責系統自動化測試系統及相關系統操作(Jenkins, Git, VM, Docker)。\n3.自動化程式撰寫Selenium, Python, Postman, Script等。\n4.產品相關測試(壓力、負載、效能)，測試規範建立、改善流程。', 'jobCategory': [{'code': '2007001004', 'description': '軟體工程師'}, {'code': '2007001006', 'description': 'Internet程式設計師'}], 'salary': '月薪35,000~50,000元', 'salaryMin': 35000, 'salaryMax': 50000, 'salaryType': 50, 'jobType': 1, 'workType': [], 'addressNo': '6001006001', 'addressRegion': '新竹市', 'addressArea': '新竹市', 'addressDetail': '新竹科學園區園區二路48號2樓', 'industryArea': '新竹科學園區', 'longitude': '121.0067597', 'latitude': '24.7739304', 'manageResp': '不需負擔管理責任', 'businessTrip': '無需出差外派', 'workPeriod': '日班，09:00~18:00', 'vacationPolicy': '週休二日', 'startWorkingDay': '不限', 'hireType': 0, 'delegatedRecruit': '', 'needEmp': '不限', 'landmark': '', 'remoteWork': None}, 'switch': 'on', 'custLogo': 'https://static.104.com.tw/b_profile/cust_picture/9000/16325089000/logo.png?v=20241107162243', 'postalCode': '300', 'closeDate': '2021-04-08', 'industry': '電腦軟體服務業', 'custNo': '16325089000', 'reportUrl': 'https://www.104.com.tw/feedback?category=2&custName=%E5%85%A8%E6%99%AF%E8%BB%9F%E9%AB%94%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&jobName=%E8%BB%9F%E9%AB%94%E8%87%AA%E5%8B%95%E5%8C%96%E6%B8%AC%E8%A9%A6%E5%B7%A5%E7%A8%8B%E5%B8%AB%28%E6%96%B0%E7%AB%B9%29', 'industryNo': '1001001002', 'employees': '170人', 'chinaCorp': False, 'interactionRecord': {'lastProcessedResumeAtTime': None, 'lastCustReplyTimestamp': 1751353490, 'nowTimestamp': 1753789855}}, 'metadata': {'enableHTML': False, 'hiddenBanner': False, 'seo': {'noindex': False}}}


================================================
FILE: crawler/project_104/task_category_104.py
================================================
# import os
# #  python -m crawler.project_104.task_category_104
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog

from crawler.database import connection as db_connection
from crawler.database import repository
from crawler.database.connection import initialize_database
from crawler.database.schemas import SourcePlatform
from crawler.project_104.client_104 import fetch_category_data_from_104_api
from crawler.project_104.config_104 import HEADERS_104, JOB_CAT_URL_104
from crawler.worker import app

# Import MAPPING from apply_classification.py
from crawler.database.category_classification_data.apply_classification import MAPPING

logger = structlog.get_logger(__name__)


def flatten_jobcat_recursive(node_list, parent_no=None):
    """
    Recursively flattens the category tree using a generator.
    Applies major category mapping for top-level categories.
    """
    for node in node_list:
        current_parent_id = parent_no
        if parent_no is None: # Only apply mapping for top-level categories
            category_name = node.get("des")
            mapped_parent_id = MAPPING[SourcePlatform.PLATFORM_104].get(category_name)
            if mapped_parent_id:
                current_parent_id = mapped_parent_id

        yield {
            "parent_source_id": current_parent_id,
            "source_category_id": node.get("no"),
            "source_category_name": node.get("des"),
            "source_platform": SourcePlatform.PLATFORM_104.value,
        }
        if "n" in node and node.get("n"):
            yield from flatten_jobcat_recursive(
                node_list=node["n"],
                parent_no=node.get("no"),
            )


@app.task()
def fetch_url_data_104(url_JobCat):
    
    logger.info("Current database connection", db_url=str(db_connection.get_engine().url))
    logger.info("Starting category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_104)

        jobcat_data = fetch_category_data_from_104_api(url_JobCat, HEADERS_104)
        if jobcat_data is None:
            logger.error("Failed to fetch category data from 104 API.", url=url_JobCat)
            return

        flattened_data = list(flatten_jobcat_recursive(jobcat_data))

        if not existing_categories:
            logger.info("Database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            repository.sync_source_categories(SourcePlatform.PLATFORM_104, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_104.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            categories_to_sync.sort(key=lambda x: x['source_category_id'])
            logger.info(
                "Found new or updated categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_104, categories_to_sync)
        else:
            logger.info("No new or updated categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    initialize_database()
    logger.info("Dispatching fetch_url_data_104 task for local testing.", url=JOB_CAT_URL_104)
    fetch_url_data_104(JOB_CAT_URL_104)


================================================
FILE: crawler/project_104/task_jobs_104.py
================================================
# import os
# # python -m crawler.project_104.task_jobs_104  
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from typing import Optional
from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_104.client_104 import fetch_job_data_from_104_api
from crawler.project_104.parser_apidata_104 import parse_job_item_to_pydantic
from crawler.database.connection import initialize_database

logger = structlog.get_logger(__name__)


@app.task()
def fetch_url_data_104(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed information for a single job vacancy from a given URL,
    parses it, stores it in the database, and marks the URL's crawl status.
    """
    job_id = None
    try:
        job_id = url.split("/")[-1].split("?")[0]
        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        data = fetch_job_data_from_104_api(job_id)
        if data is None:
            logger.error("Failed to fetch job data from 104 API.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

    except Exception as e:
        logger.error(
            "Unexpected error during API call or job ID extraction.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_api_data = data.get("data")
    if not job_api_data or job_api_data.get("switch") == "off":
        logger.warning("Job content does not exist or is closed.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_pydantic_data = parse_job_item_to_pydantic(job_api_data)

    if not job_pydantic_data:
        logger.error("Failed to parse job data.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    try:
        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when upserting job data.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    initialize_database()

    PRODUCER_BATCH_SIZE = 20000000 # Changed from 10 to 20
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]
    
    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_104,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info("Found URLs to process.", count=len(urls_to_process))
        for url in urls_to_process:
            logger.info("Processing URL.", url=url)
            fetch_url_data_104(url)
    else:
        logger.info("No URLs found to process for testing.")


================================================
FILE: crawler/project_104/task_urls_104.py
================================================
# import os
# # python -m crawler.project_104.task_urls_104
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
from collections import deque
from typing import Set, List
from crawler.worker import app
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.connection import initialize_database
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    upsert_jobs,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.project_104.client_104 import fetch_job_urls_from_104_api
from crawler.project_104.parser_apidata_104 import parse_job_item_to_pydantic
from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_104.config_104 import (
    URL_CRAWLER_BASE_URL_104,
    URL_CRAWLER_PAGE_SIZE_104,
    URL_CRAWLER_ORDER_BY_104,
    HEADERS_104_URL_CRAWLER,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_category_urls(job_category: dict, url_limit: int = 0) -> None:
    """
    Celery task: Iterates through all pages of a specified job category, fetches job URLs
    and preliminary data, and stores them in the database in batches.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    
    global_job_url_set = set()
    current_batch_jobs = []
    current_batch_urls = []
    current_batch_url_categories = []
    recent_counts = deque(maxlen=4)

    current_page = 1
    logger.info(
        "Task started: crawling job category URLs and data.",
        job_category_code=job_category_code,
        url_limit=url_limit,
    )

    while True:
        if url_limit > 0 and len(global_job_url_set) >= url_limit:
            logger.info(
                "URL limit reached. Ending task early.",
                job_category_code=job_category_code,
                url_limit=url_limit,
                collected_urls=len(global_job_url_set),
            )
            break

        if current_page % 5 == 1:
            logger.info(
                "Current page being processed.",
                page=current_page,
                job_category_code=job_category_code,
            )

        params = {
            "jobsource": "index_s",
            "page": current_page,
            "pagesize": URL_CRAWLER_PAGE_SIZE_104,
            "order": URL_CRAWLER_ORDER_BY_104,
            "jobcat": job_category_code,
            "mode": "s",
            "searchJobs": "1",
        }

        api_response = fetch_job_urls_from_104_api(
            URL_CRAWLER_BASE_URL_104,
            HEADERS_104_URL_CRAWLER,
            params,
            URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
            verify=False,
        )

        if api_response is None:
            logger.error(
                "Failed to retrieve data from 104 API.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        job_items = api_response.get("data", [])
        if not isinstance(job_items, list):
            logger.error(
                "API response 'data.list' format is incorrect or missing.",
                page=current_page,
                job_category_code=job_category_code,
                api_data_type=type(job_items),
            )
            break

        if not job_items:
            logger.info(
                "No more job items found. Ending task.",
                page=current_page,
                job_category_code=job_category_code,
            )
            break

        for job_item in job_items:
            job_pydantic = parse_job_item_to_pydantic(job_item)
            if job_pydantic and job_pydantic.url:
                if job_pydantic.url not in global_job_url_set:
                    global_job_url_set.add(job_pydantic.url)
                    current_batch_jobs.append(job_pydantic)
                    current_batch_urls.append(job_pydantic.url)
                
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=job_pydantic.url,
                        source_category_id=job_category_code,
                    ).model_dump()
                )

        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "Batch upload size reached. Starting data upload.",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_jobs(current_batch_jobs)
            upsert_urls(SourcePlatform.PLATFORM_104, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            
            current_batch_jobs.clear()
            current_batch_urls.clear()
            current_batch_url_categories.clear()

        total_jobs = len(global_job_url_set)
        recent_counts.append(total_jobs)
        if len(recent_counts) == recent_counts.maxlen and len(set(recent_counts)) == 3:
            logger.info(
                "No new data found consecutively. Ending task early.",
                job_category_code=job_category_code,
            )
            break

        current_page += 1

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining data to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_jobs(current_batch_jobs)
        upsert_urls(SourcePlatform.PLATFORM_104, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No new data collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code)


if __name__ == "__main__":
    initialize_database()

    n_days = 7  # Define n_days for local testing
    url_limit = 1000000

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_104)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_104, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    # Only process the first category for local testing
    if categories_to_dispatch:
        # categories_to_process_single = [categories_to_dispatch[0]]
        
        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")


================================================
FILE: crawler/project_1111/1111_人力銀行_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import urllib.parse
import urllib3

# 忽略不安全請求的警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

WEB_NAME = "1111_人力銀行"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.1111.com.tw",
}

print(f"開始執行 {WEB_NAME} ")
# Output:
#   開始執行 1111_人力銀行 


# ## 取得網站所有職業總覽
# # 1. 取得 JSON 資料
# # jobcat 檔案名稱

# file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
# url_JobCat = "https://www.1111.com.tw/api/v1/codeCategories/"

# response_jobcat = requests.get(url_JobCat, headers=HEADERS, verify=False, timeout=10)
# response_jobcat.raise_for_status()
# jobcat_data = response_jobcat.json()
# with open(file_jobcat_json, "w", encoding="utf-8") as f:
#     json.dump(jobcat_data, f, ensure_ascii=False, indent=4)

# print("JSON資料，分類總覽", pd.json_normalize(jobcat_data).columns)
# # 'nation', 'city', 'region', 'mrt', 'jobPosition', 'industry',
# # 'certification', 'major', 'workAbility', 'computerSkill',
# # 'companyBenefit', 'jobBenefit',
# # pd.json_normalize(jobcat_data)
# # pd.json_normalize(jobcat_data["region"][0]["categories"])
# # pd.json_normalize(jobcat_data["certification"])
# # pd.json_normalize(jobcat_data["major"])
# # pd.json_normalize(jobcat_data["workAbility"])
# # pd.json_normalize(jobcat_data["computerSkill"])
# print(f"職業總覽資料已儲存為 {file_jobcat_json}")

# df_jobcat = pd.json_normalize(jobcat_data["industry"])
# df_jobcat.to_excel(f"{WEB_NAME}_category.xlsx", index=False)
# print(f"職業總覽資料已轉換為 '{WEB_NAME}_category.xlsx'")

# mask = df_jobcat["parentCode"].astype(str).str.startswith("100")
# df_it_jobs = df_jobcat[mask]
# df_it_jobs.head(5)



def catch_1111_url(KEYWORDS, CATEGORY, ORDER="date", PAGE_NUM=1, USE_API=False):
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 1111 求職網的完整職缺網址或 API 網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。若無則傳入空字串 ""。
    CATEGORY (str or list): 職缺的類別代碼，例如 "140100" 或者類別代碼的列表。
                            若無則傳入空字串 ""。
    ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。
                           預設為 "date"。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。
    USE_API (bool, optional): 是否使用 API 網址。預設為 False。

    返回:
    str: 生成的 1111 求職網址或 API 網址。
    """
    BASE_URL = "https://www.1111.com.tw/search/job"
    API_URL = "https://www.1111.com.tw/api/v1/search/jobs/"

    # 確保頁碼至少為 1，避免負數或 0 造成計算錯誤
    safe_page_num = max(1, PAGE_NUM)

    if USE_API:
        params = {
            "page": safe_page_num,
            "fromOffset": 0,
            "sortBy": "ab" if ORDER == "relevance" else "da",
            "sortOrder": "desc",
            "conditionsText": KEYWORDS,
            "searchUrl": f"/search/job?page={safe_page_num}&col={'ab' if ORDER == 'relevance' else 'da'}&sort=desc&ks={urllib.parse.quote(KEYWORDS)}&d0={','.join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY}",
            "keyword": KEYWORDS,
        }

        # 如果有提供職務類別，加入到參數中
        if CATEGORY:
            if isinstance(CATEGORY, list):
                for code in CATEGORY:
                    params["jobPositions"] = code

        query_string = urllib.parse.urlencode(params)
        return f"{API_URL}?{query_string}"
    else:
        params = {
            "page": safe_page_num,
            "col": "ab" if ORDER == "relevance" else "da",
            "sort": "desc",
            "ks": KEYWORDS,
            "d0": ",".join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY,
        }

        query_string = urllib.parse.urlencode(params)
        return f"{BASE_URL}?{query_string}"


# # --- 測試範例 ---

# KEYWORDS_STR = "雲端工程師"
# CATEGORY_CODE1 = "140100"  # 單一類別
# CATEGORY_CODE2 = ["140100", "140200"]  # 多個類別

# # 1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁
# print("1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁:")
# url_1 = catch_1111_url(
#     KEYWORDS_STR, CATEGORY_CODE1, ORDER="relevance"
# )  # PAGE_NUM 省略，預設為 1
# print(url_1, "\n")

# # 2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁
# print("2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁:")
# url_2 = catch_1111_url("", CATEGORY_CODE2, ORDER="date", PAGE_NUM=2, USE_API=False)
# print(url_2, "\n")

# # 3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁
# print("3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁:")
# url_3 = catch_1111_url(KEYWORDS_STR, "", ORDER="date", PAGE_NUM=3, USE_API=False)
# print(url_3, "\n")

# # 4. 使用 API 生成網址
# print("4. 有關鍵字, 單一類別, 使用 API, 第 1 頁:")
# api_url_1 = catch_1111_url(
#     KEYWORDS_STR, CATEGORY_CODE1, ORDER="relevance", USE_API=True
# )
# print(api_url_1, "\n")

# # 5. 無關鍵字, 多個類別, 使用 API, 第 2 頁
# print("5. 無關鍵字, 多個類別, 使用 API, 第 2 頁:")
# api_url_2 = catch_1111_url("", CATEGORY_CODE2, ORDER="date", PAGE_NUM=2, USE_API=True)
# print(api_url_2, "\n")

# # 6. 有關鍵字, 無類別, 使用 API, 第 3 頁
# print("6. 有關鍵字, 無類別, 使用 API, 第 3 頁:")
# api_url_3 = catch_1111_url(KEYWORDS_STR, "", ORDER="date", PAGE_NUM=3, USE_API=True)
# print(api_url_3, "\n")

from tqdm.auto import tqdm
import urllib3
from typing import Optional, List, Union


def get_1111_api_data(
    KEYWORDS: str,
    CATEGORY: Union[str, List[str]],
    ORDER: str = "date",
    max_page: Optional[int] = None,
) -> pd.DataFrame:
    """
    從 1111 求職網高效擷取指定搜尋條件下的所有職缺資料。
    1.  將首次請求與總頁數獲取合併，減少 API 呼叫。
    2.  提供更穩健的錯誤處理機制，能跳過請求失敗的頁面。
    3.  程式碼結構簡潔，使用進度條顯示抓取進度。

    Args:
        KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。
        CATEGORY (Union[str, List[str]]): 職缺的類別代碼，例如 "140100" 或 ["140100", "140200"]。
        ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。預設為 "date"。
        max_page (Optional[int], optional): 欲抓取的最大頁數。若為 None，則自動抓取所有頁面。預設為 None。

    Returns:
        pd.DataFrame: 包含所有職缺資料的 DataFrame。如果抓取失敗或無資料，則返回空的 DataFrame。
    """

    all_data = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    # 請求第一頁，同時獲取總頁數
    try:
        url = catch_1111_url(KEYWORDS, CATEGORY, ORDER, PAGE_NUM=1, USE_API=True)
        response = requests.get(url, headers=headers, verify=False, timeout=15)
        response.raise_for_status()
        data = response.json()

        hits = data.get("result", {}).get("hits", [])
        if not hits:
            print("找不到任何相關職缺資料。")
            return pd.DataFrame()

        all_data.extend(hits)
        totalPage = data.get("result", {}).get("pagination", {}).get("totalPage", 1)

    except (
        requests.exceptions.RequestException,
        requests.exceptions.JSONDecodeError,
    ) as e:
        print(f"抓取第一頁時發生錯誤，無法繼續: {e}")
        return pd.DataFrame()

    # 決定最終要抓取的頁數
    limit_page = totalPage
    if max_page is not None and max_page > 0:
        limit_page = min(totalPage, max_page)

    if limit_page <= 1:
        return pd.json_normalize(all_data)

    # 迭代抓取剩餘頁面 (從第 2 頁開始)
    for page_num in tqdm(range(2, limit_page + 1), desc="正在擷取資料"):
        try:
            url = catch_1111_url(
                KEYWORDS, CATEGORY, ORDER, PAGE_NUM=page_num, USE_API=True
            )
            response = requests.get(url, headers=headers, verify=False, timeout=15)
            response.raise_for_status()

            page_data = response.json().get("result", {}).get("hits", [])
            if not page_data:
                print(f"\n在第 {page_num} 頁後已無更多資料，提前結束。")
                break

            all_data.extend(page_data)

        except (
            requests.exceptions.RequestException,
            requests.exceptions.JSONDecodeError,
        ) as e:
            print(f"\n抓取第 {page_num} 頁時發生錯誤，已跳過: {e}")
            continue

    if not all_data:
        return pd.DataFrame()

    return pd.json_normalize(all_data)


# --- 使用範例 ---
KEYWORDS = "雲端工程師"
CATEGORY = ["140100", "140200"]

df_data = get_1111_api_data(KEYWORDS, CATEGORY, max_page=2)
# df_data = get_1111_data(KEYWORDS, CATEGORY)
df_data.head(1)
# Output:
#   正在擷取資料:   0%|          | 0/1 [00:00<?, ?it/s]
#                 updateAt      jobId  companyId           companyName  \

#   0  2025/06/17 11:24:00  113019436   48931667  富邦媒體科技股份有限公司(富邦momo)   

#   

#                                            description                 title  \

#   0  1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資...  D4000 JAVA資料工程師 【台中】   

#   

#     role remind  replyInDays            salary  ...  require.certificates  \

#   0  [1]    [0]            0  面議（經常性薪資達4萬元或以上）  ...                    []   

#   

#      require.experience  require.grades       require.majors industry.id  \

#   0                   3    [16, 32, 64]  [130100, 130600, 0]      250214   

#   

#     industry.name  workCity.id workCity.name                highlight.title  \

#   0          百貨相關       100906        台中市北屯區  D4000 JAVA資料<em>工程師</em> 【台中】   

#   

#                                  highlight.description  

#   0  1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資...  

#   

#   [1 rows x 34 columns]

# 從指定的職缺網址獲取職缺的相關數據

def get_1111_page_data(job_url):
    """
    解析 1111 新版職缺頁面的 HTML 內容，並回傳結構化資料。

    參數:
    job_url (str): 職缺頁面的 URL。

    回傳:
    dict: 包含職缺資訊的結構化資料，包括職務名稱、公司名稱、工作性質、薪資待遇、學歷要求、上班地點、福利資訊及聯絡資訊等。
    """

    response = requests.get(job_url, verify=False)
    soup = BeautifulSoup(response.content, "html.parser")

    data = {}

    data["job_url"] = job_url

    # --- 1. 頁首區塊 (Top Section) ---
    header_section = soup.select_one(
        "section[data-v-e57f1019] > div.container > div.text-gray-600"
    )
    if header_section:
        data["職務名稱"] = header_section.select_one("h1").get_text(strip=True) or "N/A"
        data["公司名稱"] = (
            header_section.select_one("h2.inline").get_text(strip=True) or "N/A"
        )

        pills = header_section.select("div.flex.flex-wrap.mt-4.gap-3 > div")
        top_info = [p.get_text(strip=True, separator=" ") for p in pills]
        if len(top_info) >= 4:
            data["工作性質"], data["薪資待遇"], data["學歷要求"], data["上班地點"] = (
                top_info[:4]
            )

        info_items = header_section.select("ul.info-item > li")
        for item in info_items:
            key_tag = item.select_one("h3")
            val_tag = item.select_one("span") or item.select_one("time")
            if key_tag and val_tag:
                data[key_tag.get_text(strip=True)] = val_tag.get_text(strip=True)

    # --- 2. 主要內容區塊 (Main Content Sections) ---
    sections = soup.select("section[id]")
    for section in sections:
        section_title_tag = section.select_one("h2.text-lg.text-main")
        if not section_title_tag:
            continue

        section_title = section_title_tag.get_text(strip=True)
        section_data = {}

        # --- 標準內容區塊的通用解析 ---
        for div in section.select("div.content"):
            key_tag = div.find("h3")
            if key_tag:
                key = key_tag.get_text(strip=True)
                value_container = key_tag.find_next_sibling()
                if value_container:
                    list_items = value_container.select("li")
                    section_data[key] = (
                        [
                            li.get_text(strip=True, separator=" ")
                            .replace("、", "")
                            .strip()
                            for li in list_items
                        ]
                        if list_items
                        else value_container.get_text(separator="\n", strip=True)
                    )

        # --- 福利資訊的特殊結構處理 ---
        if section_title == "福利資訊":
            legal_welfare_h3 = section.find(
                "h3", string=lambda t: t and "法定福利" in t
            )
            if legal_welfare_h3:
                container = legal_welfare_h3.find_next_sibling("div")
                section_data["法定福利"] = [
                    p.get_text(strip=True) for p in container.select("p")
                ]

            company_welfare_h3 = section.find(
                "h3", string=lambda t: t and "公司福利" in t
            )
            if company_welfare_h3:
                welfare_map = {}
                categories = (
                    company_welfare_h3.find_parent("div")
                    .find_next_sibling("div")
                    .find_all("div", recursive=False, class_="flex")
                )
                for cat_div in categories:
                    cat_title = (
                        cat_div.select_one("h4").get_text(strip=True)
                        if cat_div.select_one("h4")
                        else "未分類"
                    )
                    items = [
                        p.get_text(strip=True)
                        for p in cat_div.select("div.flex-wrap p")
                    ]
                    welfare_map[cat_title] = items
                section_data["公司福利"] = welfare_map

            more_info_h3 = section.find("h3", string=lambda t: t and "更多說明" in t)
            if more_info_h3:
                container = more_info_h3.find_next_sibling("div")
                section_data["更多說明"] = (
                    container.get_text(separator="", strip=True)
                    .split("\n展開全部")[0]
                    .strip()
                )

        if section_data:
            data[section_title] = section_data

    # --- 單獨處理聯絡資訊 ---
    contact_section = soup.select_one("section#CONTACT_INFO")
    if contact_section:
        contact_data = {}
        contact_person_tag = contact_section.find(
            "h3", string=lambda t: t and "聯絡人員" in t
        )
        if contact_person_tag:
            contact_data["聯絡人員"] = contact_person_tag.find_next_sibling(
                "p"
            ).get_text(strip=True)
        if contact_data:
            data["聯絡資訊"] = contact_data
    # df = pd.json_normalize(data)
    return data


# 測試範例
job_url = "https://www.1111.com.tw/job/113019436"
get_1111_page_data(job_url)

# Output:
#   {'job_url': 'https://www.1111.com.tw/job/113019436',

#    '職務名稱': 'D4000 JAVA資料工程師 【台中】',

#    '公司名稱': '富邦媒體科技股份有限公司(富邦momo)',

#    '需求人數': '1  ~ 2 人',

#    '到職日期': '一個月內',

#    '更新日期': '2025 / 06 / 17',

#    '工作內容': {'職缺描述': '1.主要開發與維護 會員標籤/行銷/推薦應用相關平台\n2.開發數據資料處理平台，解決大量資料處理時所衍生之效能與維護問題\n3.雲端平台開發及維運(GCP/AWS)\n4.Data Pipeline規劃、設計、開發與監控維護\n5.研發大數據架構相關技術，優化現有架構並導入新技術\n收合內容',

#     '工作待遇': '面議 (經常性薪資達4萬元或以上)\n查看薪資水平',

#     '職務類別': ['演算法開發工程師', '軟體工程師', '網站程式設計師'],

#     '工作性質': ['全職'],

#     '工作時間': ['日班'],

#     '工作地點': '台中市\n                    北屯區\n                    文心路四段955號9樓'},

#    '要求條件': {'學歷要求': ['大學以上'],

#     '科系要求': ['數學統計學門', '電算機學門'],

#     '工作經驗': '1 年以上經驗',

#     '外語能力': ['英文 聽｜ 中等 説｜ 中等 讀｜ 中等 寫｜ 中等'],

#     '工作技能': '不拘',

#     '附加條件': '1.一年以上Java的軟體開發經驗\n2.具有 NoSQL (Redis、elasticsearch)的實務經驗\n3.具有SQL語法且具備Oracle、Postgre實務經驗\n4.具有Message Queue (例 Kafka、RabbitMQ)的實務經驗\n5.具有Git、Jenkis、Airflow 等Data Pipeline 開發實務經驗\n6.熟悉Linux作業系統操作\n\n* 如經面試錄取後，報到時須繳交「體格檢查表(一般)」，請於報到前自行至醫療機構辦理體檢 *\n展開全部',

#     '歡迎身份': ['原住民']},

#    '公司資訊': {'公司名稱': '富邦媒體科技股份有限公司(富邦momo)', '產業類別': ['百貨相關'], '公司人數': '3,100 人'},

#    '聯絡資訊': {'聯絡人員': '張小姐'}}

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime("%Y-%m-%d", time.localtime(time.time()))
JOBCAT_CODE = "140100"
KEYWORDS = "雲端工程師"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{JOBCAT_CODE}"
MAX_WORKERS = 5  # 同時運行的執行緒數量

print(f"開始執行 {FILE_NAME}")
df_api_data = get_1111_api_data(KEYWORDS, JOBCAT_CODE)
df_api_data["company_url"] = "https://www.1111.com.tw/corp/" + df_api_data["companyId"].astype(str)
df_api_data["job_url"] = "https://www.1111.com.tw/job/" + df_api_data["jobId"].astype(str)


# 使用 ThreadPoolExecutor 進行並行處理
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:  # 可以根據需要調整 max_workers 的數量
    list_page_data = list(tqdm(executor.map(get_1111_page_data, df_api_data['job_url']), total=len(df_api_data['job_url']), desc="Parsing Job URLs"))

df_page_data = pd.json_normalize(list_page_data)
all_jobs_df = pd.merge(df_api_data, df_page_data, on='job_url', how='left')

print(all_jobs_df.shape)

all_jobs_df.head(1)
# Output:
#   開始執行 (2025-06-17)_1111_人力銀行_雲端工程師_140100

#   正在擷取資料:   0%|          | 0/14 [00:00<?, ?it/s]
#   Parsing Job URLs:   0%|          | 0/437 [00:00<?, ?it/s]
#   (437, 77)

#                 updateAt     jobId  companyId companyName  \

#   0  2025/06/17 12:46:00  99025652       2169  經濟實業股份有限公司   

#   

#                                            description         title role  \

#   0  1.泵浦發電機大數據輔助設計與運維之智慧雲端平台開發相關工作\n2.泵浦、發電機運轉中數據收...  軟體分析工程師(總公司)  [1]   

#   

#     remind  replyInDays              salary  ...  工作內容.實習時段  要求條件.打字速度  \

#   0    [0]            0  月薪 45,000元~55,000元  ...        NaN        NaN   

#   

#      工作內容.發展遠景  工作內容.成就樂趣 公司資訊.品牌名稱 工作內容.遠距工作  工作性質 薪資待遇  學歷要求  上班地點  

#   0        NaN        NaN       NaN       NaN   NaN  NaN   NaN   NaN  

#   

#   [1 rows x 77 columns]

# all_jobs_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
# Output:
#   已將所有職缺資料儲存到 (2025-06-17)_1111_人力銀行_雲端工程師_140100.xlsx


all_jobs_df.columns
# Output:
#   Index(['updateAt', 'jobId', 'companyId', 'companyName', 'description', 'title',

#          'role', 'remind', 'replyInDays', 'salary', 'recruitCount', 'mrtId',

#          'mrtTime', 'mrtNear', 'benefits', 'companyTags', 'isHappiness',

#          'internship', 'jobType', 'bookmarked', 'hasMedias', 'isTop',

#          'hasCompanyLogo', 'require.drivingLicense', 'require.certificates',

#          'require.experience', 'require.grades', 'require.majors', 'industry.id',

#          'industry.name', 'workCity.id', 'workCity.name', 'highlight.title',

#          'highlight.description', 'company_url', 'job_url', '職務名稱', '公司名稱',

#          '應徵人數', '需求人數', '到職日期', '更新日期', '工作內容.職缺描述', '工作內容.工作待遇', '工作內容.職務類別',

#          '工作內容.工作性質', '工作內容.工作時間', '工作內容.休假制度', '工作內容.工作地點', '要求條件.學歷要求',

#          '要求條件.科系要求', '要求條件.工作經驗', '要求條件.外語能力', '要求條件.方言能力', '要求條件.工作技能',

#          '要求條件.歡迎身份', '公司資訊.公司名稱', '公司資訊.產業類別', '公司資訊.公司人數', '公司資訊.資本額',

#          '聯絡資訊.聯絡人員', '要求條件.附加條件', '要求條件.電腦專長', '要求條件.具備駕照', '要求條件.自備車輛',

#          '要求條件.專業證照', '要求條件.其他說明', '工作內容.實習時段', '要求條件.打字速度', '工作內容.發展遠景',

#          '工作內容.成就樂趣', '公司資訊.品牌名稱', '工作內容.遠距工作', '工作性質', '薪資待遇', '學歷要求', '上班地點'],

#         dtype='object')



================================================
FILE: crawler/project_1111/client_1111.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional, Union, List

import requests
import structlog
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import urllib.parse

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_1111.config_1111 import (
    HEADERS_1111_JOB_API,
    JOB_API_BASE_URL_1111,
    JOB_CAT_URL_1111,
    HEADERS_1111,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_api_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = False,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[Dict[str, Any]]:
    """
    通用的 API 請求函式，處理隨機延遲、請求發送、JSON 解析和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before API request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        data = response.json()
        return data
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except json.JSONDecodeError:
        logger.error(
            "Failed to parse JSON response from API.",
            url=url,
            exc_info=True,
            **log_context,
        )
        return None
    except Exception as e:
        logger.error(
            "Unexpected error during API request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None

def fetch_category_data_from_1111_api(
    api_url: str = JOB_CAT_URL_1111, headers: Dict[str, str] = HEADERS_1111
) -> Optional[Dict[str, Any]]:
    """
    從 1111 API 獲取職務分類的原始數據。
    """
    return _make_api_request(
        "GET",
        api_url,
        headers=headers,
        log_context={"api_type": "1111_category_data"},
    )

def catch_1111_url(KEYWORDS: str, CATEGORY: Union[str, List[str]], ORDER: str = "date", PAGE_NUM: int = 1, USE_API: bool = True):
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 1111 求職網的完整職缺網址或 API 網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。若無則傳入空字串 ""。
    CATEGORY (str or list): 職缺的類別代碼，例如 "140100" 或者類別代碼的列表。
                            若無則傳入空字串 ""。
    ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。
                           預設為 "date"。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。
    USE_API (bool, optional): 是否使用 API 網址。預設為 False。

    返回:
    str: 生成的 1111 求職網址或 API 網址。
    """
    BASE_URL = "https://www.1111.com.tw/search/job"
    API_URL = JOB_API_BASE_URL_1111

    # 確保頁碼至少為 1，避免負數或 0 造成計算錯誤
    safe_page_num = max(1, PAGE_NUM)

    if USE_API:
        params = {
            "page": safe_page_num,
            "fromOffset": 0,
            "sortBy": "ab" if ORDER == "relevance" else "da",
            "sortOrder": "desc",
        }

        # 如果有提供職務類別，加入到參數中
        if CATEGORY:
            if isinstance(CATEGORY, list):
                params["d0"] = ",".join(CATEGORY)
            else:
                params["d0"] = CATEGORY
        
        if KEYWORDS and KEYWORDS != "":
            params["keyword"] = KEYWORDS

        query_string = urllib.parse.urlencode(params)
        return f"{API_URL}?{query_string}"
    else:
        params = {
            "page": safe_page_num,
            "col": "ab" if ORDER == "relevance" else "da",
            "sort": "desc",
            "ks": KEYWORDS,
            "d0": ",".join(CATEGORY) if isinstance(CATEGORY, list) else CATEGORY,
        }

        query_string = urllib.parse.urlencode(params)
        return f"{BASE_URL}?{query_string}"

def fetch_job_urls_from_1111_api(
    KEYWORDS: str,
    CATEGORY: Union[str, List[str]],
    ORDER: str = "date",
    PAGE_NUM: int = 1,
) -> Optional[Dict[str, Any]]:
    """
    從 1111 API 獲取職缺 URL 列表的原始數據。
    """
    api_url = catch_1111_url(KEYWORDS, CATEGORY, ORDER, PAGE_NUM, USE_API=True)
    return _make_api_request(
        "GET",
        api_url,
        headers=HEADERS_1111_JOB_API,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "1111_job_urls",
            "keywords": KEYWORDS,
            "category": CATEGORY,
            "page": PAGE_NUM,
        },
    )

def fetch_job_data_from_1111_web(job_url: str) -> Optional[Dict[str, Any]]:
    """
    從 1111 職缺頁面抓取單一 URL 的資料。
    """
    try:
        response = requests.get(job_url, verify=False, timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS)
        response.raise_for_status()
        return {"content": response.text} # Return content for BeautifulSoup parsing
    except requests.exceptions.RequestException as e:
        logger.error(
            "Network error during 1111 job detail request.",
            url=job_url,
            error=e,
            exc_info=True,
        )
        raise # Re-raise to trigger tenacity retry if applied to this function
    except Exception as e:
        logger.error(
            "Unexpected error during 1111 job detail request.",
            url=job_url,
            error=e,
            exc_info=True,
        )
        return None



================================================
FILE: crawler/project_1111/config_1111.py
================================================
# crawler/project_1111/config_1111.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# 1111 平台相關設定
WEB_NAME_1111 = config_section.get("WEB_NAME_1111", "1111_人力銀行")
JOB_CAT_URL_1111 = config_section.get("JOB_CAT_URL_1111", "https://www.1111.com.tw/api/v1/codeCategories/")
JOB_API_BASE_URL_1111 = config_section.get("JOB_API_BASE_URL_1111", "https://www.1111.com.tw/api/v1/search/jobs/")
JOB_DETAIL_BASE_URL_1111 = config_section.get("JOB_DETAIL_BASE_URL_1111", "https://www.1111.com.tw/job/")

HEADERS_1111 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.1111.com.tw",
}

HEADERS_1111_JOB_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.1111.com.tw/search/job",
}

URL_CRAWLER_PAGE_SIZE_1111 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_1111", "20"))
URL_CRAWLER_ORDER_BY_1111 = config_section.get("URL_CRAWLER_ORDER_BY_1111", "date") # "date" or "relevance"



================================================
FILE: crawler/project_1111/page_api_data_1111.txt
================================================
{
  "result": {
    "pagination": {
      "page": 2,
      "limit": 30,
      "totalCount": 41528,
      "totalPage": 1385
    },
    "hits": [
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 98706131,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-台中福雅店】門市正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~32,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100907,
          "name": "台中市西屯區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-台中福雅店】門市正職人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/30 08:58:00",
        "jobId": 85120237,
        "companyId": 69588457,
        "companyName": "全家便利商店(全國門市聯合招募)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔",
        "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 45,000元~50,000元",
        "industry": {
          "id": 250215,
          "name": "量販流通相關"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1010"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
          "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 130198445,
        "companyId": 51471366,
        "companyName": "花間集養生事業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~",
        "title": "銷售員假日班(需輪蘆洲及重新家樂福)",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 205元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100220,
          "name": "新北市三重區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "銷售員假日班(需輪蘆洲及重新家樂福)",
          "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~"
        }
      },
      {
        "updateAt": "2025/07/29 07:23:10",
        "jobId": 113015398,
        "companyId": 117091183,
        "companyName": "金好運投注站",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.台灣彩券相關產品銷售，如大樂透,威力彩、今彩539以及刮刮樂之相關產品。\n2.無需負擔業績\n3.薪資,福利優渥,包刮交通與餐費經貼\n4.具三節獎金\n5.旅遊經貼補助",
        "title": "門市人員(海湖店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 35,000元~40,000元",
        "industry": {
          "id": 210101,
          "name": "運動服務"
        },
        "workCity": {
          "id": 100513,
          "name": "桃園市蘆竹區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1003",
          "1004",
          "1005",
          "1010",
          "1017",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員(海湖店)",
          "description": "1.台灣彩券相關產品銷售，如大樂透,威力彩、今彩539以及刮刮樂之相關產品。\n2.無需負擔業績\n3.薪資,福利優渥,包刮交通與餐費經貼\n4.具三節獎金\n5.旅遊經貼補助"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998332,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "陳列人員(縣聯店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102101,
          "name": "花蓮縣花蓮市"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "陳列人員(縣聯店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998325,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "商品陳列人員(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102104,
          "name": "花蓮縣吉安鄉"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "商品陳列人員(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 113108005,
        "companyId": 69429432,
        "companyName": "寶御食品有限公司(宝泉)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務（如：電話諮詢、調貨、修改、包裝及退換貨處理）。\n3.負責商品進貨入庫、銷售管理及庫存管理。\n4.負責商品包裝、陳列及促銷品換檔工作。\n5.維持店櫃週遭之整潔。\n6.具高度服務熱忱，有學習意願。\n7.騎機車可到，有停車場。",
        "title": "門市人員(湖口服務區)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 29,000元以上",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100702,
          "name": "新竹縣湖口鄉"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市人員(湖口服務區)",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務（如：電話諮詢、調貨、修改、包裝及退換貨處理）。\n3.負責商品進貨入庫、銷售管理及庫存管理。\n4.負責商品包裝、陳列及促銷品換檔工作。\n5.維持店櫃週遭之整潔。\n6.具高度服務熱忱，有學習意願。\n7.騎機車可到，有停車場。"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 103843529,
        "companyId": 51471366,
        "companyName": "花間集養生事業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n應徵方式加入LINE ID:flower22683268",
        "title": "銷售員假日班(林口家樂福)",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 205元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100223,
          "name": "新北市林口區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "銷售員假日班(林口家樂福)",
          "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n應徵方式加入LINE ID:flower22683268"
        }
      },
      {
        "updateAt": "2025/07/29 00:10:00",
        "jobId": 78600194,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作 \n4.進貨庫存、補貨上架、陳列擺設 \n5.執行庫存盤點、電腦行政作業等工作 \n6.維持店鋪環境整潔 \n7.可配合店鋪輪調尤佳\n\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修] \n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-內湖瑞光店】門市正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-內湖瑞光店】門市正職人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作 \n4.進貨庫存、補貨上架、陳列擺設 \n5.執行庫存盤點、電腦行政作業等工作 \n6.維持店鋪環境整潔 \n7.可配合店鋪輪調尤佳\n\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修] \n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/29 10:28:21",
        "jobId": 113134032,
        "companyId": 71959281,
        "companyName": "京旺煙酒有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 收銀、銷貨單、網購商品取貨作業\n2. 庫存作業管理。\n3. 商品展示及陳列。\n4. 消費者需求動態掌握。\n5. 門市常態工作執行。\n6. 配合總公司規則及決策。\n7. 配合總公司促銷活動執行。\n8. 上級交辦事項。\n9. 具升遷或升等機會。",
        "title": "門市人員-東明、大明(儲備幹部)-連鎖煙酒專賣 (無經驗可/獎金另計)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 5,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 250202,
          "name": "食品什貨零售"
        },
        "workCity": {
          "id": 100910,
          "name": "台中市大里區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1005",
          "1010",
          "1017"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市人員-東明、大明(儲備幹部)-連鎖煙酒專賣 (無經驗可/獎金另計)",
          "description": "1. 收銀、銷貨單、網購商品取貨作業\n2. 庫存作業管理。\n3. 商品展示及陳列。\n4. 消費者需求動態掌握。\n5. 門市常態工作執行。\n6. 配合總公司規則及決策。\n7. 配合總公司促銷活動執行。\n8. 上級交辦事項。\n9. 具升遷或升等機會。"
        }
      },
      {
        "updateAt": "2025/07/29 08:34:00",
        "jobId": 131998329,
        "companyId": 117092217,
        "companyName": "旺莊百貨有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。",
        "title": "陳列人員(全民店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,590元~40,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 102101,
          "name": "花蓮縣花蓮市"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "陳列人員(全民店)(薪30590起~40000 月休8-10天  電話:0963-701-222 聯絡時間:13點後)",
          "description": "1.商品補貨上架。\n2.商品進貨點貨。\n3.整理賣場商品陳列。\n4.環境清潔。"
        }
      },
      {
        "updateAt": "2025/07/28 00:10:00",
        "jobId": 132040158,
        "companyId": 117198099,
        "companyName": "四寶貝美食團購有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "160100",
            "100100",
            "100200"
          ]
        },
        "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月",
        "title": "門市人員 (土城店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 250116,
          "name": "綜合商品批發代理"
        },
        "workCity": {
          "id": 100216,
          "name": "新北市土城區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1010",
          "1017",
          "1023",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員 (土城店)",
          "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月"
        }
      },
      {
        "updateAt": "2025/07/28 00:10:00",
        "jobId": 79956370,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-板橋大遠百店】正職門市人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100203,
          "name": "新北市板橋區"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1006"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-板橋大遠百店】正職門市人員",
          "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/28 14:12:33",
        "jobId": 98744798,
        "companyId": 2915040,
        "companyName": "旺來昌實業股份有限公司",
        "require": {
          "drivingLicense": [
            "69"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.商品管理-陳列補貨/商品效期檢視/貨架清潔/維持賣場環境整潔 \n\n2.互相協助、代理同事工作，完成主管交辦事項\n\n3.倉庫區商品整理定位，維持倉庫整齊與清潔 \n\n4.接受顧客詢問或主動提供諮商建議給顧客\n\n5.向顧客說明商品性質、特徵、品質與價格\n\n6.可配合公司輪班(排班制)\n\n7.須能獨立搬貨物25-30公斤\n\n日班:08:30~17:30 休息一小時\n晚班:12:45-21:15 休息半小時",
        "title": "正職營業人員（鳳山店）",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 29,000元以上",
        "industry": {
          "id": 250102,
          "name": "食品什貨批發"
        },
        "workCity": {
          "id": 101824,
          "name": "高雄市鳳山區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1003",
          "1005"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "正職營業人員（鳳山店）",
          "description": "1.商品管理-陳列補貨/商品效期檢視/貨架清潔/維持賣場環境整潔 \n\n2.互相協助、代理同事工作，完成主管交辦事項\n\n3.倉庫區商品整理定位，維持倉庫整齊與清潔 \n\n4.接受顧客詢問或主動提供諮商建議給顧客\n\n5.向顧客說明商品性質、特徵、品質與價格\n\n6.可配合公司輪班(排班制)\n\n7.須能獨立搬貨物25-30公斤\n\n日班:08:30~17:30 休息一小時\n晚班:12:45-21:15 休息半小時"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 130293013,
        "companyId": 595118,
        "companyName": "英達資訊有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "2",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "GARMIN 品牌銷售　GPS衛星導航／行車紀錄器／運動休閒手錶／配件／週邊商品\n＊產品介紹 \n＊顧客服務 \n＊庫存管理\n＊整潔維護",
        "title": "Garmin 科技生活體驗館－(八德)門市人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 36,000元~60,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 100101,
          "name": "台北市中正區"
        },
        "recruitCount": 1,
        "mrtId": 110514,
        "mrtTime": 303,
        "mrtNear": 360,
        "benefits": [
          "1005",
          "1010",
          "1014",
          "1017",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "Garmin 科技生活體驗館－(八德)門市人員",
          "description": "GARMIN 品牌銷售　GPS衛星導航／行車紀錄器／運動休閒手錶／配件／週邊商品\n＊產品介紹 \n＊顧客服務 \n＊庫存管理\n＊整潔維護"
        }
      },
      {
        "updateAt": "2025/07/27 00:00:00",
        "jobId": 132086538,
        "companyId": 72429252,
        "companyName": "卷卷有限公司",
        "require": {
          "drivingLicense": [
            "1"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表",
        "title": "台中清水門市銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100926,
          "name": "台中市清水區"
        },
        "recruitCount": 11,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "台中清水門市銷售人員",
          "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 132040162,
        "companyId": 117198099,
        "companyName": "四寶貝美食團購有限公司",
        "require": {
          "drivingLicense": [
            "64"
          ],
          "certificates": [],
          "experience": "4",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "160100",
            "100100",
            "100200"
          ]
        },
        "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月",
        "title": "門市人員 (江翠店)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 250116,
          "name": "綜合商品批發代理"
        },
        "workCity": {
          "id": 100203,
          "name": "新北市板橋區"
        },
        "recruitCount": 1,
        "mrtId": 110509,
        "mrtTime": 0,
        "mrtNear": 433,
        "benefits": [
          "1001",
          "1005",
          "1010",
          "1017",
          "1023",
          "1024",
          "1026"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市人員 (江翠店)",
          "description": "著重在門市取貨、進貨\n（待人親切是最重要的，我們的出發點都希望客人有個愉快的購物）\n1.每天進貨、盤點\n（商品大多數是生鮮食品有點重，要能搬得動箱子唷）\n（大約需要 1小時-2小時）\n2.查詢團員訂單、前台結帳(大約3-4小時)\n3.通知團員取貨（大約需要 30分鐘-1小時）\n4.私訊回覆（大約需要 30分鐘-1小時）\n（其他小細節會在面試時詳談）\n工作內容會接觸到電腦，使用excel是基本技能\n面試會測試中打速度及快捷鍵(複製、貼上、搜尋)\n5. 主管交辦事項\n6. 需先在四寶貝相關門市訓練2~3個月"
        }
      },
      {
        "updateAt": "2025/07/27 00:00:00",
        "jobId": 132086539,
        "companyId": 72429252,
        "companyName": "卷卷有限公司",
        "require": {
          "drivingLicense": [
            "1"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表",
        "title": "台中北屯門市銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100906,
          "name": "台中市北屯區"
        },
        "recruitCount": 10,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "台中北屯門市銷售人員",
          "description": "接受顧客詢問或主動提供諮商建議給顧客\n陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n向顧客說明貨品的性質、特徵、品質與價格\n向客戶顯示商品的優點，以協助顧客選擇\n在成交後，包裝商品、收取款項、交付商品、開發票或收據，完成交易手續\n在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 103770365,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "喜歡甜點、享受與人互動的你，歡迎加入糖村團隊！\n我們正在尋找對銷售服務有熱情、願意學習、細心負責的你，擔任現場銷售夥伴！\n\n~工作內容~\n1.接待顧客，介紹糖村經典商品，提供親切服務。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.協助商品進貨、入庫與銷售及庫存管理。\n4.商品包裝、陳列與促銷品更換。\n5.執行庫存盤點、電腦行政作業等工作。\n6.協助維持店鋪整潔、營造良好購物環境。\n7.節慶期間支援專案活動（如：電話開發、報價、送樣等）。\n\n\n• 具備服務熱誠與主動積極的態度\n• 樂於學習，能配合假日與彈性排班\n• 喜歡與人互動、有銷售或門市經驗更佳\n• 能細心完成包裝、陳列等工作內容\n\n※完整教育訓練計畫與穩定升遷管道\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-台北瑞光店】門市兼職人員",
        "role": [
          "2"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "時薪 190元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100110,
          "name": "台北市內湖區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 2,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-台北瑞光店】門市兼職人員",
          "description": "喜歡甜點、享受與人互動的你，歡迎加入糖村團隊！\n我們正在尋找對銷售服務有熱情、願意學習、細心負責的你，擔任現場銷售夥伴！\n\n~工作內容~\n1.接待顧客，介紹糖村經典商品，提供親切服務。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.協助商品進貨、入庫與銷售及庫存管理。\n4.商品包裝、陳列與促銷品更換。\n5.執行庫存盤點、電腦行政作業等工作。\n6.協助維持店鋪整潔、營造良好購物環境。\n7.節慶期間支援專案活動（如：電話開發、報價、送樣等）。\n\n\n• 具備服務熱誠與主動積極的態度\n• 樂於學習，能配合假日與彈性排班\n• 喜歡與人互動、有銷售或門市經驗更佳\n• 能細心完成包裝、陳列等工作內容\n\n※完整教育訓練計畫與穩定升遷管道\n＜幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      },
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 132009129,
        "companyId": 69601638,
        "companyName": "金車生物科技股份有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。",
        "title": "金車生物科技★礁溪門市人員★日班8:00-17:00★礁溪場",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 190101,
          "name": "農藝╱園藝相關"
        },
        "workCity": {
          "id": 100403,
          "name": "宜蘭縣礁溪鄉"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "金車生物科技★礁溪門市人員★日班8:00-17:00★礁溪場",
          "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。"
        }
      },
      {
        "updateAt": "2025/07/14 13:40:47",
        "jobId": 130157790,
        "companyId": 50921958,
        "companyName": "億進寢具企業有限公司(總公司)",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "2024年2月11日開始『億進寢具』在不景氣環境下及考慮員工工作與生活平衡，全面調整『縮短營業時間』及『提升薪資待遇』!!!\n\n✨全台直營連鎖店正持續擴展中~目前已有44家直營門市!!!積極招募熱情、願意學習、願意自我挑戰、具有企圖心，想為自己人生創造高薪收入，對銷售有興趣的夥伴們，加入我們『億進寢具』成為幸福億家人。✨\n\n✨本公司重視員工教育訓練與客戶服務，提供完善的教育訓練制度及專業的學習培訓環境，經勞動部發展署TTQS人才發展品質管理系統(企業機構版)評核，榮獲銅牌肯定!✨\n\n✨我們非常重視員工健康生活，提供多元運動項目、減重比賽等活動，連續2年榮獲教育部體育署辦理職工運動活動及聘用運動人才之表揚企業!✨\n\n【工作內容】\n1.商品陳列及換檔等工作。\n2.商品進出貨、銷售管理、庫存管理。\n3.介紹及銷售公司之商品與服務。\n4.門市環境清潔與維護。\n5.配合公司人力調度與配置(工作排班)、人員工作指導與教育訓練。\n6.從事門市營運必要之工作其他公司交辦之事項。\n\n【履歷投遞說明】\n1.履歷請填寫完整。\n2.初步須審查履歷，若未合格者恕無另行通知。",
        "title": "門市銷售專員／台南安南店",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 35,033元~60,000元",
        "industry": {
          "id": 110305,
          "name": "其他紡織製品製造"
        },
        "workCity": {
          "id": 101606,
          "name": "台南市安南區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1005",
          "1006",
          "1010",
          "1014"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "門市銷售專員／台南安南店",
          "description": "2024年2月11日開始『億進寢具』在不景氣環境下及考慮員工工作與生活平衡，全面調整『縮短營業時間』及『提升薪資待遇』!!!\n\n✨全台直營連鎖店正持續擴展中~目前已有44家直營門市!!!積極招募熱情、願意學習、願意自我挑戰、具有企圖心，想為自己人生創造高薪收入，對銷售有興趣的夥伴們，加入我們『億進寢具』成為幸福億家人。✨\n\n✨本公司重視員工教育訓練與客戶服務，提供完善的教育訓練制度及專業的學習培訓環境，經勞動部發展署TTQS人才發展品質管理系統(企業機構版)評核，榮獲銅牌肯定!✨\n\n✨我們非常重視員工健康生活，提供多元運動項目、減重比賽等活動，連續2年榮獲教育部體育署辦理職工運動活動及聘用運動人才之表揚企業!✨\n\n【工作內容】\n1.商品陳列及換檔等工作。\n2.商品進出貨、銷售管理、庫存管理。\n3.介紹及銷售公司之商品與服務。\n4.門市環境清潔與維護。\n5.配合公司人力調度與配置(工作排班)、人員工作指導與教育訓練。\n6.從事門市營運必要之工作其他公司交辦之事項。\n\n【履歷投遞說明】\n1.履歷請填寫完整。\n2.初步須審查履歷，若未合格者恕無另行通知。"
        }
      },
      {
        "updateAt": "2025/07/31 00:00:00",
        "jobId": 92234280,
        "companyId": 73463474,
        "companyName": "崑山玩具精品",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "我們是一家專注於玩具銷售的專業門市。我們的主要服務對象涵蓋各年齡層的顧客，並提供各類具創意與教育意義的玩具。致力打造一個讓顧客感受到快樂與興趣的購物環境。\n我們誠摯邀請熱愛玩具、擁有服務熱忱的伙伴加入我們，一起為顧客帶來美好的購物體驗。\n\n\n工作內容：\n1. 商品的陳列與擺放，確保商品依照陳列標準分類，可吸引顧客目光。\n2. 維護營業場地的清潔與舒適，包括定時巡檢與環境整理。\n3. 提供專業的產品與價格諮詢，向顧客解釋商品特性及用途。\n4. 操作櫃檯收銀機進行收銀作業，包括結帳、發票及現金管理。\n5. 主動與顧客互動，熱情解答問題，增強顧客購物的滿意度。\n6. 快速學習玩具產品知識，能準確地介紹新品及促銷資訊。\n7. 協助庫存管理，包括商品上架、補貨與庫存數據維護。\n8. 支援促銷活動的佈置與執行，包含活動區域設置、商品介紹。\n\n【注意事項】\n有時間觀念，不遲到\n注重禮儀\n\n【親洽面試】\n預約好時間，到店洽詢店長\n敬請重視求職禮儀!並請記得攜帶履歷!\n\n\n【薪資面議】\n依照工作能力經驗而定及入職後調整\n\n\n我們正在尋找注重細節、善於溝通且充滿熱誠的您加入我們，與我們一起成為業界中的佼佼者，共同為顧客提供愉快的購物體驗！立即投遞履歷，我們期待您的加入！",
        "title": "★左營店★ 門市人員/有經驗者佳/親洽優先",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元以上",
        "industry": {
          "id": 111602,
          "name": "玩具相關"
        },
        "workCity": {
          "id": 101811,
          "name": "高雄市左營區"
        },
        "recruitCount": 3,
        "mrtId": 280114,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "★左營店★ 門市人員/有經驗者佳/親洽優先",
          "description": "我們是一家專注於玩具銷售的專業門市。我們的主要服務對象涵蓋各年齡層的顧客，並提供各類具創意與教育意義的玩具。致力打造一個讓顧客感受到快樂與興趣的購物環境。\n我們誠摯邀請熱愛玩具、擁有服務熱忱的伙伴加入我們，一起為顧客帶來美好的購物體驗。\n\n\n工作內容：\n1. 商品的陳列與擺放，確保商品依照陳列標準分類，可吸引顧客目光。\n2. 維護營業場地的清潔與舒適，包括定時巡檢與環境整理。\n3. 提供專業的產品與價格諮詢，向顧客解釋商品特性及用途。\n4. 操作櫃檯收銀機進行收銀作業，包括結帳、發票及現金管理。\n5. 主動與顧客互動，熱情解答問題，增強顧客購物的滿意度。\n6. 快速學習玩具產品知識，能準確地介紹新品及促銷資訊。\n7. 協助庫存管理，包括商品上架、補貨與庫存數據維護。\n8. 支援促銷活動的佈置與執行，包含活動區域設置、商品介紹。\n\n【注意事項】\n有時間觀念，不遲到\n注重禮儀\n\n【親洽面試】\n預約好時間，到店洽詢店長\n敬請重視求職禮儀!並請記得攜帶履歷!\n\n\n【薪資面議】\n依照工作能力經驗而定及入職後調整\n\n\n我們正在尋找注重細節、善於溝通且充滿熱誠的您加入我們，與我們一起成為業界中的佼佼者，共同為顧客提供愉快的購物體驗！立即投遞履歷，我們期待您的加入！"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 130431473,
        "companyId": 490216,
        "companyName": "三井資訊股份有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "本公司擴大營業，廣邀人才，提供絕佳的『升遷』機會！提供保障薪資+優渥的銷售獎金+完善的員工褔利 + 完整的訓練發展 + 絕佳的『升遷』機會。『時勢造英雄』，歡迎您加入，一起共創未來。\n【工作內容】: \n1.羅技商品銷售與陳列管理。\n2.客戶服務與管理 。\n3.門市營運作業 。\n4.營運幹部培訓。\n5.偶爾須配合支援資訊展等活動。\n6.業績獎金另計。\n7.月休9~10天。\n8.本門市預計三月開幕，故報到後會先至鄰近門市受訓。",
        "title": "門市服務銷售專員【羅技南港lalaport旗艦館】",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 31,590元~55,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 100111,
          "name": "台北市南港區"
        },
        "recruitCount": 1,
        "mrtId": 110123,
        "mrtTime": 377,
        "mrtNear": 464,
        "benefits": [
          "1005",
          "1006",
          "1014",
          "1019",
          "1024"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "門市服務銷售專員【羅技南港lalaport旗艦館】",
          "description": "本公司擴大營業，廣邀人才，提供絕佳的『升遷』機會！提供保障薪資+優渥的銷售獎金+完善的員工褔利 + 完整的訓練發展 + 絕佳的『升遷』機會。『時勢造英雄』，歡迎您加入，一起共創未來。\n【工作內容】: \n1.羅技商品銷售與陳列管理。\n2.客戶服務與管理 。\n3.門市營運作業 。\n4.營運幹部培訓。\n5.偶爾須配合支援資訊展等活動。\n6.業績獎金另計。\n7.月休9~10天。\n8.本門市預計三月開幕，故報到後會先至鄰近門市受訓。"
        }
      },
      {
        "updateAt": "2025/07/31 01:00:33",
        "jobId": 78431188,
        "companyId": 70672475,
        "companyName": "嘉鑫科技有限公司",
        "require": {
          "drivingLicense": [
            "65"
          ],
          "certificates": [],
          "experience": "1",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 負責智慧型手機及相關通訊商品的銷售，清楚介紹功能與規格特性。  \n2. 提供門號資費方案規劃，依客戶需求設計最適合的通訊方案。  \n3. 接待顧客，協助解答電話諮詢，處理調貨與產品設定服務。  \n4. 負責商品進貨入庫檢查，定期執行銷售與庫存管理。  \n5. 負責通訊商品的包裝作業、商品陳列及促銷品的更新更換。  \n6. 維護門市內外環境整潔，提升顧客購物體驗。  \n7. 執行銷售數據分析，制定改善策略以達成門市業績目標。  \n8. 參與定期培訓與會議，深化專業知識並提升服務品質。",
        "title": "【 嘉新電腦通訊埔里門市】通訊門市人員/儲備幹部 (薪資30000~65000)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~65,000元",
        "industry": {
          "id": 250216,
          "name": "電子通訊╱電腦週邊零售業"
        },
        "workCity": {
          "id": 101105,
          "name": "南投縣埔里鎮"
        },
        "recruitCount": 4,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1014",
          "1019",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【 嘉新電腦通訊埔里門市】通訊門市人員/儲備幹部 (薪資30000~65000)",
          "description": "1. 負責智慧型手機及相關通訊商品的銷售，清楚介紹功能與規格特性。  \n2. 提供門號資費方案規劃，依客戶需求設計最適合的通訊方案。  \n3. 接待顧客，協助解答電話諮詢，處理調貨與產品設定服務。  \n4. 負責商品進貨入庫檢查，定期執行銷售與庫存管理。  \n5. 負責通訊商品的包裝作業、商品陳列及促銷品的更新更換。  \n6. 維護門市內外環境整潔，提升顧客購物體驗。  \n7. 執行銷售數據分析，制定改善策略以達成門市業績目標。  \n8. 參與定期培訓與會議，深化專業知識並提升服務品質。"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 113056625,
        "companyId": 73207520,
        "companyName": "統一超商(玉泓企業社)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 負責連鎖系列直營店或分店內督導及協調所屬人員之日常工作及營運。\n2. 於專櫃、賣場或門市裡提供商品介紹及銷售服務。\n3. 本類人員處理客戶訂單、查詢及處理一般性客戶投訴事宜、門市經營等人員。",
        "title": "(代天府門市)正職人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~30,000元",
        "industry": {
          "id": 250218,
          "name": "其他零售業"
        },
        "workCity": {
          "id": 101624,
          "name": "台南市北門區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "(代天府門市)正職人員",
          "description": "1. 負責連鎖系列直營店或分店內督導及協調所屬人員之日常工作及營運。\n2. 於專櫃、賣場或門市裡提供商品介紹及銷售服務。\n3. 本類人員處理客戶訂單、查詢及處理一般性客戶投訴事宜、門市經營等人員。"
        }
      },
      {
        "updateAt": "2025/07/31 00:10:00",
        "jobId": 98638344,
        "companyId": 73217699,
        "companyName": "百吉玩具有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "🎮誠徵夜市遊戲攤位儲備幹部🎮\n               （男女不拘)\n🔴地點：高雄市區（瑞豐夜市）\n\n🟡上班時間：\n星期二、四、五  17:00-01:00\n星期六、日          16:00-01:00\n每日8小時、加班費另計\n\n🔴工作內容：擺攤、招呼客人、收攤、等等...\n\n❮薪資❯\n•底薪  薪資:$30000~$40000\n\n❮獎金❯  (其他獎金另外加給‼️)\n•全勤獎金:$2000\n•職位津貼:依職位進行加給\n•團體業績獎金:依團體整體業績,每月進行分紅。\n•個人業績獎金:依個人負責攤位業績,每月進行分紅。\n•勞保、健保補貼\n\n\n🟡意者請洽👉林先生 0921063975 \n\n🔴意者請洽👉陳先生 0981442561\n\n盡量用Line聯繫、以免沒回覆到各位\n（電話聯絡或加LINE的、請用電話搜尋👆同上方電話）\n\n本公司正在擴大招募人才、對於挑戰高薪這邊是個好地方、只要您肯學習、只要您夠積極、只要您夠努力💪、還是您肯吃苦、又或著是您另有其他過人之處、歡迎你們來加入我們的團隊。\n\n公司有提供非常好的學習環境、訓練流程、及地表最強團隊、歡迎👏想轉換跑道看看的新鮮人！！！",
        "title": "【儲備幹部】遊戲攤位人員",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~40,000元",
        "industry": {
          "id": 210102,
          "name": "休閒服務"
        },
        "workCity": {
          "id": 101811,
          "name": "高雄市左營區"
        },
        "recruitCount": 1,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "【儲備幹部】遊戲攤位人員",
          "description": "🎮誠徵夜市遊戲攤位儲備幹部🎮\n               （男女不拘)\n🔴地點：高雄市區（瑞豐夜市）\n\n🟡上班時間：\n星期二、四、五  17:00-01:00\n星期六、日          16:00-01:00\n每日8小時、加班費另計\n\n🔴工作內容：擺攤、招呼客人、收攤、等等...\n\n❮薪資❯\n•底薪  薪資:$30000~$40000\n\n❮獎金❯  (其他獎金另外加給‼️)\n•全勤獎金:$2000\n•職位津貼:依職位進行加給\n•團體業績獎金:依團體整體業績,每月進行分紅。\n•個人業績獎金:依個人負責攤位業績,每月進行分紅。\n•勞保、健保補貼\n\n\n🟡意者請洽👉林先生 0921063975 \n\n🔴意者請洽👉陳先生 0981442561\n\n盡量用Line聯繫、以免沒回覆到各位\n（電話聯絡或加LINE的、請用電話搜尋👆同上方電話）\n\n本公司正在擴大招募人才、對於挑戰高薪這邊是個好地方、只要您肯學習、只要您夠積極、只要您夠努力💪、還是您肯吃苦、又或著是您另有其他過人之處、歡迎你們來加入我們的團隊。\n\n公司有提供非常好的學習環境、訓練流程、及地表最強團隊、歡迎👏想轉換跑道看看的新鮮人！！！"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 132027113,
        "companyId": 9780802,
        "companyName": "耶里食品有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.接待顧客、結帳收銀。\n2.補貨、簡易商品擺設整理。\n3.協助店內日常清潔與運作。\n\n【待遇符勞基法規定】\n勞、健保  等\n\n【員工福利】\n 免費 晚餐天天有，每月可省好幾千。",
        "title": "[供晚餐]晚班門市 (仁愛本店) 請電洽2705-2578",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100105,
          "name": "台北市大安區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "[供晚餐]晚班門市 (仁愛本店) 請電洽2705-2578",
          "description": "1.接待顧客、結帳收銀。\n2.補貨、簡易商品擺設整理。\n3.協助店內日常清潔與運作。\n\n【待遇符勞基法規定】\n勞、健保  等\n\n【員工福利】\n 免費 晚餐天天有，每月可省好幾千。"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 78421167,
        "companyId": 69601638,
        "companyName": "金車生物科技股份有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。",
        "title": "金車生物科技★水產門市人員★日班8:00-17:00★礁溪場",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,600元~35,000元",
        "industry": {
          "id": 190101,
          "name": "農藝╱園藝相關"
        },
        "workCity": {
          "id": 100403,
          "name": "宜蘭縣礁溪鄉"
        },
        "recruitCount": 4,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16,
          134217728
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "金車生物科技★水產門市人員★日班8:00-17:00★礁溪場",
          "description": "職務內容：\n\t•\t提供親切且專業的顧客服務，解答商品相關問題。\n\t•\t負責商品銷售、推薦及顧客結帳服務。\n\t•\t維持門市整潔、陳列商品及庫存管理。\n\t•\t處理顧客需求，提升購物體驗與滿意度。\n\t•\t協助店內活動推廣，增加品牌曝光度。\n\n職位要求：\n\t•\t具備良好的溝通能力，熱愛服務業，親切有耐心。\n\t•\t具備銷售經驗者佳，無經驗可培訓。\n\t•\t能適應輪班及假日排班，具高度責任感。\n\t•\t具備團隊合作精神，能與同事良好協作。\n\n我們提供：\n\t•\t穩定薪資+獎金制度。\n\t•\t員工折扣優惠，讓你更了解品牌與商品。\n\t•\t友善工作環境與專業培訓，助你提升職場競爭力。\n\t•\t享有勞健保、年假、各類獎金與福利制度。"
        }
      },
      {
        "updateAt": "2025/07/30 00:00:00",
        "jobId": 132087785,
        "companyId": 73541456,
        "companyName": "鑫富航企業有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [
            2,
            8,
            16
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.介紹品牌及銷售商品\n2.顧客接待及提供需求服務(如：調貨、包裝及退換貨處理、諮詢等)\n3.商品收銀、進貨、庫存管理\n4.商品包裝、陳列及促銷品換檔\n5.維持店櫃周遭整潔",
        "title": "百貨專櫃人員 (天母大葉高島屋)",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 28,590元~80,000元",
        "industry": {
          "id": 250214,
          "name": "百貨相關"
        },
        "workCity": {
          "id": 100108,
          "name": "台北市士林區"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1010"
        ],
        "companyTags": [],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "百貨專櫃人員 (天母大葉高島屋)",
          "description": "1.介紹品牌及銷售商品\n2.顧客接待及提供需求服務(如：調貨、包裝及退換貨處理、諮詢等)\n3.商品收銀、進貨、庫存管理\n4.商品包裝、陳列及促銷品換檔\n5.維持店櫃周遭整潔"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 78471984,
        "companyId": 6248,
        "companyName": "糖村 (笠豐食品有限公司)",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "3",
          "grades": [],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.門市服務: 對台灣顧客貼心、外國旅客熱情! \n2.介紹銷售: 常客、熟客、外國客，哪個是他們最喜歡的?\n用專業知識與經驗match客人需要的! \n我們不是銷售員，我們是客人專業的消費顧問!\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n店舖經營管理、人事溝通技巧、商圈店舖開發",
        "title": "【糖村-信義A8】專櫃正職人員",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 33,000元~35,000元",
        "industry": {
          "id": 220204,
          "name": "麵包店╱點心烘焙店"
        },
        "workCity": {
          "id": 100107,
          "name": "台北市信義區"
        },
        "recruitCount": 3,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1001",
          "1005",
          "1006"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": true,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "【糖村-信義A8】專櫃正職人員",
          "description": "1.門市服務: 對台灣顧客貼心、外國旅客熱情! \n2.介紹銷售: 常客、熟客、外國客，哪個是他們最喜歡的?\n用專業知識與經驗match客人需要的! \n我們不是銷售員，我們是客人專業的消費顧問!\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n店舖經營管理、人事溝通技巧、商圈店舖開發"
        }
      }
    ],
    "fromOffset": 0
  },
  "recommendJobs": {
    "hits": [
      {
        "updateAt": "2025/07/27 00:10:00",
        "jobId": 75808167,
        "companyId": 68754471,
        "companyName": "高北牛乳大王",
        "require": {
          "drivingLicense": [
            "2"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1. 接受顧客詢問或主動提供諮商建議給顧客\n2. 陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n3. 向顧客說明貨品的性質、特徵、品質與價格\n4. 在成交後，包裝商品、收取款項、交付商品，完成交易手續\n5. 在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n6. POS系統收銀機操作、結帳\n7. 每日填寫工作報表。\n8. 需不定期回總公司接受教育訓練課程。\n9. 視公司需求支援面銷活動。",
        "title": "行動餐車部晚班客服正職 高北牛乳大王",
        "role": [
          "1"
        ],
        "remind": [
          0
        ],
        "replyInDays": 0,
        "salary": "月薪 30,000元~36,000元",
        "industry": {
          "id": 220202,
          "name": "飲料店"
        },
        "workCity": {
          "id": 101208,
          "name": "彰化縣和美鎮"
        },
        "recruitCount": 0,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "1003",
          "1004",
          "1006",
          "1019",
          "1024"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": false,
        "highlight": {
          "title": "行動餐車部晚班客服正職 高北牛乳大王",
          "description": "1. 接受顧客詢問或主動提供諮商建議給顧客\n2. 陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n3. 向顧客說明貨品的性質、特徵、品質與價格\n4. 在成交後，包裝商品、收取款項、交付商品，完成交易手續\n5. 在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n6. POS系統收銀機操作、結帳\n7. 每日填寫工作報表。\n8. 需不定期回總公司接受教育訓練課程。\n9. 視公司需求支援面銷活動。"
        }
      },
      {
        "updateAt": "2025/07/29 09:55:13",
        "jobId": 132050507,
        "companyId": 73519128,
        "companyName": "沛思彩虹行銷有限公司",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            1,
            2,
            8,
            16,
            32
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "1.3C配件、微家電、耳機，各式手機貼膜銷售,無經驗可教導培訓\n2.店面環境整潔\n3.協助店長交辦事宜。\n4.銷售獎金另計\n\n*表現良好或對此產業有高度興趣者可培訓為儲備店長*\n*我們有多家分店,需可配合支援北部分店*",
        "title": "＜桃園新光影城＞手機貼膜/3C產品門市人員(無經驗可培訓)",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 34,000元以上",
        "industry": {
          "id": 250114,
          "name": "電子通訊╱電腦週邊批發"
        },
        "workCity": {
          "id": 100501,
          "name": "桃園市中壢區"
        },
        "recruitCount": 5,
        "mrtId": 150118,
        "mrtTime": 0,
        "mrtNear": 331,
        "benefits": [
          "0"
        ],
        "companyTags": [],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "＜桃園新光影城＞手機貼膜/3C產品門市人員(無經驗可培訓)",
          "description": "1.3C配件、微家電、耳機，各式手機貼膜銷售,無經驗可教導培訓\n2.店面環境整潔\n3.協助店長交辦事宜。\n4.銷售獎金另計\n\n*表現良好或對此產業有高度興趣者可培訓為儲備店長*\n*我們有多家分店,需可配合支援北部分店*"
        }
      },
      {
        "updateAt": "2025/07/30 00:10:00",
        "jobId": 92137968,
        "companyId": 73075810,
        "companyName": "冠宏通信行",
        "require": {
          "drivingLicense": [
            "0"
          ],
          "certificates": [],
          "experience": "0",
          "grades": [
            2,
            8,
            16,
            32,
            64
          ],
          "majors": [
            "0",
            "0",
            "0"
          ]
        },
        "description": "【門市服務、門市銷售及電信業務推廣】 \n\n1. 電信門市、遠傳電信門號申辦、通訊商品(配件)、門市銷售服務 \n2. 提供顧客之接待與需求服務 \n3. 月薪35000元 . 另有個人及團隊獎金 . 平均月收入5萬 . 不倒扣底薪  \n4. 月排休9天 . 團隊內自己討論排休 . 可排連休 . 可排假日 \n5. 明確晉升制度依照個人能力及成績取得晉升 \n6. 無經驗可 \n*有一年以上電信系統店或通訊行門市銷售經驗者薪資另議公司來進行面談！〝",
        "title": "遠傳電信 北港大同 銷售人員",
        "role": [
          "1"
        ],
        "remind": [
          2
        ],
        "replyInDays": 0,
        "salary": "月薪 35,000元~50,000元",
        "industry": {
          "id": 100201,
          "name": "電信相關"
        },
        "workCity": {
          "id": 101316,
          "name": "雲林縣北港鎮"
        },
        "recruitCount": 2,
        "mrtId": 0,
        "mrtTime": 0,
        "mrtNear": 0,
        "benefits": [
          "0"
        ],
        "companyTags": [
          16
        ],
        "isHappiness": false,
        "internship": [
          1
        ],
        "jobType": 1,
        "bookmarked": false,
        "hasMedias": false,
        "isTop": false,
        "hasCompanyLogo": true,
        "highlight": {
          "title": "遠傳電信 北港大同 銷售人員",
          "description": "【門市服務、門市銷售及電信業務推廣】 \n\n1. 電信門市、遠傳電信門號申辦、通訊商品(配件)、門市銷售服務 \n2. 提供顧客之接待與需求服務 \n3. 月薪35000元 . 另有個人及團隊獎金 . 平均月收入5萬 . 不倒扣底薪  \n4. 月排休9天 . 團隊內自己討論排休 . 可排連休 . 可排假日 \n5. 明確晉升制度依照個人能力及成績取得晉升 \n6. 無經驗可 \n*有一年以上電信系統店或通訊行門市銷售經驗者薪資另議公司來進行面談！〝"
        }
      }
    ]
  },
  "seo": {
    "canonicalLink": "https://www.1111.com.tw/search/job?page=2&d0=120400",
    "nextLink": "https://www.1111.com.tw/search/job?page=3&d0=120400",
    "title": "「門市銷售」職缺 - 2025年7月熱門工作機會｜1111人力銀行",
    "keyword": "門市銷售,找工作,工作,求職,搜尋工作,查詢工作,職缺,工作機會",
    "description": "(2025/7/31)－(41528) 個工作職缺機會｜(門市人員【金好運投注站】、門市人員【寶御食品有限公司(宝泉)】、銷售員假日班【花間集養生事業有限公司】、門市人員【四寶貝美食團購有限公司】、正職營業人員（鳳山店）【旺來昌實業股份有限公司】)....等。1111提供全台最齊全的工作職缺及求職網路平台服務，更多找工作職缺請上1111。",
    "imageUrl": "https://www.1111.com.tw/talents/MicroBlogging/images/facebook_banner01.jpg",
    "ldJson": {
      "@context": "https://schema.org/",
      "@type": "CollectionPage",
      "name": "1111人力銀行 - 職位搜尋結果",
      "description": "搜尋結果頁面，顯示與關鍵字相關的職位列表。",
      "url": "https://www.1111.com.tw/search/job",
      "mainEntity": {
        "@type": "ItemList",
        "itemListElement": [
          {
            "item": {
              "title": "【糖村-台中福雅店】門市正職人員",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "糖村"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "台中市"
              },
              "datePosted": "2022/3/21",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/98706131",
              "description": "1.負責介紹及銷售門市商品。\n2.提供顧客之接待與需求服務。\n（如：電話諮詢、調貨、修改、包裝及退換貨處理）\n3.結帳包裝、陳列及促銷品換檔工作\n4.進貨庫存、補貨上架、陳列擺設\n5.執行庫存盤點、電腦行政作業等工作\n6.維持店鋪環境整潔\n7.可配合店鋪輪調尤佳\n\n幹部＞五大組別工作規劃執行[排班、訓練、訂貨、客服、維修]\n＜主管＞店舖經營管理、人事溝通技巧、商圈店舖開發"
            },
            "@type": "ListItem",
            "position": 1
          },
          {
            "item": {
              "title": "全家便利商店-加盟店【民善店】大夜班正職人員",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "全家便利商店"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "台北市"
              },
              "datePosted": "2018/9/26",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/85120237",
              "description": "1.櫃檯收銀\n2.賣場清潔、商品補貨\n3.機台清潔"
            },
            "@type": "ListItem",
            "position": 2
          },
          {
            "item": {
              "title": "銷售員假日班",
              "hiringOrganization": {
                "@type": "Organization",
                "name": "花間集養生事業有限公司"
              },
              "jobLocation": {
                "@type": "Place",
                "name": "新北市"
              },
              "datePosted": "2025/5/21",
              "@type": "JobPosting",
              "url": "https://www.1111.com.tw/job/130198445",
              "description": "1.熟悉欲銷售之產品、公司規章作業流程\n2.接受顧客詢問或主動提供諮商建議給顧客\n3.陳列商品、清潔櫥窗、維持營業場所的整潔與美觀\n4.向顧客說明貨品的性質、特徵、品質與價格\n5.在當天結束營業前，統計銷售情形、盤點貨品存量及撰寫當日業務報表\n\n星期六、日(10:30-21:30)\n午餐及晚餐各用餐一小時\n享勞健保及勞退、 日薪高2000元以上\n\n(需輪蘆洲及重新家樂福)\n\n*如您想進一步了解詳細工作內容，應徵方式請先加入LINE ID:flower22683268 \n加入完成後請傳訊息給我，你要應徵的門市店名，我們先以在LINE方式先做簡易面試~"
            },
            "@type": "ListItem",
            "position": 3
          }
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://www.1111.com.tw/search/job?ks={search_term_string}",
        "query-input": "required name=search_term_string"
      },
      "@keywords": [
        "門市銷售",
        "找工作",
        "工作",
        "求職",
        "搜尋工作",
        "查詢工作",
        "職缺",
        "工作機會"
      ]
    }
  },
  "query": null
}


================================================
FILE: crawler/project_1111/parser_apidata_1111.py
================================================
import re
from datetime import datetime
from typing import Optional
import structlog
from bs4 import BeautifulSoup

from crawler.database.schemas import (
    JobPydantic,
    SourcePlatform,
    JobStatus,
    JobType,
)
from crawler.project_1111.config_1111 import JOB_DETAIL_BASE_URL_1111
from crawler.utils.salary_parser import parse_salary_text # Import the new parser

logger = structlog.get_logger(__name__)

# 1111 API 的 jobType 到我們內部 JobType Enum 的映射
JOB_TYPE_MAPPING_1111 = {
    "全職": JobType.FULL_TIME,
    "兼職": JobType.PART_TIME,
    "實習": JobType.INTERNSHIP,
    "派遣": JobType.CONTRACT,
    "約聘": JobType.TEMPORARY,
    "其他": JobType.OTHER,
}

# 1111 API 的 jobType (整數) 到字串的映射
JOB_TYPE_INT_TO_STR_MAPPING_1111 = {
    1: "全職",
    2: "兼職",
    3: "實習",
    4: "派遣",
    5: "約聘",
}

# 1111 API 的教育程度 (grades) 映射
EDUCATION_MAPPING_1111 = {
    2: "高中",
    8: "專科",
    16: "大學",
    32: "碩士",
    64: "博士",
}

# Removed the old parse_salary function

# This function is no longer needed as the logic is now in salary_parser.py
# def derive_salary_type(salary_text: str, salary_min: Optional[int], job_type: Optional[JobType]) -> Optional[SalaryType]:
#     logger.debug("Deriving salary type", salary_text=salary_text, salary_min=salary_min, job_type=job_type)
#     text = salary_text.replace(",", "").replace(" ", "").lower()
#
#     # Priority 1: Explicit keywords
#     if "月薪" in text:
#         logger.debug("Derived: MONTHLY (keyword)")
#         return SalaryType.MONTHLY
#     elif "時薪" in text:
#         logger.debug("Derived: HOURLY (keyword)")
#         return SalaryType.HOURLY
#     elif "年薪" in text:
#         logger.debug("Derived: YEARLY (keyword)")
#         return SalaryType.YEARLY
#     elif "日薪" in text:
#         logger.debug("Derived: DAILY (keyword)")
#         return SalaryType.DAILY
#     elif "論件計酬" in text:
#         logger.debug("Derived: BY_CASE (keyword)")
#         return SalaryType.BY_CASE
#     
#     # Priority 2: "面議" combined with "萬" (implying monthly)
#     # This handles "面議（經常性薪資達4萬元或以上）" -> 月薪
#     if "面議" in text and "萬" in text:
#         logger.debug("Derived: MONTHLY (negotiable + wan)")
#         return SalaryType.MONTHLY # Assuming "萬" in "面議" context implies monthly
#
#     # Priority 3: If job_type is FULL_TIME, assume MONTHLY
#     if job_type == JobType.FULL_TIME:
#         logger.debug("Derived: MONTHLY (full-time job type)")
#         return SalaryType.MONTHLY
#
#     # Priority 4: General "面議"
#     if "面議" in text:
#         logger.debug("Derived: NEGOTIABLE (general negotiable)")
#         return SalaryType.NEGOTIABLE
#     
#     # Priority 5: Numerical inference based on salary_min (if no keyword found)
#     if salary_min is not None:
#         if salary_min < 2000: # Adjusted threshold for hourly
#             logger.debug("Derived: HOURLY (numerical)")
#             return SalaryType.HOURLY
#         elif salary_min > 200000:
#             logger.debug("Derived: YEARLY (numerical)")
#             return SalaryType.YEARLY
#         else:
#             logger.debug("Derived: MONTHLY (numerical)")
#             return SalaryType.MONTHLY # Default to monthly for typical ranges
#
#     logger.debug("Derived: None (no match)")
#     return None

def parse_job_list_json_to_pydantic(job_item: dict) -> Optional[JobPydantic]:
    """
    從 1111 列表頁 API 的 JSON 數據解析並轉換為 JobPydantic 物件。
    """
    try:
        job_id = str(job_item.get("jobId"))
        company_source_id = str(job_item.get("companyId"))

        url = f"{JOB_DETAIL_BASE_URL_1111}{job_id}"
        company_url = f"https://www.1111.com.tw/corp/{company_source_id}"

        title = job_item.get("title")
        company_name = job_item.get("companyName")
        description = job_item.get("description")
        location_text = job_item.get("workCity", {}).get("name", "").strip() or None

        posted_at = None
        update_at_str = job_item.get("updateAt")
        if update_at_str:
            try:
                posted_at = datetime.strptime(update_at_str, "%Y/%m/%d %H:%M:%S")
            except ValueError:
                logger.warning(
                    "Could not parse posted_at date format from list API.",
                    update_at=update_at_str,
                    job_id=job_id,
                )

        salary_text = job_item.get("salary", "")
        # Derive job_type first, as it's needed for derive_salary_type
        job_type_int = job_item.get("jobType")
        job_type_str = JOB_TYPE_INT_TO_STR_MAPPING_1111.get(job_type_int)
        job_type = JOB_TYPE_MAPPING_1111.get(job_type_str) if job_type_str else None

        # Use the new parse_salary_text from salary_parser.py
        salary_min, salary_max, salary_type = parse_salary_text(salary_text)
        # The following logic is now handled by the new salary_parser.py
        # salary_min, salary_max = parse_salary_text(salary_text)
        # # Derive salary_type using the new function, passing job_type
        # salary_type = derive_salary_type(salary_text, salary_min, job_type)

        experience_required_text = job_item.get("require", {}).get("experience")
        if experience_required_text == "0":
            experience_required_text = "不拘"
        elif experience_required_text is None:
            experience_required_text = "不拘"

        education_required_text = "不拘"
        education_grades = job_item.get("require", {}).get("grades")
        if education_grades and isinstance(education_grades, list):
            valid_grades = sorted(
                [g for g in education_grades if g in EDUCATION_MAPPING_1111]
            )
            if valid_grades:
                min_edu_code = min(valid_grades)
                education_required_text = EDUCATION_MAPPING_1111.get(
                    min_edu_code, "不拘"
                )
                if any(g > min_edu_code for g in valid_grades):
                    education_required_text += "以上"

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_1111,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=str(experience_required_text),
            education_required_text=education_required_text,
            company_source_id=company_source_id,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing 1111 job list JSON.",
            error=e,
            job_item=job_item,
            exc_info=True,
        )
        return None


def parse_job_detail_html_to_pydantic(
    html_content: str, url: str
) -> Optional[JobPydantic]:
    """
    從 1111 職缺頁面的 HTML 內容解析並轉換為 JobPydantic 物件。
    """
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        job_id = url.split("/")[-1].split("?")[0]

        # --- 1. 頁首區塊 (Top Section) ---
        header_section = soup.select_one(
            "section[data-v-e57f1019] > div.container > div.text-gray-600"
        )

        title = None
        company_name = None
        job_type_str = None
        salary_text = None
        education_required_text = None
        location_text = None
        posted_at = None
        experience_required_text = None
        description = None
        company_url = None
        company_source_id = None

        if header_section:
            title = header_section.select_one("h1").get_text(strip=True) or None
            company_name = (
                header_section.select_one("h2.inline").get_text(strip=True) or None
            )
            # Extract company_url from company_name's parent a tag
            company_link_tag = header_section.select_one("h2.inline a")
            if company_link_tag and "href" in company_link_tag.attrs:
                company_url = company_link_tag["href"]
                # Extract company_source_id from company_url if possible
                match = re.search(r"/corp/(\d+)", company_url)
                if match:
                    company_source_id = match.group(1)

            pills = header_section.select("div.flex.flex-wrap.mt-4.gap-3 > div")
            top_info = [p.get_text(strip=True, separator=" ") for p in pills]
            if len(top_info) >= 4:
                job_type_str = top_info[0]  # e.g., "全職"
                salary_text = top_info[1]
                education_required_text = top_info[2]
                location_text = top_info[3]

            info_items = header_section.select("ul.info-item > li")
            for item in info_items:
                key_tag = item.select_one("h3")
                val_tag = item.select_one("span") or item.select_one("time")
                if key_tag and val_tag:
                    key = key_tag.get_text(strip=True)
                    value = val_tag.get_text(strip=True)
                    if "更新日期" in key:
                        try:
                            posted_at = datetime.strptime(
                                value.replace(" ", ""), "%Y/%m/%d"
                            )
                        except ValueError:
                            logger.warning(
                                "Could not parse posted_at date format.",
                                value=value,
                                job_id=job_id,
                            )
                    elif "工作經驗" in key:
                        experience_required_text = value

        # --- 2. 主要內容區塊 (Main Content Sections) ---
        sections = soup.select("section[id]")
        for section in sections:
            section_title_tag = section.select_one("h2.text-lg.text-main")
            if not section_title_tag:
                continue

            section_title = section_title_tag.get_text(strip=True)

            if section_title == "工作內容":
                # Find the job description within the '工作內容' section
                job_description_h3 = section.find(
                    "h3", string=lambda t: t and "職缺描述" in t
                )
                if job_description_h3:
                    description_container = job_description_h3.find_next_sibling()
                    if description_container:
                        description = description_container.get_text(
                            separator="\n", strip=True
                        )

        # Derive job_type first, as it's needed for derive_salary_type
        job_type = JOB_TYPE_MAPPING_1111.get(job_type_str)
        if job_type is None:
            job_type = JobType.OTHER

        # Use the new parse_salary_text from salary_parser.py
        salary_min, salary_max, salary_type = parse_salary_text(salary_text or "")
        # The following logic is now handled by the new salary_parser.py
        # salary_min, salary_max = parse_salary_text(salary_text or "")
        # # Derive salary_type using the new function, passing job_type
        # salary_type = derive_salary_type(salary_text or "", salary_min, job_type)

        if experience_required_text is None:
            experience_required_text = "不拘"
        if education_required_text is None:
            education_required_text = "不拘"

        job_pydantic_data = JobPydantic(
            source_platform=SourcePlatform.PLATFORM_1111,
            source_job_id=job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=title,
            description=description,
            job_type=job_type,
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=company_source_id,
            company_name=company_name,
            company_url=company_url,
        )
        return job_pydantic_data

    except Exception as e:
        logger.error(
            "Unexpected error when parsing 1111 job HTML.",
            error=e,
            url=url,
            exc_info=True,
        )
        return None


================================================
FILE: crawler/project_1111/producer_category_1111.py
================================================
from crawler.project_1111.task_category_1111 import fetch_and_sync_1111_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_1111.config_1111 import JOB_CAT_URL_1111

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_1111_categories.s(JOB_CAT_URL_1111).apply_async(queue='producer_category_1111')
logger.info("send task_category_1111 url", url=JOB_CAT_URL_1111, queue='producer_category_1111')


================================================
FILE: crawler/project_1111/producer_jobs_1111.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_1111.task_jobs_1111 import fetch_url_data_1111
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_1111_job_urls():
    """
    從資料庫讀取待處理或失敗的 1111 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 1111 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_1111,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 1111 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 1111 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 1111 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_1111.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_1111")

        logger.info(
            "已成功分發一批 1111 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_1111"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_1111/producer_urls_1111.py
================================================
from crawler.database.repository import get_all_categories_for_platform
from crawler.project_1111.task_urls_1111 import crawl_and_store_1111_category_urls
from crawler.database.models import SourcePlatform
import structlog

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Starting URL task distribution for all 1111 categories.")

all_1111_categories = get_all_categories_for_platform(SourcePlatform.PLATFORM_1111)

if all_1111_categories:
    logger.info("Found categories for PLATFORM_1111.", count=len(all_1111_categories))
    root_categories = [
        cat for cat in all_1111_categories if cat.parent_source_id is None
    ]

    if root_categories:
        logger.info(
            "Found root categories for PLATFORM_1111.", count=len(root_categories)
        )

        for category_info in root_categories:
            category_id: str = category_info.source_category_id
            logger.info("分發 URL 抓取任務", category_id=category_id)
            crawl_and_store_1111_category_urls.delay(category_info.model_dump())
    else:
        logger.info("No root categories found for PLATFORM_1111.")
else:
    logger.info("No categories found for PLATFORM_1111.")


================================================
FILE: crawler/project_1111/task_category_1111.py
================================================
# import os
# # python -m crawler.project_1111.task_category_1111
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog

from crawler.database import connection as db_connection
from crawler.database import repository
from crawler.database.connection import initialize_database
from crawler.database.schemas import SourcePlatform
from crawler.project_1111.client_1111 import fetch_category_data_from_1111_api
from crawler.project_1111.config_1111 import HEADERS_1111, JOB_CAT_URL_1111
from crawler.worker import app

# Import MAPPING from apply_classification.py
from crawler.database.category_classification_data.apply_classification import MAPPING

logger = structlog.get_logger(__name__)


def flatten_jobcat_recursive(node_list):
    """
    Flattens the 1111 job categories list, extracting main/sub categories.
    Applies major category mapping for top-level categories.
    """
    for node in node_list:
        current_parent_id = str(node.get("parentCode")) if node.get("parentCode") else None
        
        if current_parent_id is None: # Only apply mapping for top-level categories
            category_name = node.get("name")
            mapped_parent_id = MAPPING[SourcePlatform.PLATFORM_1111].get(category_name)
            if mapped_parent_id:
                current_parent_id = mapped_parent_id

        yield {
            "parent_source_id": current_parent_id,
            "source_category_id": str(node.get("code")),
            "source_category_name": node.get("name"),
            "source_platform": SourcePlatform.PLATFORM_1111.value,
        }


@app.task()
def fetch_and_sync_1111_categories(url_JobCat: str = JOB_CAT_URL_1111):
    logger.info("Current database connection", db_url=str(db_connection.get_engine().url))
    logger.info("Starting 1111 category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_1111)

        jobcat_data = fetch_category_data_from_1111_api(url_JobCat, HEADERS_1111)
        if jobcat_data is None:
            logger.error("Failed to fetch 1111 category data from API.", url=url_JobCat)
            return

        job_position_data = jobcat_data.get("jobPosition", [])
        if not job_position_data:
            logger.warning("No 'jobPosition' data found in 1111 category API response.", url=url_JobCat)
            return

        flattened_data = list(flatten_jobcat_recursive(job_position_data))

        if not existing_categories:
            logger.info("1111 category database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            repository.sync_source_categories(SourcePlatform.PLATFORM_1111, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_1111.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            logger.info(
                "Found new or updated 1111 categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_1111, categories_to_sync)
        else:
            logger.info("No new or updated 1111 categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during 1111 category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    initialize_database()
    logger.info("Dispatching fetch_and_sync_1111_categories task for local testing.", url=JOB_CAT_URL_1111)
    fetch_and_sync_1111_categories(JOB_CAT_URL_1111)



================================================
FILE: crawler/project_1111/task_jobs_1111.py
================================================
# import os
# # # python -m crawler.project_1111.task_jobs_1111
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

    
import structlog
from typing import Optional
from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_1111.client_1111 import fetch_job_data_from_1111_web
from crawler.project_1111.parser_apidata_1111 import parse_job_detail_html_to_pydantic
from crawler.database.connection import initialize_database

logger = structlog.get_logger(__name__)


@app.task()
def fetch_url_data_1111(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed information for a single job vacancy from a given URL,
    parses it, stores it in the database, and marks the URL's crawl status.
    """
    job_id = None
    try:
        job_id = url.split("/")[-1].split("?")[0]
        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        response_data = fetch_job_data_from_1111_web(url)
        if response_data is None or "content" not in response_data:
            logger.error("Failed to fetch job data from 1111 web.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        html_content = response_data["content"]

    except Exception as e:
        logger.error(
            "Unexpected error during web fetch or job ID extraction.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    job_pydantic_data = parse_job_detail_html_to_pydantic(html_content, url)

    if not job_pydantic_data:
        logger.error("Failed to parse job data.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None

    try:
        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error(
            "Unexpected error when upserting job data.",
            error=e,
            job_id=job_id,
            url=url,
            exc_info=True,
        )
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    initialize_database()

    PRODUCER_BATCH_SIZE = 20000000 # Changed from 10 to 20
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]

    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_1111,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info("Found URLs to process.", count=len(urls_to_process))
        for url in urls_to_process:
            logger.info("Processing URL.", url=url)
            fetch_url_data_1111(url)
    else:
        logger.info("No URLs found to process for testing.")


================================================
FILE: crawler/project_1111/task_urls_1111.py
================================================
# import os
# # python -m crawler.project_1111.task_urls_1111
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog

from typing import Set, List, Optional, Dict
import concurrent.futures

from crawler.worker import app
from crawler.database.connection import initialize_database
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    upsert_jobs,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.project_1111.client_1111 import fetch_job_urls_from_1111_api
from crawler.project_1111.parser_apidata_1111 import parse_job_list_json_to_pydantic
from crawler.config import (
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_1111.config_1111 import (
    URL_CRAWLER_ORDER_BY_1111,
)

# Define concurrency level for fetching pages
CONCURRENCY_LEVEL = 100 # Can be moved to config.py if needed

logger = structlog.get_logger(__name__)


def _fetch_and_parse_single_page(job_category_code: str, page_num: int) -> Optional[List[Dict]]:
    """
    Fetches job items for a single page from the 1111 API and parses them.
    Returns a list of job item dictionaries or None if an error occurs or no items are found.
    """
    api_response = fetch_job_urls_from_1111_api(
        KEYWORDS="",
        CATEGORY=job_category_code,
        ORDER=URL_CRAWLER_ORDER_BY_1111,
        PAGE_NUM=page_num,
    )

    if api_response is None:
        logger.error(
            "Failed to retrieve data from 1111 API for single page.",
            page=page_num,
            job_category_code=job_category_code,
        )
        return None

    job_items = api_response.get("result", {}).get("hits", [])
    if not isinstance(job_items, list):
        logger.error(
            "API response 'result.hits' format is incorrect or missing for single page.",
            page=page_num,
            job_category_code=job_category_code,
            api_data_type=type(job_items),
        )
        return None

    if not job_items:
        logger.debug(
            "No job items found on single page.",
            page=page_num,
            job_category_code=job_category_code,
        )
        return None
    
    return job_items


@app.task
def crawl_and_store_1111_category_urls(job_category: dict, url_limit: int = 0) -> int:
    """
    Celery task: Iterates through all pages of a specified 1111 job category, fetches job URLs
    and preliminary data, and stores them in the database in batches using concurrent fetching.
    """
    job_category = CategorySourcePydantic.model_validate(job_category)
    job_category_code = job_category.source_category_id
    
    global_job_url_set = set()
    current_batch_jobs = []
    current_batch_urls = []
    current_batch_url_categories = []

    logger.info(
        "Task started: crawling 1111 job category URLs and data.",
        job_category_code=job_category_code,
        url_limit=url_limit,
    )

    # Step 1: Fetch the first page synchronously to get totalPage
    first_page_job_items = _fetch_and_parse_single_page(job_category_code, 1)
    
    if first_page_job_items is None:
        logger.error(
            "Failed to fetch first page, cannot proceed with crawling.",
            job_category_code=job_category_code,
        )
        return 0

    # Get totalPage from the first page's API response
    # Re-fetch API response for page 1 to get pagination data
    api_response_first_page = fetch_job_urls_from_1111_api(
        KEYWORDS="",
        CATEGORY=job_category_code,
        ORDER=URL_CRAWLER_ORDER_BY_1111,
        PAGE_NUM=1,
    )
    total_pages = 1 # Default to 1 page if totalPage not found
    if api_response_first_page:
        pagination_data = api_response_first_page.get("result", {}).get("pagination", {})
        if "totalPage" in pagination_data:
            total_pages = pagination_data["totalPage"]
            logger.info(
                "Total pages discovered from API.",
                job_category_code=job_category_code,
                total_pages=total_pages,
            )
    
    # Process first page items
    for job_item in first_page_job_items:
        job_pydantic = parse_job_list_json_to_pydantic(job_item)
        if job_pydantic and job_pydantic.url:
            if job_pydantic.url not in global_job_url_set:
                global_job_url_set.add(job_pydantic.url)
                current_batch_jobs.append(job_pydantic)
                current_batch_urls.append(job_pydantic.url)
            
            current_batch_url_categories.append(
                UrlCategoryPydantic(
                    source_url=job_pydantic.url,
                    source_category_id=job_category_code,
                ).model_dump()
            )

    # Step 2: Use ThreadPoolExecutor for concurrent fetching of remaining pages
    pages_to_fetch = range(2, total_pages + 1) # Start from page 2
    if url_limit > 0:
        # Estimate how many pages are needed to reach url_limit
        # Assuming each page has URL_CRAWLER_UPLOAD_BATCH_SIZE items (approx)
        estimated_pages_for_limit = (url_limit - len(global_job_url_set)) // URL_CRAWLER_UPLOAD_BATCH_SIZE + 2
        pages_to_fetch = range(2, min(total_pages + 1, estimated_pages_for_limit))
        logger.info(
            "Adjusting pages to fetch based on URL limit.",
            job_category_code=job_category_code,
            url_limit=url_limit,
            estimated_pages=len(pages_to_fetch),
        )

    with concurrent.futures.ThreadPoolExecutor(max_workers=CONCURRENCY_LEVEL) as executor:
        future_to_page = {
            executor.submit(_fetch_and_parse_single_page, job_category_code, page_num): page_num
            for page_num in pages_to_fetch
            if (url_limit == 0 or len(global_job_url_set) < url_limit) # Only submit if limit not reached
        }

        for future in concurrent.futures.as_completed(future_to_page):
            page_num = future_to_page[future]
            try:
                job_items_on_page = future.result()
                if job_items_on_page:
                    logger.debug(
                        "Successfully fetched and parsed page.",
                        page=page_num,
                        job_category_code=job_category_code,
                        items_count=len(job_items_on_page),
                    )
                    for job_item in job_items_on_page:
                        job_pydantic = parse_job_list_json_to_pydantic(job_item)
                        if job_pydantic and job_pydantic.url:
                            if job_pydantic.url not in global_job_url_set:
                                global_job_url_set.add(job_pydantic.url)
                                current_batch_jobs.append(job_pydantic)
                                current_batch_urls.append(job_pydantic.url)
                            
                            current_batch_url_categories.append(
                                UrlCategoryPydantic(
                                    source_url=job_pydantic.url,
                                    source_category_id=job_category_code,
                                ).model_dump()
                            )
                
                # Check if batch size reached after processing each page's items
                if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
                    logger.info(
                        "Batch upload size reached. Starting data upload.",
                        count=len(current_batch_urls),
                        job_category_code=job_category_code,
                    )
                    upsert_jobs(current_batch_jobs)
                    upsert_urls(SourcePlatform.PLATFORM_1111, current_batch_urls)
                    upsert_url_categories(current_batch_url_categories)
                    
                    current_batch_jobs.clear()
                    current_batch_urls.clear()
                    current_batch_url_categories.clear()
                
                # Check URL limit again after processing a page
                if url_limit > 0 and len(global_job_url_set) >= url_limit:
                    logger.info(
                        "URL limit reached during concurrent fetching. Stopping further submissions.",
                        job_category_code=job_category_code,
                        url_limit=url_limit,
                        collected_urls=len(global_job_url_set),
                    )
                    # Cancel remaining futures if limit is reached
                    for remaining_future in future_to_page:
                        if not remaining_future.done():
                            remaining_future.cancel()
                    break # Break from the as_completed loop

            except concurrent.futures.CancelledError:
                logger.info("Future was cancelled due to URL limit being reached.", page=page_num)
            except Exception as exc:
                logger.error(
                    "Page generation failed.",
                    page=page_num,
                    job_category_code=job_category_code,
                    error=exc,
                    exc_info=True,
                )

    if current_batch_urls:
        logger.info(
            "Task completed. Storing remaining data to database.",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_jobs(current_batch_jobs)
        upsert_urls(SourcePlatform.PLATFORM_1111, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    else:
        logger.info(
            "Task completed. No new data collected, skipping database storage.",
            job_category_code=job_category_code,
        )

    logger.info("Task execution finished.", job_category_code=job_category_code, total_collected=len(global_job_url_set))
    return len(global_job_url_set)


if __name__ == "__main__":
    initialize_database()

    n_days = 7  # Define n_days for local testing
    url_limit = 100000

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_1111)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_1111)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_1111, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]


    if categories_to_dispatch:
        # categories_to_process_single = [categories_to_dispatch[0]]

        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_1111_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_1111_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")



================================================
FILE: crawler/project_cakeresume/Cake_me_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import random
import json
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import pandas as pd
from collections import deque
from concurrent.futures import ThreadPoolExecutor

WEB_NAME = 'Cake_me'
WEB_URL = "https://www.cake.me/job"

# 取得職業類別


# 1. 取得 JSON 資料
# jobcat 檔案名稱
file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
jobtt = requests.get(WEB_URL)
jobtt_soup = BeautifulSoup(jobtt.text, 'html.parser')
scripts = jobtt_soup.find_all('script')
jobcat_data = json.loads(scripts[-1].string)['props']['pageProps']['_nextI18Next']

# 2. 儲存 JSON 資料
with open(file_jobcat_json, "w", encoding="utf-8") as f:
    json.dump(jobcat_data, f, ensure_ascii=False, indent=4)
print(f"職業總覽資料已儲存為 {file_jobcat_json}")

df = pd.json_normalize(jobcat_data)

sector_prefix = 'initialI18nStore.zh-TW.sector.sectors'
sector_groups_prefix = 'initialI18nStore.zh-TW.sector.sector_groups.'
prefix = sector_prefix

filtered_df = (
            df.filter(like=prefix)
              .rename(columns=lambda col: col.replace(prefix, ''))
        )

filtered_df
# Output:
#   職業總覽資料已儲存為 Cake_me_jobcat_json.txt

#     .advertising-marketing-agency_adtech-martech  \

#   0                                  廣告技術 / 行銷技術   

#   

#     .advertising-marketing-agency_advertising  \

#   0                                        廣告   

#   

#     .advertising-marketing-agency_design .advertising-marketing-agency_digital  \

#   0                                   設計                                    數位   

#   

#     .advertising-marketing-agency_event-management  \

#   0                                           事件管理   

#   

#     .advertising-marketing-agency_marketing-communications  \

#   0                                            行銷 / 溝通       

#   

#     .advertising-marketing-agency_public-relations  \

#   0                                           公共關係   

#   

#     .agriculture_agricultural-technology .agriculture_dairy  \

#   0                                 農業科技           乳製品 / 酪農   

#   

#     .agriculture_farming  ... .tech_gambling-casinos .tech_games .tech_hardware  \

#   0                   農業  ...                博弈 / 賭場          遊戲             硬體   

#   

#     .tech_information-services .tech_internet .tech_mobile-apps .tech_robotics  \

#   0                       資訊服務           網際網路            手機應用程式          機器人科學   

#   

#     .tech_saas-cloud-services .tech_semiconductor .tech_software  

#   0               軟體即服務 / 雲服務                 半導體             軟體  

#   

#   [1 rows x 140 columns]

# 產生cake.me網址 https://www.cake.me 根據提供的(關鍵字和職缺類別) 轉換為職缺網址

def cake_me_url(KEYWORDS, CATEGORY, ORDER=None):
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (str, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。

    # 測試範例
    url_1 = cake_me_url("雲端工程師", "it", "latest")    
    # https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page=

    url_2 = cake_me_url("雲端工程師", "")      
    # https://www.cake.me/jobs/雲端工程師?page=

    url_3 = cake_me_url("", "it", "latest")             
    # https://www.cake.me/jobs/categories/it?order=latest&page=

    url_4 = cake_me_url("", "")               
    # https://www.cake.me/jobs?page=
    
    """

    BASE_URL = "https://www.cake.me/jobs"

    if KEYWORDS and CATEGORY:
        url = f"{BASE_URL}/{KEYWORDS}?profession[0]={CATEGORY}&page="
    elif KEYWORDS:
        url = f"{BASE_URL}/{KEYWORDS}?page="
    elif CATEGORY:
        url = f"{BASE_URL}/categories/{CATEGORY}?page="
    else:
        url = f"{BASE_URL}?page="

    if ORDER:  # 只在 ORDER 不為 None 時添加
        url = url.replace("?page=", f"?order={ORDER}&page=")

    return url


# # # 測試範例  類別: {軟體, it}
# url_1 = cake_me_url("雲端工程師", "it", "latest")    
# print(url_1)  # https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page=

# url_2 = cake_me_url("雲端工程師", "")      
# print(url_2)  # https://www.cake.me/jobs/雲端工程師?page=

# url_3 = cake_me_url("", "it", "latest")             
# print(url_3)  # https://www.cake.me/jobs/categories/it?order=latest&page=

# url_4 = cake_me_url("", "")               
# print(url_4)  # https://www.cake.me/jobs?page=

#  從指定的職缺網址獲取工作職缺的網址

def fetch_job_url(joburl):
    """
    這個函數會遍歷多個頁面，並從每個頁面中提取工作職缺的網址，將其存儲在一個集合中以避免重複。
    使用 tqdm 顯示進度條，並在每次請求之間隨機延遲以避免過於頻繁的請求。

    參數:
    joburl (str): 職缺列表的基本網址。
    PAGE (int): 起始頁碼。

    返回:
    list: 包含所有獲取到的工作職缺網址的列表。
    """

    PAGE = 0
    MAX_PAGE = 1

    MAX_LENGTH = 4
    recent_counts = deque(maxlen=MAX_LENGTH)

    job_url_set = set()  # 使用 set() 來存儲網址
    with tqdm(total=MAX_PAGE, desc="cake.me職缺列表 ", unit="PAGE", leave=True) as pbar:
        while True:
            # 獲取當前頁面內容的工作網址
            response = requests.get(f"{joburl}{PAGE}")
            response_soup = BeautifulSoup(response.text, 'html.parser')
            job_urls = response_soup.find_all('a', class_='JobSearchItem_jobTitle__bu6yO')
            for job_url in job_urls:
                job_url_set.add("https://www.cake.me" + job_url['href'])  # 添加到 set 中
            
            # 檢查是否有新資料
            total_jobs = len(job_url_set) 
            recent_counts.append(total_jobs)
            if len(recent_counts) == MAX_LENGTH and len(set(recent_counts)) == 1:
                print(f"連續{MAX_LENGTH}次沒有新資料，提前結束。")
                print(f"Total unique job URLs fetched: {len(job_url_set)}")
                break
               
            # 獲取總頁數
            time.sleep(random.uniform(0.5, 1.5))
            pagination_items = response_soup.find_all('a', class_='Pagination_itemNumber___enNq')
            if pagination_items:
                MAX_PAGE = int(pagination_items[-1].text) + 1
                pbar.total = MAX_PAGE  # 更新進度條的總頁數
            pbar.set_postfix_str(f"目前頁面 {PAGE}, 最大頁數: {MAX_PAGE}")
            pbar.update(1)

            if PAGE <= MAX_PAGE:  
                PAGE = PAGE + 1 
            else:
                break

        return list(job_url_set)  # 將 set 轉換為 list
    
    
# # 測試範例
# joburl = "https://www.cake.me/jobs/雲端工程師?order=latest&profession[0]=it&page="
# job_urls = fetch_job_url(joburl)
# job_urls[0]

# 從指定的職缺網址獲取職缺的相關數據

import requests
import pandas as pd
import json
from bs4 import BeautifulSoup

def clean_html_if_string(value):
    """
    輔助函數：只在輸入值為字串時，才清除 HTML 標籤。
    對於其他類型（數字、列表、None 等），直接返回原值。
    """
    if isinstance(value, str):
        # 使用 .get_text() 是從 HTML 中提取純文字的標準方法
        # separator=' ' 在標籤間插入空格，讓文字更自然
        # strip=True 移除開頭和結尾的空白
        return BeautifulSoup(value, "html.parser").get_text(separator=' ', strip=True)
    return value
    

def fetch_job_data(job_url: str) -> pd.DataFrame:
    """
    這個函數會發送 GET 請求到提供的職缺網址，並使用 BeautifulSoup 解析返回的 HTML 文檔。
    它會從頁面中的 JavaScript 代碼中提取職缺的元數據，將其轉換為 Pandas DataFrame，
    並清除特定欄位中的 HTML 標籤。

    參數:
    job_url (str): 職缺的網址。

    返回:
    pd.DataFrame: 包含職缺詳細信息的 DataFrame。如果發生錯誤，則返回一個空的 DataFrame。
    """
    try:
        time.sleep(random.uniform(0.3, 0.8)) 
        
        # 加上 User-Agent，讓請求看起來更像來自瀏覽器
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(job_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        response_soup = BeautifulSoup(response.text, 'html.parser')
        
        data_script = response_soup.find('script', id='__NEXT_DATA__')
        
        if not data_script:
            print("錯誤：在頁面中找不到職缺資料 (script#__NEXT_DATA__) 。")
            return pd.DataFrame()

        page_props = json.loads(data_script.string)['props']['pageProps']

        # 前一版本
        # jobMetaData = json.loads(scripts[-1].string)['props']['pageProps']['ssr']['jobMetaData']['job']
        # df = pd.json_normalize(jobMetaData)
        # # 公司名稱、網址、其他職缺、目前職缺網址
        # office_name = json.loads(scripts[1].string)['itemListElement'][0]['item']['name']
        # office_url = json.loads(scripts[1].string)['itemListElement'][0]['item']['@id']
        # job_other = json.loads(scripts[1].string)['itemListElement'][1]['item']['@id']
        # job_url = json.loads(scripts[1].string)['itemListElement'][2]['item']['@id']
        # df['job_url'] = job_url
        # df['office_name'] = office_name
        # df['office_url'] = office_url
        # df['job_other'] = job_other


        # 在這裡，我們直接從 pageProps 提取 'job' 的資料，而不是 'company'
        # 因為職缺頁面的核心是職缺本身，它內部已經包含了公司資訊
        job_details = page_props.get('job')
        
        if not job_details:
            print("錯誤：無法從 JSON 中解析出職缺資料 ('job' key not found)。")
            return pd.DataFrame()
        
        df = pd.json_normalize([job_details])

        columns_to_clean = [
            'description',
            'job_responsibilities',
            'job_requirements',
            'requirements'
            'job_preferred_requirements',
            'company.description'
        ]

        # 2. 只對存在於 DataFrame 且需要清洗的欄位進行操作
        for col in columns_to_clean:
            if col in df.columns:
        # for col in df.columns:
        #     if col in df.columns and df[col].dtype == 'object':
                df[col] = df[col].apply(clean_html_if_string)
        
        return df

    except requests.exceptions.RequestException as e:
        print(f"網路請求失敗: {e}")
        return pd.DataFrame()
    except (KeyError, TypeError, json.JSONDecodeError) as e:
        print(f"解析頁面資料失敗，頁面結構可能已變更: {e}")
        return pd.DataFrame()


# # --- 測試範例 ---
# # 使用您的範例網址
# job_url = 'https://www.cakeresume.com/companies/qdm/jobs/2f3db0'
# job_data = fetch_job_data(job_url)
# job_data

# 根據關鍵字與職業類別 獲取所有工作職位的資料

SEARCH_TIMESTAMP = time.strftime('%Y-%m-%d', time.localtime(time.time()))
KEYWORDS = "雲端工程師"
CATAGORY = "it"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{CATAGORY}"

print( f"開始執行 {FILE_NAME}" )
job_data_list = []
SEARCH_PAGE_URL = cake_me_url(KEYWORDS, CATAGORY)       # 產生cake.me網址
job_urls = fetch_job_url(SEARCH_PAGE_URL)               # 列出職缺列表
with ThreadPoolExecutor(max_workers=3) as executor:    # 列出職缺細項
    futures = [executor.submit(fetch_job_data, url) for url in job_urls]
    for future in tqdm(futures):
        result = future.result()
        if result is not None and not result.empty:  # 確保結果不為空
            job_data_list.append(result)

all_jobs_df = pd.concat(job_data_list, axis=0)  
all_jobs_df.reset_index(drop=True, inplace=True)
all_jobs_df['job_url'] = 'https://www.cake.me/companies/geovision/jobs/' + all_jobs_df['path']

all_jobs_df.shape

all_jobs_df['job_url'] = 'https://www.cake.me/companies/geovision/jobs/' + all_jobs_df['path']
all_jobs_df

# merged_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
all_jobs_df.head(1)

all_jobs_df.columns

column_names = [
    {"序號": 1, "英文": "path", "中文": "職缺頁面路徑"},
    {"序號": 2, "英文": "title", "中文": "職缺標題"},
    {"序號": 3, "英文": "profession_v2", "中文": "職業類別"},
    {"序號": 4, "英文": "job_type", "中文": "工作類型"}, # 例如: 全職, 兼職
    {"序號": 5, "英文": "seniority_level", "中文": "資歷級別"}, # 例如: Entry, Mid, Senior
    {"序號": 6, "英文": "min_work_exp_year", "中文": "最少工作經驗年資"},
    {"序號": 7, "英文": "locations", "中文": "工作地點"},
    {"序號": 8, "英文": "remote", "中文": "是否可遠端工作"},
    {"序號": 9, "英文": "number_of_management", "中文": "管理職責人數"},
    {"序號": 10, "英文": "number_of_openings", "中文": "招聘人數"},
    {"序號": 11, "英文": "salary_min", "中文": "最低薪資"},
    {"序號": 12, "英文": "salary_max", "中文": "最高薪資"},
    {"序號": 13, "英文": "salary_currency", "中文": "薪資貨幣"}, # 例如: TWD, USD
    {"序號": 14, "英文": "salary_type", "中文": "薪資類型"}, # 例如: 月薪, 年薪
    {"序號": 15, "英文": "inclusivity_traits", "中文": "多元共融特質"},
    {"序號": 16, "英文": "tags", "中文": "技能標籤"},
    {"序號": 17, "英文": "description", "中文": "職缺描述"},
    {"序號": 18, "英文": "requirements", "中文": "應徵條件"},
    {"序號": 19, "英文": "category", "中文": "職缺類別(總)"},
    {"序號": 20, "英文": "hide_salary_completely", "中文": "完全隱藏薪資"},
    {"序號": 21, "英文": "hide_salary_max", "中文": "隱藏最高薪資"},
    {"序號": 22, "英文": "year_of_seniority", "中文": "資歷年限(另一種)"},
    {"序號": 23, "英文": "lang", "中文": "刊登語言"},
    {"序號": 24, "英文": "signing_bonus", "中文": "簽約獎金"},
    {"序號": 25, "英文": "unique_impression_count", "中文": "不重複瀏覽次數"},
    {"序號": 26, "英文": "cached_votes_up", "中文": "讚數(快取)"},
    {"序號": 27, "英文": "content_updated_at", "中文": "內容更新時間"},
    {"序號": 28, "英文": "sponsored", "中文": "是否為贊助職缺"},
    {"序號": 29, "英文": "external_url", "中文": "外部應徵連結"},
    {"序號": 30, "英文": "impression_count", "中文": "總瀏覽次數"},
    {"序號": 31, "英文": "interview_process", "中文": "面試流程"},
    {"序號": 32, "英文": "created_at", "中文": "建立時間"},
    {"序號": 33, "英文": "updated_at", "中文": "更新時間"},
    {"序號": 34, "英文": "liked", "中文": "是否已收藏"}, # 使用者個人狀態
    {"序號": 35, "英文": "job_recruiters", "中文": "負責招募者"},
    {"序號": 36, "英文": "job_questions", "中文": "應徵問題"},
    {"序號": 37, "英文": "noindex", "中文": "禁止搜尋引擎索引"},
    {"序號": 38, "英文": "processed_description", "中文": "處理過的職缺描述"},
    {"序號": 39, "英文": "impression_token", "中文": "瀏覽令牌"}, # 技術性欄位
    {"序號": 40, "英文": "common_applied_jobs", "中文": "常見相關應徵職缺"},
    {"序號": 41, "英文": "aasm_state", "中文": "職缺狀態"}, # 技術性欄位 (例如: published, archived)
    {"序號": 42, "英文": "metadata.images", "中文": "相關圖片"},
]

df_new = pd.json_normalize(column_names)
df_new.columns = ["序號", "cake.me_英文", "cake.me_中文"]
df_new

#  前一版本
# column_names = [
#     {"序號": 1, "英文": "name", "中文": "公司名稱"},
#     {"序號": 2, "英文": "description", "中文": "公司簡介"},
#     {"序號": 3, "英文": "path", "中文": "公司頁面路徑"},
#     {"序號": 4, "英文": "unique_impression_count", "中文": "不重複曝光次數"},
#     {"序號": 5, "英文": "address", "中文": "公司地址"},
#     {"序號": 6, "英文": "country", "中文": "國家"},
#     {"序號": 7, "英文": "contact_name", "中文": "聯絡人姓名"},
#     {"序號": 8, "英文": "contact_phone", "中文": "聯絡電話"},
#     {"序號": 9, "英文": "aasm_state", "中文": "工作流程狀態"},
#     {"序號": 10, "英文": "content_updated_at", "中文": "內容更新時間"},
#     {"序號": 11, "英文": "twitter_handle", "中文": "Twitter 帳號"},
#     {"序號": 12, "英文": "email", "中文": "電子郵件"},
#     {"序號": 13, "英文": "phone", "中文": "電話"},
#     {"序號": 14, "英文": "work_environment", "中文": "工作環境"},
#     {"序號": 15, "英文": "employee_benefits", "中文": "員工福利"},
#     {"序號": 16, "英文": "products_or_services", "中文": "產品或服務"},
#     {"序號": 17, "英文": "mission", "中文": "公司使命"},
#     {"序號": 18, "英文": "media_coverage", "中文": "媒體報導"},
#     {"序號": 19, "英文": "cover_image", "中文": "封面圖片"},
#     {"序號": 20, "英文": "youtube_video_url", "中文": "YouTube 影片網址"},
#     {"序號": 21, "英文": "recruiter_contact_info", "中文": "招募人員聯絡資訊"},
#     {"序號": 22, "英文": "recruiting_website_url", "中文": "招募網站網址"},
#     {"序號": 23, "英文": "founded_year", "中文": "創立年份"},
#     {"序號": 24, "英文": "created_at", "中文": "建立時間"},
#     {"序號": 25, "英文": "updated_at", "中文": "更新時間"},
#     {"序號": 26, "英文": "company", "中文": "公司"},
#     {"序號": 27, "英文": "website_url", "中文": "公司網站網址"},
#     {"序號": 28, "英文": "geo_city", "中文": "地理位置-城市"},
#     {"序號": 29, "英文": "geo_formatted_address", "中文": "地理位置-格式化地址"},
#     {"序號": 30, "英文": "geo_state_code", "中文": "地理位置-州/省代碼"},
#     {"序號": 31, "英文": "geo_country_code", "中文": "地理位置-國家代碼"},
#     {"序號": 32, "英文": "geo_zip", "中文": "地理位置-郵遞區號"},
#     {"序號": 33, "英文": "featured", "中文": "精選狀態"},
#     {"序號": 34, "英文": "geo_state_name", "中文": "地理位置-州/省名稱"},
#     {"序號": 35, "英文": "geo_city_l", "中文": "地理位置-城市 (本地化)"},
#     {"序號": 36, "英文": "geo_formatted_address_l", "中文": "地理位置-格式化地址 (本地化)"},
#     {"序號": 37, "英文": "geo_state_name_l", "中文": "地理位置-州/省名稱 (本地化)"},
#     {"序號": 38, "英文": "geo_street_address_l", "中文": "地理位置-街道地址 (本地化)"},
#     {"序號": 39, "英文": "geo_l_locale", "中文": "地理位置-本地化語系"},
#     {"序號": 40, "英文": "sector", "中文": "產業類別"},
#     {"序號": 41, "英文": "facebook_url", "中文": "Facebook 網址"},
#     {"序號": 42, "英文": "linkedin_url", "中文": "LinkedIn 網址"},
#     {"序號": 43, "英文": "instagram_url", "中文": "Instagram 網址"},
#     {"序號": 44, "英文": "medium_url", "中文": "Medium 網址"},
#     {"序號": 45, "英文": "whatsapp_id", "中文": "WhatsApp ID"},
#     {"序號": 46, "英文": "geo_country_l", "中文": "地理位置-國家 (本地化)"},
#     {"序號": 47, "英文": "application_read_rate", "中文": "應徵讀取率"},
#     {"序號": 48, "英文": "application_read_time", "中文": "應徵讀取時間"},
#     {"序號": 49, "英文": "candidate_read_rate", "中文": "應徵者資料讀取率"},
#     {"序號": 50, "英文": "candidate_read_time", "中文": "應徵者資料讀取時間"},
#     {"序號": 51, "英文": "published_at", "中文": "發布時間"},
#     {"序號": 52, "英文": "logo", "中文": "公司 Logo"},
#     {"序號": 53, "英文": "company_overview", "中文": "公司概覽"},
#     {"序號": 54, "英文": "amount_of_capital", "中文": "資本額"},
#     {"序號": 55, "英文": "number_of_employees", "中文": "員工人數"},
#     {"序號": 56, "英文": "og_image", "中文": "OG 圖片 (社群分享預覽圖)"},
#     {"序號": 57, "英文": "ga_tracking_code", "中文": "Google Analytics 追蹤碼"},
#     {"序號": 58, "英文": "tax_id_number", "中文": "統一編號 / 稅務識別碼"},
#     {"序號": 59, "英文": "last_active_at", "中文": "最後活躍時間"},
#     {"序號": 60, "英文": "labels", "中文": "標籤"},
#     {"序號": 61, "英文": "teches", "中文": "使用技術"},
#     {"序號": 62, "英文": "faq_items", "中文": "常見問題項目"},
#     {"序號": 63, "英文": "work_environment_images", "中文": "工作環境圖片"},
#     {"序號": 64, "英文": "currency_symbol", "中文": "貨幣符號"},
#     {"序號": 65, "英文": "followed", "中文": "已追蹤"},
#     {"序號": 66, "英文": "followed_job_notification_type", "中文": "追蹤職缺通知類型"},
#     {"序號": 67, "英文": "total_followers", "中文": "總追蹤人數"},
#     {"序號": 68, "英文": "currency_code", "中文": "貨幣代碼"},
#     {"序號": 69, "英文": "seems_spam", "中文": "疑似垃圾訊息"},
#     {"序號": 70, "英文": "noindex", "中文": "禁止搜尋引擎索引"},
#     {"序號": 71, "英文": "impression_token", "中文": "曝光權杖 (Token)"},
#     {"序號": 72, "英文": "profession_job_counts", "中文": "各職業類別職缺數"},
#     {"序號": 73, "英文": "listed_job_count", "中文": "上架中職缺數"},
#     {"序號": 74, "英文": "sanitized_description", "中文": "純文字(淨化後)的公司簡介"},
#     {"序號": 75, "英文": "sanitized_products_or_services", "中文": "純文字(淨化後)的產品或服務"},
#     {"序號": 76, "英文": "sanitized_mission", "中文": "純文字(淨化後)的公司使命"},
#     {"序號": 77, "英文": "sanitized_media_coverage", "中文": "純文字(淨化後)的媒體報導"},
#     {"序號": 78, "英文": "sanitized_employee_benefits", "中文": "純文字(淨化後)的員工福利"},
#     {"序號": 79, "英文": "sanitized_work_environment", "中文": "純文字(淨化後)的工作環境"}
# ]

# df_new = pd.json_normalize(column_names)
# df_new.columns = ["序號", "cake.me_英文", "cake.me_中文"]
# df_new

# 補充 :  網頁結構解析

url = "https://www.cake.me/jobs/雲端工程師"

response = requests.get(url)
response_soup = BeautifulSoup(response.text, 'html.parser')
scripts = response_soup.find_all('script')


# 查看篩選欄位選項
# options = []
# catagorys = 1
# DropdownButton_contents = response_soup.find_all('div', 'JobSearchPage_searchFilter__ts_A0')
# for JobSearchPage_searchFilter in DropdownButton_contents:
#     title  = JobSearchPage_searchFilter.find('div', 'DropdownButton_content__XZwFf').text
#     # print(title)
#     Checkbox_texts = JobSearchPage_searchFilter.find_all('div', 'Checkbox_text__g6TLq')
#     for Checkbox in Checkbox_texts:
#         # print(Checkbox.text)
#         option_text = Checkbox.text.strip()
#         options.append({'catagorys':catagorys, 'title': title, 'option_text': option_text})
#     catagorys = catagorys +1
# df_searchFilter = pd.DataFrame(options)
# df_searchFilter
# df_searchFilter['option_text'][(df_searchFilter['catagorys']==1)]



# 提取資料結構 使用遞歸函數提取子結構
def extract_keys_with_branch_structure(value_in, current_path=''):
    
    branch_structure = {}
    if isinstance(value_in, dict):
        for index, (key, value) in enumerate(value_in.items()):
            path = f"{current_path}-{index + 1}" if current_path else f"{index + 1}"
            branch_structure[path] = key  # 添加當前層級的鍵

            # path = f"{current_path}-{key}" if current_path else f"{key}"
            # branch_structure[path] = value  # 添加當前層級的鍵
            
            # 使用遞歸查找 子結構、提取與合併
            if isinstance(value, dict):
                branch_structure.update(extract_keys_with_branch_structure(value, path))

    return branch_structure

data = json.loads(scripts[-1].string)
branch_structure = extract_keys_with_branch_structure(data)

json.loads(scripts[-1].string)['props']['pageProps']['serverState']['initialResults']['Job']['state']['query']




================================================
FILE: crawler/project_cakeresume/client_cakeresume.py
================================================
import json
import random
import time
from typing import Any, Dict, Optional

import requests
import structlog
from bs4 import BeautifulSoup
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import (
    RetryCallState,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
from crawler.logging_config import configure_logging
from crawler.project_cakeresume.config_cakeresume import (
    HEADERS_CAKERESUME,
    JOB_CAT_URL_CAKERESUME,
    JOB_LISTING_BASE_URL_CAKERESUME,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


def log_before_retry(retry_state: "RetryCallState") -> None:
    """Log before retrying a request, showing attempt number and wait time."""
    logger.warning(
        "Request failed, retrying...",
        attempt=retry_state.attempt_number,
        wait_seconds=retry_state.next_action.sleep,
        error=retry_state.outcome.exception(),
    )


@retry(
    stop=stop_after_attempt(7),
    wait=wait_exponential(multiplier=1, min=4, max=30),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    before_sleep=log_before_retry,
    reraise=True,
)
def _make_web_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[str]:  # Return HTML content as string
    """
    通用的網頁請求函式，處理隨機延遲、請求發送、和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before web request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        return response.text
    except requests.exceptions.RequestException as e:
        # If it's a 404 error, it might mean the category has no jobs.
        # Log a warning and return None so the caller can handle it without retrying.
        if (
            isinstance(e, requests.exceptions.HTTPError)
            and e.response.status_code == 404
        ):
            logger.warning(
                "HTTP 404 Not Found for URL. This might indicate an empty category.",
                url=url,
                **log_context,
            )
            return None

        # For other network errors, log an error and re-raise to trigger tenacity retry.
        logger.error(
            "Network error during web request.",
            error=e,
            exc_info=True,
            **log_context,
        )
        raise  # Re-raise the exception to trigger tenacity retry
    except Exception as e:
        logger.error(
            "Unexpected error during web request.",
            url=url,
            error=e,
            exc_info=True,
            **log_context,
        )
        return None

def fetch_cakeresume_category_page_html(
    url: str = JOB_CAT_URL_CAKERESUME, headers: Dict[str, str] = HEADERS_CAKERESUME
) -> Optional[str]:
    """
    從 CakeResume 獲取職務分類頁面的原始 HTML 內容。
    """
    return _make_web_request("GET", url, headers=headers, log_context={"api_type": "cakeresume_category_page_html"})

def extract_next_data_json_from_html(html_content: str) -> Optional[Dict[str, Any]]:
    """
    從 HTML 內容中提取 __NEXT_DATA__ script 標籤的 JSON 內容。
    """
    if not html_content:
        return None
    
    soup = BeautifulSoup(html_content, 'html.parser')
    data_script = soup.find('script', id='__NEXT_DATA__')

    if data_script and data_script.string:
        try:
            return json.loads(data_script.string)
        except (json.JSONDecodeError, KeyError) as e:
            logger.error("Failed to parse JSON data from __NEXT_DATA__ script.", error=e, exc_info=True)
            return None
    return None

def fetch_cakeresume_category_data(
    url: str = JOB_CAT_URL_CAKERESUME, headers: Dict[str, str] = HEADERS_CAKERESUME
) -> Optional[Dict[str, Any]]:
    """
    從 CakeResume 獲取職務分類的原始數據 (主要為 sector 資料)。
    """
    html_content = fetch_cakeresume_category_page_html(url, headers)
    if html_content:
        data = extract_next_data_json_from_html(html_content)
        if data:
            # The relevant data is nested under props.pageProps._nextI18Next.initialI18nStore.zh-TW.sector
            i18n_store_zh_tw_sector = data.get('props', {}).get('pageProps', {}).get('_nextI18Next', {}).get('initialI18nStore', {}).get('zh-TW', {}).get('sector', {})
            return {'initialI18nStore': {'zh-TW': {'sector': i18n_store_zh_tw_sector}}}
    return None

def cake_me_url(KEYWORDS: str, CATEGORY: str, ORDER: Optional[str] = None) -> str:
    """
    這個函數會根據給定的關鍵字和類別參數構建一個完整的職缺網址。
    如果同時提供了關鍵字和類別，將會包含兩者；如果只提供其中一個，則只會包含該參數。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別。
    ORDER (str, optional): 排序的參數，預設為 None。

    返回:
    str: 生成的職缺網址。
    """

    BASE_URL = JOB_LISTING_BASE_URL_CAKERESUME
    logger.debug("Using BASE_URL for CakeResume job listing", base_url=BASE_URL)

    if KEYWORDS and CATEGORY:
        url = f"{BASE_URL}/{KEYWORDS}?profession[0]={CATEGORY}&page="
    elif KEYWORDS:
        url = f"{BASE_URL}/{KEYWORDS}?page="
    elif CATEGORY:
        url = f"{BASE_URL}/categories/{CATEGORY}?page="
    else:
        url = f"{BASE_URL}?page="

    if ORDER:  # 只在 ORDER 不為 None 時添加
        url = url.replace("?page=", f"?order={ORDER}&page=")

    return url

def fetch_cakeresume_job_urls(
    KEYWORDS: str,
    CATEGORY: str,
    ORDER: Optional[str] = None,
    PAGE_NUM: int = 0,
) -> Optional[str]: # Returns HTML content of the job listing page
    """
    從 CakeResume 獲取職缺 URL 列表的原始數據 (HTML 內容)。
    """
    url = cake_me_url(KEYWORDS, CATEGORY, ORDER) + str(PAGE_NUM)
    logger.debug("Final URL for CakeResume job listing", final_url=url)
    return _make_web_request(
        "GET",
        url,
        headers=HEADERS_CAKERESUME,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "cakeresume_job_urls",
            "keywords": KEYWORDS,
            "category": CATEGORY,
            "page": PAGE_NUM,
        },
    )

def fetch_cakeresume_job_data(job_url: str) -> Optional[str]: # Returns HTML content of the job detail page
    """
    從 CakeResume 職缺頁面抓取單一 URL 的資料 (HTML 內容)。
    """
    return _make_web_request(
        "GET",
        job_url,
        headers=HEADERS_CAKERESUME,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "cakeresume_job_detail",
            "url": job_url,
        },
    )




================================================
FILE: crawler/project_cakeresume/config_cakeresume.py
================================================
# crawler/project_cakeresume/config_cakeresume.py
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# CakeResume 平台相關設定
WEB_NAME_CAKERESUME = config_section.get("WEB_NAME_CAKERESUME", "CakeResume")
JOB_CAT_URL_CAKERESUME = config_section.get("JOB_CAT_URL_CAKERESUME", "https://www.cake.me/jobs")
JOB_LISTING_BASE_URL_CAKERESUME = config_section.get("JOB_LISTING_BASE_URL_CAKERESUME", "https://www.cake.me/jobs")
logger.info("JOB_LISTING_BASE_URL_CAKERESUME loaded", url=JOB_LISTING_BASE_URL_CAKERESUME)
JOB_DETAIL_BASE_URL_CAKERESUME = config_section.get("JOB_DETAIL_BASE_URL_CAKERESUME", "https://www.cake.me") # Job URLs are constructed as base + path

HEADERS_CAKERESUME = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.cake.me",
}

URL_CRAWLER_PAGE_SIZE_CAKERESUME = int(config_section.get("URL_CRAWLER_PAGE_SIZE_CAKERESUME", "20")) # Not directly used by CakeResume, but for consistency
URL_CRAWLER_ORDER_BY_CAKERESUME = config_section.get("URL_CRAWLER_ORDER_BY_CAKERESUME", "latest") # "latest" or other options



================================================
FILE: crawler/project_cakeresume/parser_cakeresume.py
================================================
# crawler/project_cakeresume/parser_cakeresume.py
"""
Parsers for Cakeresume, handling data transformation from the __NEXT_DATA__ script tag.
"""
import structlog
from datetime import datetime
from typing import Optional, Dict, Any
from urllib.parse import urljoin
import re
from bs4 import BeautifulSoup

from crawler.database.schemas import JobPydantic, SourcePlatform, JobStatus, SalaryType, JobType
from crawler.utils.clean_text import clean_text

logger = structlog.get_logger(__name__)

def _parse_cakeresume_salary(
    job_details: Dict[str, Any]
) -> tuple[Optional[int], Optional[int], Optional[SalaryType], Optional[str]]:
    """Parses salary information from Cakeresume's structured data."""
    salary_min_raw = job_details.get("salary_min")
    salary_max_raw = job_details.get("salary_max")
    salary_type_raw = job_details.get("salary_type")
    salary_currency = job_details.get("salary_currency")

    salary_min = int(salary_min_raw) if isinstance(salary_min_raw, (int, float)) else None
    salary_max = int(salary_max_raw) if isinstance(salary_max_raw, (int, float)) else None

    salary_type_map = {
        'per_month': SalaryType.MONTHLY,
        'per_year': SalaryType.YEARLY,
        'per_hour': SalaryType.HOURLY,
        'per_day': SalaryType.DAILY,
        'piece_rate_pay': SalaryType.BY_CASE,
    }
    salary_type = salary_type_map.get(str(salary_type_raw)) if salary_type_raw else None

    salary_text = None
    if salary_min is not None and salary_max is not None and salary_type and salary_currency:
        if salary_min == 0 and salary_max == 0:
            salary_text = "面議"
            salary_type = SalaryType.NEGOTIABLE
        elif salary_min == salary_max:
            salary_text = f"{salary_currency} {salary_min:,}"
        else:
            salary_text = f"{salary_currency} {salary_min:,} ~ {salary_max:,}"
        
        if salary_type != SalaryType.NEGOTIABLE:
            type_text_map = {
                SalaryType.MONTHLY: " / 月",
                SalaryType.YEARLY: " / 年",
                SalaryType.HOURLY: " / 時",
                SalaryType.DAILY: " / 日",
            }
            salary_text += type_text_map.get(salary_type, "")
    elif job_details.get("hide_salary_completely") or "面議" in str(job_details):
        salary_text = "面議"
        salary_type = SalaryType.NEGOTIABLE

    return salary_min, salary_max, salary_type, salary_text

def _parse_job_type(job_details: Dict[str, Any]) -> JobType:
    """Maps job type text to JobType enum."""
    job_type_raw = job_details.get("job_type")
    job_type_map = {
        "full_time": JobType.FULL_TIME,
        "part_time": JobType.PART_TIME,
        "contract": JobType.CONTRACT,
        "internship": JobType.INTERNSHIP,
        "temporary": JobType.TEMPORARY,
        "freelance": JobType.CONTRACT,
    }
    return job_type_map.get(str(job_type_raw), JobType.OTHER)

def parse_job_details_to_pydantic(job_details: Dict[str, Any], html_content: str, url: str) -> Optional[JobPydantic]:
    """
    Parses the job data extracted from the __NEXT_DATA__ script tag into a JobPydantic object.
    """
    try:
        source_job_id = str(job_details.get("path"))
        if not source_job_id:
            match = re.search(r'jobs/([a-zA-Z0-9_-]+)', url)
            if match:
                source_job_id = match.group(1)
            else:
                logger.warning("Could not determine source_job_id from JSON or URL.", url=url)
                return None

        company_data = job_details.get("company", {})
        company_name = company_data.get("name")
        company_path = company_data.get("path")
        company_url = f"https://www.cakeresume.com/companies/{company_path}" if company_path else None

        description_html = job_details.get("description", "")
        description_plain = job_details.get("description_plain_text", "")
        description = clean_text(description_html) or clean_text(description_plain)

        soup = BeautifulSoup(html_content, "html.parser")

        if not company_name:
            company_tag = soup.select_one(".JobDescriptionLeftColumn_name__ABAp9")
            if company_tag:
                company_name = company_tag.get_text(strip=True)
        
        if not company_url:
            company_link = soup.select_one("a.JobDescriptionLeftColumn_name__ABAp9")
            if company_link and company_link.has_attr('href'):
                company_url = urljoin("https://www.cakeresume.com", company_link['href'])
        
        if not company_path:
            if company_url:
                company_path = company_url.split("/companies/")[-1]

        description_html = job_details.get("description", "")
        location_tags = soup.select("div.JobDescriptionRightColumn_locationsWrapper__N_fz_ a")
        location_text = ", ".join([clean_text(tag.get_text()) for tag in location_tags]) if location_tags else None

        posted_at_raw = job_details.get("content_updated_at")
        posted_at = None
        if posted_at_raw:
            try:
                posted_at = datetime.fromisoformat(str(posted_at_raw).replace("Z", "+00:00"))
            except (ValueError, AttributeError):
                logger.warning("Failed to parse 'content_updated_at' date.", value=posted_at_raw)

        salary_min, salary_max, salary_type, salary_text = _parse_cakeresume_salary(job_details)

        if not salary_text:
            salary_tag = soup.select_one(".JobDescriptionRightColumn_salaryWrapper__Q_8IL span")
            if salary_tag:
                salary_text = salary_tag.get_text(strip=True)
                # Attempt to parse min and max from the scraped text
                numbers = [int(s) for s in re.findall(r'\d+', salary_text.replace(",", ""))]
                if len(numbers) == 2:
                    salary_min, salary_max = numbers
                elif len(numbers) == 1:
                    salary_min = numbers[0]
                if "月" in salary_text:
                    salary_type = SalaryType.MONTHLY

        min_exp_year = job_details.get("min_work_exp_year")
        experience_required_text = f"{int(min_exp_year)} 年以上" if isinstance(min_exp_year, int) and min_exp_year > 0 else "不拘"

        requirements_text = job_details.get("requirements_plain_text", "")
        edu_match = re.search(r'(高中|專科|大學|碩士|博士)', requirements_text)
        education_required_text = edu_match.group(1) if edu_match else "不拘"

        return JobPydantic(
            source_platform=SourcePlatform.PLATFORM_CAKERESUME,
            source_job_id=source_job_id,
            url=url,
            status=JobStatus.ACTIVE,
            title=clean_text(job_details.get("title")),
            description=description,
            job_type=_parse_job_type(job_details),
            location_text=location_text,
            posted_at=posted_at,
            salary_text=salary_text,
            salary_min=salary_min,
            salary_max=salary_max,
            salary_type=salary_type,
            experience_required_text=experience_required_text,
            education_required_text=education_required_text,
            company_source_id=company_path,
            company_name=clean_text(company_name),
            company_url=company_url,
        )

    except Exception as e:
        logger.error("Failed to parse Cakeresume script JSON.", url=url, error=e, exc_info=True)
        return None



================================================
FILE: crawler/project_cakeresume/producer_category_cakeresume.py
================================================
from crawler.project_cakeresume.task_category_cakeresume import fetch_and_sync_cakeresume_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_cakeresume.config_cakeresume import JOB_CAT_URL_CAKERESUME

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_cakeresume_categories.s(JOB_CAT_URL_CAKERESUME).apply_async(queue='producer_category_cakeresume')
logger.info("send task_category_cakeresume url", url=JOB_CAT_URL_CAKERESUME, queue='producer_category_cakeresume')


================================================
FILE: crawler/project_cakeresume/producer_jobs_cakeresume.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_cakeresume.task_jobs_cakeresume import fetch_url_data_cakeresume
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_cakeresume_job_urls():
    """
    從資料庫讀取待處理或失敗的 CakeResume 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 CakeResume 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_CAKERESUME,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 CakeResume 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 CakeResume URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 CakeResume URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_cakeresume.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_cakeresume")

        logger.info(
            "已成功分發一批 CakeResume 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_cakeresume"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_cakeresume/producer_urls_cakeresume.py
================================================
from crawler.database.repository import get_all_categories_for_platform
from crawler.project_cakeresume.task_urls_cakeresume import crawl_and_store_cakeresume_category_urls
from crawler.database.models import SourcePlatform
import structlog

from crawler.logging_config import configure_logging

configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Starting URL task distribution for all CakeResume categories.")

all_cakeresume_categories = get_all_categories_for_platform(SourcePlatform.PLATFORM_CAKERESUME)

if all_cakeresume_categories:
    logger.info("Found categories for PLATFORM_CAKERESUME.", count=len(all_cakeresume_categories))
    for category_info in all_cakeresume_categories:
        category_id: str = category_info.source_category_id
        logger.info("分發 URL 抓取任務", category_id=category_id)
        crawl_and_store_cakeresume_category_urls.delay(category_info.model_dump())
else:
    logger.info("No categories found for PLATFORM_CAKERESUME.")


================================================
FILE: crawler/project_cakeresume/task_category_cakeresume.py
================================================
# import os
# # python -m crawler.project_cakeresume.task_category_cakeresume
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import structlog
import json
from typing import List, Dict, Any
from bs4 import BeautifulSoup

from crawler.database import connection as db_connection
from crawler.database import repository
from crawler.database.connection import initialize_database
from crawler.database.schemas import SourcePlatform
from crawler.project_cakeresume.client_cakeresume import (
    fetch_cakeresume_category_page_html,
)
from crawler.project_cakeresume.config_cakeresume import JOB_CAT_URL_CAKERESUME
from crawler.worker import app
from crawler.database.category_classification_data.apply_classification import apply_category_classification

logger = structlog.get_logger(__name__)


def parse_next_data_for_i18n_categories(html_content: str) -> List[Dict[str, Any]]:
    """
    Finds the __NEXT_DATA__ script tag, parses its JSON content,
    and extracts the hierarchical category data from the i18n (internationalization) object.
    This is the most reliable method.
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    next_data_script = soup.find('script', id='__NEXT_DATA__')
    
    if not next_data_script or not hasattr(next_data_script, 'string') or not next_data_script.string:
        raise ValueError("Could not find __NEXT_DATA__ script tag or it is empty in the HTML.")

    try:
        data = json.loads(next_data_script.string)
    except json.JSONDecodeError:
        raise ValueError("Failed to parse JSON from __NEXT_DATA__ script tag.")

    try:
        # The definitive path to the translation data
        i18n_data = data['props']['pageProps']['_nextI18Next']['initialI18nStore']['zh-TW']['profession']
    except KeyError as e:
        raise ValueError(f"Unexpected JSON structure in __NEXT_DATA__. Missing key: {e}")

    flat_list = []
    parent_map = {}

    # First pass: Get all parent categories (e.g., "profession_groups.it": "軟體")
    for key, value in i18n_data.items():
        if key.startswith("profession_groups."):
            parent_id = key.replace("profession_groups.", "")
            parent_name = value
            parent_map[parent_id] = parent_name
            flat_list.append({
                "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
                "source_category_id": parent_id,
                "source_category_name": parent_name,
                "parent_source_id": None,
            })

    # Second pass: Get all sub-categories and link them to parents
    for key, value in i18n_data.items():
        if key.startswith("professions."):
            full_id = key.replace("professions.", "")
            parts = full_id.split('_', 1)
            if len(parts) > 1:
                parent_id = parts[0]
                if parent_id in parent_map:
                    flat_list.append({
                        "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
                        "source_category_id": full_id,
                        "source_category_name": value,
                        "parent_source_id": parent_id,
                    })
                else:
                    logger.warning(f"Found orphan sub-category '{full_id}' with no matching parent '{parent_id}'. Skipping.")

    logger.info("Successfully extracted categories from __NEXT_DATA__.", count=len(flat_list))
    return flat_list


@app.task()
def fetch_url_data_cakeresume(url_JobCat: str = JOB_CAT_URL_CAKERESUME):
    logger.info("Current database connection", db_url=str(db_connection.get_engine().url))
    logger.info("Starting CakeResume profession category data fetch and sync.", url=url_JobCat)

    try:
        html_content = fetch_cakeresume_category_page_html(url_JobCat)
        profession_data = []
        if html_content is None:
            logger.error("Failed to fetch CakeResume category page HTML for profession data.", url=url_JobCat)
        else:
            try:
                profession_data = parse_next_data_for_i18n_categories(html_content)
            except ValueError as e:
                logger.error("Failed to parse CakeResume profession category data from HTML.", error=e, exc_info=True, url=url_JobCat)
        
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_CAKERESUME)

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in profession_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_CAKERESUME.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            categories_to_sync.sort(key=lambda x: x['source_category_id'])
            logger.info(
                "Found new or updated CakeResume profession categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_CAKERESUME, categories_to_sync)
        else:
            logger.info("No new or updated CakeResume profession categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(profession_data))

        # Apply classification to update parent_source_id for profession categories
        logger.info("Applying category classification for CakeResume profession categories.")
        apply_category_classification(SourcePlatform.PLATFORM_CAKERESUME)

    except Exception as e:
        logger.error("An unexpected error occurred during CakeResume category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    initialize_database()
    logger.info("Dispatching fetch_url_data_cakeresume task for local testing.", url=JOB_CAT_URL_CAKERESUME)
    fetch_url_data_cakeresume(JOB_CAT_URL_CAKERESUME)


================================================
FILE: crawler/project_cakeresume/task_jobs_cakeresume.py
================================================
# import os
# # python -m crawler.project_cakeresume.task_jobs_cakeresume
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---


import time
import structlog
import json
from typing import Optional
from bs4 import BeautifulSoup
import re

from crawler.worker import app
from crawler.database.schemas import CrawlStatus, SourcePlatform
from crawler.database.connection import initialize_database
from crawler.database.repository import upsert_jobs, mark_urls_as_crawled, get_urls_by_crawl_status
from crawler.project_cakeresume.client_cakeresume import fetch_cakeresume_job_data
from crawler.project_cakeresume.parser_cakeresume import parse_job_details_to_pydantic

logger = structlog.get_logger(__name__)

@app.task(rate_limit='60/m')
def fetch_url_data_cakeresume(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed job info from a URL, parses it, stores it, and marks the URL status.
    """
    original_url = url
    if "www.cake.me/jobs/" in url:
        new_url = url.replace("https://www.cake.me/jobs/", "https://www.cake.me/companies/")
        if new_url != url:
            url = new_url
            logger.info("Transformed URL for processing.", original_url=original_url, new_url=url)

    job_id = None
    try:
        match = re.search(r'/jobs/([a-zA-Z0-9_-]+)', original_url)
        job_id = match.group(1) if match else None

        if not job_id:
            logger.error("Failed to extract job_id from URL.", url=original_url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
            return None

        html_content = fetch_cakeresume_job_data(url)
        if not html_content:
            logger.error("Failed to fetch job data from CakeResume.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
            return None

        soup = BeautifulSoup(html_content, 'html.parser')
        data_script = soup.find('script', id='__NEXT_DATA__')

        if not data_script:
            logger.error("Could not find __NEXT_DATA__ script tag.", url=url, job_id=job_id)
            mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
            return None

        page_props = json.loads(data_script.string).get('props', {}).get('pageProps', {})
        job_details = page_props.get('job')

        if not job_details:
            logger.error("Could not find job details in __NEXT_DATA__.", url=url, job_id=job_id)
            mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
            return None
        
        job_pydantic_data = parse_job_details_to_pydantic(job_details, html_content, url)

        if not job_pydantic_data:
            logger.error("Failed to parse job data to Pydantic.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
            return None

        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [original_url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error("Unexpected error processing CakeResume job data.", error=e, job_id=job_id, url=original_url, exc_info=True)
        mark_urls_as_crawled({CrawlStatus.FAILED: [original_url]})
        return None


if __name__ == "__main__":
    initialize_database()

    PRODUCER_BATCH_SIZE = 20000000
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]
    
    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_CAKERESUME,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info("Found URLs to process.", count=len(urls_to_process))
        for url in urls_to_process:
            logger.info("Processing URL.", url=url)
            fetch_url_data_cakeresume(url)
            time.sleep(1)
    else:
        logger.info("No URLs found to process for testing.")


================================================
FILE: crawler/project_cakeresume/task_urls_cakeresume.py
================================================
# import os
# # python -m crawler.project_cakeresume.task_urls_cakeresume
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

import json
import structlog
from collections import deque
from bs4 import BeautifulSoup
from typing import Set, List, Dict

from crawler.worker import app
from crawler.database.schemas import SourcePlatform, UrlCategoryPydantic, CategorySourcePydantic
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.project_cakeresume.client_cakeresume import fetch_cakeresume_job_urls
from crawler.database.connection import initialize_database
from crawler.config import (
    URL_CRAWLER_UPLOAD_BATCH_SIZE,
)
from crawler.project_cakeresume.config_cakeresume import (
    URL_CRAWLER_ORDER_BY_CAKERESUME,
    JOB_DETAIL_BASE_URL_CAKERESUME,
)

logger = structlog.get_logger(__name__)


@app.task
def crawl_and_store_cakeresume_category_urls(job_category: dict, url_limit: int = 0) -> None:
    """
    Celery task: 迭代指定 CakeResume 工作分類的所有頁面，獲取職缺 URL，並將其存儲到資料庫。
    """
    try:
        crawler = CakeResumeCrawler(job_category, url_limit)
        crawler.run()
    except Exception as e:
        logger.error(
            "An unexpected error occurred during the crawling task.",
            job_category=job_category,
            error=str(e),
            exc_info=True
        )

class CakeResumeCrawler:
    """
    將爬蟲的狀態和邏輯封裝在此類中，以提高程式碼的可讀性和可維護性。
    """
    def __init__(self, job_category: Dict, url_limit: int = 0):
        self.job_category = CategorySourcePydantic.model_validate(job_category)
        self.job_category_code = self.job_category.source_category_id
        self.url_limit = url_limit
        self.global_job_url_set: Set[str] = set()
        self.current_batch_urls: List[str] = []
        self.current_batch_url_categories: List[Dict] = []
        self.recent_counts = deque(maxlen=4)
        self.current_page = 1

        logger.info(
            "Crawler initialized for CakeResume job category.",
            job_category_code=self.job_category_code,
            url_limit=self.url_limit
        )

    def run(self) -> None:
        """
        執行爬蟲任務的主函式。
        """
        while True:
            if 0 < self.url_limit <= len(self.global_job_url_set):
                logger.info(
                    "URL limit reached. Ending task early.",
                    collected_urls=len(self.global_job_url_set)
                )
                break
            
            if self.current_page % 5 == 0 or self.current_page == 1:
                 logger.info(
                    "Current page being processed.",
                    page=self.current_page,
                    job_category_code=self.job_category_code,
                )

            html_content = fetch_cakeresume_job_urls(
                KEYWORDS="",
                CATEGORY=self.job_category_code,
                ORDER=URL_CRAWLER_ORDER_BY_CAKERESUME,
                PAGE_NUM=self.current_page,
            )

            if not html_content:
                logger.info(
                    "No content retrieved, indicating end of pages.",
                    page=self.current_page,
                    job_category_code=self.job_category_code,
                )
                break
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            job_urls_on_page = self._parse_job_urls(soup)
            if not job_urls_on_page:
                logger.info("No job URLs found on page, indicating end of pages.", page=self.current_page, job_category_code=self.job_category_code)
                break

            new_urls_found = self._process_urls(job_urls_on_page)
            if not new_urls_found and len(self.global_job_url_set) > 0:
                logger.info("No new unique URLs found on this page. Ending task.", page=self.current_page, job_category_code=self.job_category_code)
                break

            if self._check_for_stagnation():
                break

            self.current_page += 1

        self._flush_batch_to_db()
        logger.info("Crawling task finished for job category.", job_category_code=self.job_category_code)

    def _parse_job_urls(self, soup: BeautifulSoup) -> List[str]:
        """從 HTML 中解析出職缺的 URL 列表。"""
        urls = []
        
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        if next_data_script:
            try:
                data = json.loads(next_data_script.string)
                results = data.get('props', {}).get('pageProps', {}).get('serverState', {}).get('initialResults', {}).get('Job', {}).get('results', [{}])[0].get('hits', [])
                for job in results:
                    if 'path' in job and 'page' in job and 'path' in job['page']:
                        full_url = f"{JOB_DETAIL_BASE_URL_CAKERESUME}/companies/{job['page']['path']}/jobs/{job['path']}"
                        urls.append(full_url)
                if urls:
                    return urls
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning("Could not parse __NEXT_DATA__ JSON or key not found.", error=str(e))

        job_links = soup.find_all('a', class_='JobSearchItem_jobTitle__bu6yO')
        for link in job_links:
            href = link.get('href')
            if href:
                if href.startswith('http'):
                    full_url = href
                else:
                    full_url = f"{JOB_DETAIL_BASE_URL_CAKERESUME}{href}"
                urls.append(full_url)
        return urls
    
    def _process_urls(self, urls: List[str]) -> bool:
        """處理新抓取的 URL，並在需要時將其寫入資料庫。"""
        new_urls_added = False
        for url in urls:
            if url not in self.global_job_url_set:
                new_urls_added = True
                self.global_job_url_set.add(url)
                self.current_batch_urls.append(url)
            
            self.current_batch_url_categories.append(
                UrlCategoryPydantic(
                    source_url=url,
                    source_category_id=self.job_category_code,
                ).model_dump()
            )

        if len(self.current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            self._flush_batch_to_db()
            
        return new_urls_added

    def _flush_batch_to_db(self) -> None:
        """將累積的批次資料寫入資料庫。"""
        if not self.current_batch_urls:
            return

        logger.info(
            "Storing batch of URLs and URL-category relations to database.",
            url_count=len(self.current_batch_urls),
            category_relation_count=len(self.current_batch_url_categories),
        )
        upsert_urls(SourcePlatform.PLATFORM_CAKERESUME, self.current_batch_urls)
        upsert_url_categories(self.current_batch_url_categories)
        self.current_batch_urls.clear()
        self.current_batch_url_categories.clear()

    def _check_for_stagnation(self) -> bool:
        """檢查是否連續多頁沒有抓到新的職缺。"""
        total_jobs = len(self.global_job_url_set)
        self.recent_counts.append(total_jobs)
        if len(self.recent_counts) == self.recent_counts.maxlen and len(set(self.recent_counts)) == 1 and total_jobs > 0:
            logger.info(
                "No new data found for the last few pages. Ending task early.",
                max_len=self.recent_counts.maxlen
            )
            return True
        return False


if __name__ == "__main__":
    initialize_database()

    n_days = 7
    url_limit = 1000000

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_CAKERESUME)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_CAKERESUME)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_CAKERESUME, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    if categories_to_dispatch:
        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_cakeresume_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_cakeresume_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")


================================================
FILE: crawler/project_yes123/client_yes123.py
================================================
import random
import time
from typing import Any, Dict, Optional

import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import urllib.parse

from crawler.config import (
    URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
    URL_CRAWLER_SLEEP_MAX_SECONDS,
    URL_CRAWLER_SLEEP_MIN_SECONDS,
)
import traceback

import structlog

from crawler.logging_config import configure_logging
from crawler.project_yes123.config_yes123 import (
    HEADERS_YES123,
    JOB_LISTING_BASE_URL_YES123,
    JOB_CAT_URL_YES123,
)

# Suppress only the single InsecureRequestWarning from urllib3 needed
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


configure_logging()
logger = structlog.get_logger(__name__)


# 使用 tenacity 庫來處理請求重試邏輯
# stop_after_attempt(5): 最多重試 5 次
# wait_exponential(multiplier=1, min=4, max=10): 每次重試等待時間呈指數增長，從 4 秒到 10 秒
# retry_if_exception_type(requests.exceptions.RequestException): 僅在發生 requests.exceptions.RequestException 異常時重試
# reraise=True: 重試次數用盡後，如果仍然失敗，則重新拋出最後一個異常
@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(requests.exceptions.RequestException),
    reraise=True,
)
def _make_web_request(
    method: str,
    url: str,
    headers: Optional[Dict[str, str]] = None,
    params: Optional[Dict[str, Any]] = None,
    timeout: int = 10,
    verify: bool = True,
    log_context: Optional[Dict[str, Any]] = None,
) -> Optional[str]: # Return HTML content as string
    """
    通用的網頁請求函式，處理隨機延遲、請求發送、和錯誤處理。
    """
    if log_context is None:
        log_context = {}

    # Add random delay before making API request
    sleep_time = random.uniform(
        URL_CRAWLER_SLEEP_MIN_SECONDS, URL_CRAWLER_SLEEP_MAX_SECONDS
    )
    logger.debug("Sleeping before web request.", duration=sleep_time, **log_context)
    time.sleep(sleep_time)

    try:
        response = requests.request(
            method,
            url,
            headers=headers,
            params=params,
            timeout=timeout,
            verify=verify,
        )
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)
        return response.text
        
    except requests.exceptions.RequestException:
        logger.error(f"Network error during web request: {url}")
        traceback.print_exc()
        raise  # Re-raise the exception to trigger tenacity retry
    except Exception:
        logger.error(f"Unexpected error during web request: {url}")
        traceback.print_exc()
        raise # Re-raise the exception to see full traceback

def fetch_yes123_category_data(
    url: str = JOB_CAT_URL_YES123, headers: Dict[str, str] = HEADERS_YES123
) -> Optional[str]:
    """
    從 yes123 獲取職務分類的原始數據 (HTML 內容)。
    """
    return _make_web_request("GET", url, headers=headers, log_context={"api_type": "yes123_category_data"})

def yes123_url(
    KEYWORDS: str = "",
    CATEGORY: str = "",
    ORDER: str = "new",
    STRREC: int = 0, # Changed from PAGE_NUM to STRREC
) -> str:
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 yes123 求職網的完整職缺網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字。
    CATEGORY (str): 職缺的類別代碼。
    ORDER (str, optional): 排序方式。預設為 "new" (最新)。
    STRREC (int, optional): 指定的起始記錄數 (offset)。預設為 0。

    返回:
    str: 生成的 yes123 求職網址。
    """
    base_url = JOB_LISTING_BASE_URL_YES123
    params = {
        "strrec": STRREC, # Changed from "p" to "strrec"
        "s_key": KEYWORDS,
        "find_work_mode1": CATEGORY, # Changed from "job_kind" to "find_work_mode1"
        "order_by": ORDER, # Changed from "order" to "order_by"
        "order_ascend": "desc", # Added based on example URLs
        "search_from": "joblist", # Added based on example URLs
    }
    query_string = urllib.parse.urlencode(params)
    return f"{base_url}?{query_string}"



def fetch_yes123_job_data(job_url: str) -> Optional[str]: # Returns HTML content of the job detail page
    """
    從 yes123 職缺頁面抓取單一 URL 的資料 (HTML 內容)。
    """
    return _make_web_request(
        "GET",
        job_url,
        headers=HEADERS_YES123,
        timeout=URL_CRAWLER_REQUEST_TIMEOUT_SECONDS,
        verify=False,
        log_context={
            "api_type": "yes123_job_detail",
            "url": job_url,
        },
    )



================================================
FILE: crawler/project_yes123/config_yes123.py
================================================
import structlog
from crawler.config import config_section

logger = structlog.get_logger(__name__)

# yes123 平台相關設定
WEB_NAME_YES123 = config_section.get("WEB_NAME_YES123", "yes123_求職網")
JOB_CAT_URL_YES123 = config_section.get("JOB_CAT_URL_YES123", "https://www.yes123.com.tw/json_file/work_mode.json")
JOB_LISTING_BASE_URL_YES123 = config_section.get("JOB_LISTING_BASE_URL_YES123", "https://www.yes123.com.tw/findjob/job_list.asp")
JOB_DETAIL_BASE_URL_YES123 = config_section.get("JOB_DETAIL_BASE_URL_YES123", "https://www.yes123.com.tw/findjob/job_detail.asp")

HEADERS_YES123 = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.yes123.com.tw",
}

URL_CRAWLER_PAGE_SIZE_YES123 = int(config_section.get("URL_CRAWLER_PAGE_SIZE_YES123", "100"))
URL_CRAWLER_ORDER_BY_YES123 = config_section.get("URL_CRAWLER_ORDER_BY_YES123", "new") # "new" or other options



================================================
FILE: crawler/project_yes123/hight_Level_refine_url_yes123.py
================================================
import os
import structlog
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3
from typing import Set, List

# --- Local Test Environment Setup ---
if __name__ == "__main__":
    os.environ['CRAWLER_DB_NAME'] = 'test_db'
    # os.environ.setdefault('CRAWLER_DB_NAME', 'test_db')
# --- End Local Test Environment Setup ---

from crawler.worker import app
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.database.connection import initialize_database
from crawler.project_yes123.config_yes123 import HEADERS_YES123

# --- 常數定義 ---
BASE_URL = "https://www.yes123.com.tw"
JOB_LIST_URL_TEMPLATE = f"{BASE_URL}/wk_index/joblist.asp?find_work_mode1={{job_category_code}}&order_by=m_date&order_ascend=desc"
JOB_LINK_SELECTOR = "a.Job_opening_block"
DEFAULT_TIMEOUT = 15
CONSECUTIVE_EMPTY_PAGE_LIMIT = 3  # 連續 3 個空頁面後就停止

logger = structlog.get_logger(__name__)

# --- 網路請求與共享 Session ---
def create_session_with_retries() -> requests.Session:
    # (此函式與之前版本相同，保持不變)
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update(HEADERS_YES123)
    session.verify = False
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    return session

# --- Celery 任務定義 ---

@app.task(
    bind=True, 
    autoretry_for=(requests.exceptions.RequestException,), 
    retry_kwargs={'max_retries': 3, 'countdown': 5}
)
def task_crawl_yes123_page_and_chain(self, job_category_code: str, page_num: int, consecutive_empty_count: int):
    """
    【處理與鏈接任務】
    爬取單一頁面，如果找到內容則繼續鏈接下一頁任務；
    如果頁面為空，則增加空頁計數，直到達到上限為止。
    """
    page_url = f"{JOB_LIST_URL_TEMPLATE.format(job_category_code=job_category_code)}&strrec={(page_num - 1) * 30}"
    logger.info(
        "開始處理頁面",
        page_num=page_num,
        category=job_category_code,
        empty_count=consecutive_empty_count,
    )
    
    session = create_session_with_retries()
    
    try:
        response = session.get(page_url, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()
        response.encoding = 'utf-8-sig'
        soup = BeautifulSoup(response.text, "html.parser")

        job_links = [
            urljoin(BASE_URL, tag["href"])
            for tag in soup.select(JOB_LINK_SELECTOR)
            if "href" in tag.attrs
        ]

        next_empty_count = 0
        if job_links:
            # 找到 URL，存儲數據並重置計數器
            logger.info("頁面找到 URLs", count=len(job_links), page=page_num, category=job_category_code)
            url_category_data = [
                UrlCategoryPydantic(source_url=link, source_category_id=job_category_code).model_dump()
                for link in job_links
            ]
            upsert_urls(SourcePlatform.PLATFORM_YES123, job_links)
            upsert_url_categories(url_category_data)
            next_empty_count = 0
        else:
            # 未找到 URL，增加計數器
            logger.warning("頁面為空或未找到 URLs", page=page_num, category=job_category_code)
            next_empty_count = consecutive_empty_count + 1

        # 決定是否繼續鏈接
        if next_empty_count >= CONSECUTIVE_EMPTY_PAGE_LIMIT:
            logger.info(
                "連續空頁面已達上限，終止任務鏈",
                limit=CONSECUTIVE_EMPTY_PAGE_LIMIT,
                category=job_category_code,
            )
            return

        # 派送下一個頁面的任務
        next_page_num = page_num + 1
        logger.info("鏈接下一頁任務", next_page=next_page_num, category=job_category_code)
        task_crawl_yes123_page_and_chain.delay(
            job_category_code=job_category_code,
            page_num=next_page_num,
            consecutive_empty_count=next_empty_count
        )

    except Exception as e:
        logger.error("處理頁面時發生未知嚴重錯誤，任務將重試", url=page_url, error=str(e))
        # 對於非請求錯誤，也觸發重試
        raise self.retry(exc=e)


@app.task(name="tasks.start_yes123_crawl")
def task_start_yes123_crawl_chain(job_category: dict):
    """
    【啟動任務】
    這是爬取一個完整職缺類別的入口點。
    它只負責啟動第一個頁面的爬取任務。
    """
    try:
        category = CategorySourcePydantic.model_validate(job_category)
        job_category_code = category.source_category_id
    except Exception as e:
        logger.error("無效的職缺類別資料，啟動失敗", data=job_category, error=str(e))
        return
    
    logger.info("啟動任務鏈", job_category_code=job_category_code)
    # 啟動鏈條的第一環：從第 1 頁開始，空頁計數為 0
    task_crawl_yes123_page_and_chain.delay(
        job_category_code=job_category_code,
        page_num=1,
        consecutive_empty_count=0
    )


# --- 本地測試執行區塊 ---
def _run_local_test():
    """執行本地測試的函式。"""
    initialize_database()
    n_days = 7
    
    logger.info("本地測試開始：獲取需要處理的職缺類別...")
    # (這部分的邏輯與之前相同)
    all_categories: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_YES123)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories}
    crawled_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_YES123)
    stale_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_YES123, n_days)
    dispatch_ids = (all_category_ids - crawled_ids) | stale_ids
    categories_to_dispatch = [cat for cat in all_categories if cat.source_category_id in dispatch_ids]

    if not categories_to_dispatch:
        logger.info("沒有需要處理的職缺類別。")
        return

    logger.info(f"共發現 {len(categories_to_dispatch)} 個類別需要啟動爬取鏈。")
    
    # categories_to_dispatch = categories_to_dispatch[:1] 

    for category in categories_to_dispatch:
        logger.info("本地調度啟動任務", job_category_code=category.source_category_id)
        # 在本地直接調用啟動函式來模擬行為
        task_start_yes123_crawl_chain(category.model_dump())

if __name__ == "__main__":
    _run_local_test()


================================================
FILE: crawler/project_yes123/producer_category_yes123.py
================================================
from crawler.project_yes123.task_category_yes123 import fetch_and_sync_yes123_categories
import structlog

from crawler.logging_config import configure_logging
from crawler.project_yes123.config_yes123 import JOB_CAT_URL_YES123

configure_logging()
logger = structlog.get_logger(__name__)

# 這段代碼保持原樣，用於在 Celery 環境中異步分派任務
fetch_and_sync_yes123_categories.s(JOB_CAT_URL_YES123).apply_async(queue='producer_category_yes123')
logger.info("send task_category_yes123 url", url=JOB_CAT_URL_YES123, queue='producer_category_yes123')


================================================
FILE: crawler/project_yes123/producer_jobs_yes123.py
================================================
import structlog
from celery import group
from sqlalchemy.exc import SQLAlchemyError

from crawler.project_yes123.task_jobs_yes123 import fetch_url_data_yes123
from crawler.database.repository import get_urls_by_crawl_status, update_urls_status
from crawler.database.models import SourcePlatform, CrawlStatus
from crawler.logging_config import configure_logging
from crawler.config import PRODUCER_BATCH_SIZE

# --- 初始化 ---
configure_logging()
logger = structlog.get_logger(__name__)

logger.info("Producer configuration loaded.", producer_batch_size=PRODUCER_BATCH_SIZE)


def dispatch_yes123_job_urls():
    """
    從資料庫讀取待處理或失敗的 yes123 職缺 URL，更新其狀態，然後分發給 Celery worker。
    """
    logger.info("開始從資料庫讀取 yes123 職缺 URL 並分發任務...")

    try:
        # 1. 讀取新任務 (PENDING) 和失敗的任務 (FAILED)
        statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING]
        urls_to_process = get_urls_by_crawl_status(
            platform=SourcePlatform.PLATFORM_YES123,
            statuses=statuses_to_fetch,
            limit=PRODUCER_BATCH_SIZE,
        )

        if not urls_to_process:
            logger.info("沒有找到符合條件的 yes123 職缺 URL 可供分發。")
            return

        logger.info("從資料庫讀取到一批 yes123 URL", count=len(urls_to_process))

        # 2. 立即更新這些 URL 的狀態為 QUEUED，防止其他 producer 重複讀取
        update_urls_status(urls_to_process, CrawlStatus.QUEUED)
        logger.info("已更新 yes123 URL 狀態為 QUEUED", count=len(urls_to_process))

        # 3. 使用 group 高效地批次分發任務，並指定佇列
        task_group = group(fetch_url_data_yes123.s(url.source_url) for url in urls_to_process)
        task_group.apply_async(queue="producer_jobs_yes123")

        logger.info(
            "已成功分發一批 yes123 職缺 URL 任務", count=len(urls_to_process), queue="producer_jobs_yes123"
        )

    except SQLAlchemyError as e:
        logger.error("資料庫操作失敗", error=str(e))
    except Exception as e:
        logger.error("分發任務時發生未預期的錯誤", error=str(e))


================================================
FILE: crawler/project_yes123/producer_urls_yes123.py
================================================
[Binary file]


================================================
FILE: crawler/project_yes123/task_category_yes123.py
================================================
# import os
# # python -m crawler.project_yes123.task_category_yes123
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

import json

import structlog

from crawler.database import connection as db_connection
from crawler.database import repository
from crawler.database.connection import initialize_database
from crawler.database.schemas import SourcePlatform
from crawler.project_yes123.client_yes123 import fetch_yes123_category_data
from crawler.project_yes123.config_yes123 import JOB_CAT_URL_YES123
from crawler.worker import app

# Import MAPPING from apply_classification.py
from crawler.database.category_classification_data.apply_classification import MAPPING

logger = structlog.get_logger(__name__)


def flatten_yes123_categories(json_content):
    """
    Parses the JSON content from yes123 category data and flattens the category structure.
    Applies major category mapping for top-level categories.
    """
    flattened = []
    try:
        data = json.loads(json_content.encode('utf-8').decode('utf-8-sig'))
        list_obj = data.get('listObj', [])

        for level1_item in list_obj:
            for level2_item in level1_item.get('list_2', []):
                source_category_id = level2_item.get('code')
                source_category_name = level2_item.get('level_2_name')

                parent_source_id = None
                if source_category_id:
                    parts = source_category_id.split('_')
                    if len(parts) == 4:
                        if parts[3] != '0000':
                            parent_source_id = f"{parts[0]}_{parts[1]}_{parts[2]}_0000"
                        else: # This is a top-level category in Yes123's original structure
                            mapped_parent_id = MAPPING[SourcePlatform.PLATFORM_YES123].get(source_category_name)
                            if mapped_parent_id:
                                parent_source_id = mapped_parent_id
                    else:
                        logger.warning("Unexpected code format for parent derivation.", code=source_category_id)


                if source_category_id and source_category_name:
                    flattened.append({
                        "parent_source_id": parent_source_id,
                        "source_category_id": source_category_id,
                        "source_category_name": source_category_name,
                        "source_platform": SourcePlatform.PLATFORM_YES123.value,
                    })
    except json.JSONDecodeError as e:
        logger.error("Failed to decode JSON content for yes123 categories.", error=e, exc_info=True)
    except Exception as e:
        logger.error("An unexpected error occurred during yes123 category flattening.", error=e, exc_info=True)

    return flattened


@app.task()
def fetch_url_data_yes123(url_JobCat: str = JOB_CAT_URL_YES123):
    logger.info("Current database connection", db_url=str(db_connection.get_engine().url))
    logger.info("Starting yes123 category data fetch and sync.", url=url_JobCat)

    try:
        existing_categories = repository.get_source_categories(SourcePlatform.PLATFORM_YES123)

        html_content = fetch_yes123_category_data(url_JobCat)
        if html_content is None:
            logger.error("Failed to fetch yes123 category data from web.", url=url_JobCat)
            return

        flattened_data = flatten_yes123_categories(html_content)

        if not flattened_data:
            logger.warning("No categories found in yes123 category data.", url=url_JobCat)
            return

        if not existing_categories:
            logger.info("yes123 category database is empty. Performing initial bulk sync.", total_api_categories=len(flattened_data))
            repository.sync_source_categories(SourcePlatform.PLATFORM_YES123, flattened_data)
            return

        api_categories_set = {
            (d["source_category_id"], d["source_category_name"], d["parent_source_id"])
            for d in flattened_data
        }
        db_categories_set = {
            (
                category.source_category_id,
                category.source_category_name,
                category.parent_source_id,
            )
            for category in existing_categories
        }

        categories_to_sync_set = api_categories_set - db_categories_set

        if categories_to_sync_set:
            categories_to_sync = [
                {
                    "source_category_id": cat_id,
                    "source_category_name": name,
                    "parent_source_id": parent_id,
                    "source_platform": SourcePlatform.PLATFORM_YES123.value,
                }
                for cat_id, name, parent_id in categories_to_sync_set
            ]
            categories_to_sync.sort(key=lambda x: x['source_category_id'])
            logger.info(
                "Found new or updated yes123 categories to sync.",
                count=len(categories_to_sync),
            )
            repository.sync_source_categories(SourcePlatform.PLATFORM_YES123, categories_to_sync)
        else:
            logger.info("No new or updated yes123 categories to sync.", existing_categories_count=len(existing_categories), api_categories_count=len(flattened_data))

    except Exception as e:
        logger.error("An unexpected error occurred during yes123 category sync.", error=e, exc_info=True, url=url_JobCat)


if __name__ == "__main__":
    initialize_database()
    logger.info("Dispatching fetch_url_data_yes123 task for local testing.", url=JOB_CAT_URL_YES123)
    fetch_url_data_yes123(JOB_CAT_URL_YES123)


================================================
FILE: crawler/project_yes123/task_jobs_yes123.py
================================================
# import os
# # python -m crawler.project_yes123.task_jobs_yes123
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

import structlog
from typing import Optional, Dict
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import re
from urllib.parse import urljoin

import requests
import urllib3

from crawler.worker import app
from crawler.database.schemas import (
    CrawlStatus,
    SourcePlatform,
    JobStatus,
    JobPydantic,
    JobType,
)
from crawler.database.connection import initialize_database
from crawler.database.repository import (
    get_urls_by_crawl_status,
    upsert_jobs,
    mark_urls_as_crawled,
)
from crawler.utils.salary_parser import parse_salary_text
from crawler.project_yes123.config_yes123 import HEADERS_YES123

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# Constants
BASE_URL = "https://www.yes123.com.tw"
DEFAULT_TIMEOUT = 15

logger = structlog.get_logger(__name__)


def fetch_yes123_job_data(job_url: str, headers: dict, timeout: int = DEFAULT_TIMEOUT) -> Optional[dict]:
    """
    Fetches and scrapes detailed information from a given yes123 job URL.
    """
    try:
        response = requests.get(job_url, headers=headers, timeout=timeout, verify=False)
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        response.raise_for_status()

        if "此工作機會已關閉" in response.text or "您要找的頁面不存在" in response.text:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        scraped_data = {"職缺網址": job_url}
        
        header_block = soup.select_one("div.box_job_header_center")
        if header_block:
            title_tag = header_block.select_one("h1")
            scraped_data["職缺名稱"] = title_tag.get_text(strip=True) if title_tag else "N/A"
            company_tag = header_block.select_one("a.link_text_black")
            scraped_data["公司名稱"] = company_tag.get_text(strip=True) if company_tag else "N/A"
            if company_tag and "href" in company_tag.attrs:
                scraped_data["公司網址"] = urljoin(BASE_URL, company_tag["href"])
            else:
                scraped_data["公司網址"] = "N/A"

        posted_at_tag = soup.find(string=re.compile(r"職缺更新："))
        if posted_at_tag:
            posted_at_text = posted_at_tag.get_text(strip=True).replace("職缺更新：", "")
            if "今天" in posted_at_text:
                scraped_data["發布日期"] = datetime.now(timezone.utc)
            else:
                try:
                    current_year = datetime.now(timezone.utc).year
                    posted_at_date = datetime.strptime(f"{current_year}.{posted_at_text}", "%Y.%m.%d").replace(tzinfo=timezone.utc)
                    scraped_data["發布日期"] = posted_at_date
                except ValueError:
                    try:
                        posted_at_date = datetime.strptime(posted_at_text, "%Y.%m.%d").replace(tzinfo=timezone.utc)
                        scraped_data["發布日期"] = posted_at_date
                    except ValueError:
                        scraped_data["發布日期"] = None
        else:
            scraped_data["發布日期"] = None

        for section in soup.select("div.job_explain"):
            section_title_tag = section.select_one("h3")
            if not section_title_tag:
                continue
            section_title = section_title_tag.get_text(strip=True)

            if section_title in ["徵才說明", "工作條件", "企業福利", "技能與求職專長"]:
                for item in section.select("ul > li"):
                    key_tag = item.select_one("span.left_title")
                    value_tag = item.select_one("span.right_main")
                    if key_tag and value_tag:
                        key = key_tag.get_text(strip=True).replace("：", "")
                        value = value_tag.get_text(strip=True, separator="\n")
                        scraped_data.setdefault(key, "")
                        scraped_data[key] += f"\n(補充) {value}" if scraped_data[key] else value

        return scraped_data

    except requests.exceptions.RequestException as e:
        logger.error("Failed to fetch yes123 job data.", url=job_url, error=e)
        return None
    except Exception as e:
        logger.error("An unexpected error occurred during scraping.", url=job_url, error=e, exc_info=True)
        return None


def _parse_job_type(job_nature_text: Optional[str]) -> JobType:
    """Maps job nature text to JobType enum."""
    if not job_nature_text:
        return JobType.OTHER
    if "全職" in job_nature_text:
        return JobType.FULL_TIME
    if "兼職" in job_nature_text:
        return JobType.PART_TIME
    if "工讀" in job_nature_text:
        return JobType.INTERNSHIP
    return JobType.OTHER


def parse_job_details_to_pydantic(job_data: Dict[str, any], url: str) -> Optional[JobPydantic]:
    """
    Parses the scraped job data dictionary and converts it into a JobPydantic object.
    """
    try:
        job_id = None
        if "job_id=" in url:
            job_id = url.split("job_id=")[-1]
        elif "p_id=" in url:
            job_id = url.split("p_id=")[-1].split("&")[0]

        salary_text = job_data.get("薪資待遇", "")
        min_salary, max_salary, salary_type = parse_salary_text(salary_text)

        education_required_text = job_data.get("學歷要求", "") or "不拘"
        experience_required_text = job_data.get("工作經驗", "") or "不拘"

        return JobPydantic(
            source_platform=SourcePlatform.PLATFORM_YES123,
            source_job_id=job_id if job_id else url,
            url=job_data.get("職缺網址", url),
            status=JobStatus.ACTIVE,
            title=job_data.get("職缺名稱", ""),
            description=job_data.get("工作內容", ""),
            salary_text=salary_text,
            salary_min=min_salary,
            salary_max=max_salary,
            salary_type=salary_type,
            location_text=job_data.get("工作地點", ""),
            education_required_text=education_required_text,
            experience_required_text=experience_required_text,
            company_name=job_data.get("公司名稱", ""),
            company_url=job_data.get("公司網址", ""),
            posted_at=job_data.get("發布日期"),
            job_type=_parse_job_type(job_data.get("工作性質")),
        )
    except Exception as e:
        logger.error("Failed to parse job data to Pydantic.", error=e, job_data=job_data, url=url, exc_info=True)
        return None


@app.task()
def fetch_url_data_yes123(url: str) -> Optional[dict]:
    """
    Celery task: Fetches detailed job info from a URL, parses, stores it, and marks the URL status.
    """
    job_id = None
    try:
        if "job_id=" in url:
            job_id = url.split("job_id=")[-1]
        elif "p_id=" in url:
            job_id = url.split("p_id=")[-1].split("&")[0]

        job_data = fetch_yes123_job_data(url, HEADERS_YES123)
        if not job_data:
            logger.warning("Failed to fetch job data or job is closed.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        job_pydantic_data = parse_job_details_to_pydantic(job_data, url)
        if not job_pydantic_data:
            logger.error("Failed to parse job data.", job_id=job_id, url=url)
            mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
            return None

        upsert_jobs([job_pydantic_data])
        logger.info("Job parsed and upserted successfully.", job_id=job_id, url=url)
        mark_urls_as_crawled({CrawlStatus.SUCCESS: [url]})
        return job_pydantic_data.model_dump()

    except Exception as e:
        logger.error("Unexpected error processing URL.", error=e, job_id=job_id, url=url, exc_info=True)
        mark_urls_as_crawled({CrawlStatus.FAILED: [url]})
        return None


if __name__ == "__main__":
    initialize_database()

    PRODUCER_BATCH_SIZE = 20000000000000
    statuses_to_fetch = [CrawlStatus.FAILED, CrawlStatus.PENDING, CrawlStatus.QUEUED]

    logger.info("Fetching URLs to process for local testing.", statuses=statuses_to_fetch, limit=PRODUCER_BATCH_SIZE)

    urls_to_process = get_urls_by_crawl_status(
        platform=SourcePlatform.PLATFORM_YES123,
        statuses=statuses_to_fetch,
        limit=PRODUCER_BATCH_SIZE,
    )

    if urls_to_process:
        logger.info("Found URLs to process.", count=len(urls_to_process))
        for url in urls_to_process:
            logger.info("Processing URL.", url=url)
            fetch_url_data_yes123(url)
    else:
        logger.info("No URLs found to process for testing.")



================================================
FILE: crawler/project_yes123/task_urls_yes123.py
================================================
# import os
# # python -m crawler.project_yes123.task_urls_yes123
# # --- Local Test Environment Setup ---
# if __name__ == "__main__":
#     os.environ['CRAWLER_DB_NAME'] = 'test_db'
# # --- End Local Test Environment Setup ---

import structlog


import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import urllib3
from collections import deque
from typing import Set, List, Tuple
import ssl # Added for SSLContext

from crawler.worker import app
from crawler.database.schemas import (
    SourcePlatform,
    UrlCategoryPydantic,
    CategorySourcePydantic,
)
from crawler.database.repository import (
    upsert_urls,
    upsert_url_categories,
    get_all_categories_for_platform,
    get_all_crawled_category_ids_pandas,
    get_stale_crawled_category_ids_pandas,
)
from crawler.database.connection import initialize_database
from crawler.config import URL_CRAWLER_UPLOAD_BATCH_SIZE
from crawler.project_yes123.config_yes123 import HEADERS_YES123

# Suppress only the single InsecureRequestWarning from urllib3 needed
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logger = structlog.get_logger(__name__)

# 常數定義
BASE_URL = "https://www.yes123.com.tw"
JOB_LIST_URL_TEMPLATE = f"{BASE_URL}/wk_index/joblist.asp?find_work_mode1={{job_category_code}}&order_by=m_date&order_ascend=desc"
JOB_LINK_SELECTOR = "a.Job_opening_block"
PAGE_SELECTOR = "#inputState option"
DEFAULT_TIMEOUT = 15
CONSECUTIVE_EMPTY_PAGE_LIMIT = 4


def create_session_with_retries() -> requests.Session:
    """
    建立一個具有重試機制的 requests.Session 物件。
    """
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)

    # Attempt to force TLSv1.2
    try:
        context = ssl.create_default_context()
        context.minimum_version = ssl.TLSVersion.TLSv1_2
        adapter.ssl_context = context
    except AttributeError:
        logger.warning("SSLContext.minimum_version not available, cannot force TLS version.")

    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update(HEADERS_YES123)
    # yes123 的 SSL 憑證有時會有問題，因此設定 verify=False
    session.verify = False
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    return session


def fetch_yes123_job_urls_for_page(
    session: requests.Session, base_url: str, page: int = 1, timeout: int = 15
) -> Tuple[List[str], int]:
    """
    從 yes123 網站擷取指定頁面的職缺網址和最大頁數。

    Args:
        session (requests.Session): 用於發送請求的 Session 物件。
        base_url (str): 搜尋結果的第一頁網址 (不含分頁參數)。
        page (int): 欲抓取的頁碼。預設為 1。
        timeout (int): 請求的超時秒數。預設為 15。

    Returns:
        Tuple[List[str], int]: 一個包含職缺 URL 列表和最大頁數的元組。

    Raises:
        requests.exceptions.RequestException: 當請求失敗時拋出。
    """
    job_url_list = []
    max_total_pages = 1000
    current_url = f"{base_url}&strrec={(page - 1) * 30}"

    try:
        response = session.get(current_url, timeout=timeout)
        response.raise_for_status()
        response.encoding = 'utf-8-sig' # Ensure correct encoding
        soup = BeautifulSoup(response.text, "html.parser")

        link_tags = soup.select(JOB_LINK_SELECTOR)
        for tag in link_tags:
            if "href" in tag.attrs:
                full_url = urljoin(current_url, tag["href"])
                job_url_list.append(full_url)

        options = soup.select(PAGE_SELECTOR)
        max_total_pages = max(
            (int(opt["value"]) for opt in options if opt.get("value", "").isdigit()),
            default=1
        )

    except requests.exceptions.RequestException as e:
        logger.error("請求 yes123 頁面時發生錯誤", url=current_url, error=str(e))
    except Exception as e:
        logger.error("解析 yes123 頁面時發生未知錯誤", url=current_url, error=str(e))

    return list(set(job_url_list)), max_total_pages # Return unique URLs for the current page and max_total_pages


@app.task
def crawl_and_store_yes123_category_urls(job_category: dict, url_limit: int = 0) -> None:
    _crawl_and_store_yes123_category_urls_core(job_category, url_limit)


def _crawl_and_store_yes123_category_urls_core(job_category: dict, url_limit: int = 0) -> None:
    """
    Core function: Iterates through all pages of a specified yes123 job category, fetches job URLs,
    and stores them in the database in batches.
    """
    try:
        category = CategorySourcePydantic.model_validate(job_category)
        job_category_code = category.source_category_id
    except Exception as e:
        logger.error("無效的職缺類別資料", data=job_category, error=str(e))
        return
 
    logger.info(
        "開始爬取 yes123 職缺類別的 URL",
        job_category_code=job_category_code,
        url_limit=url_limit or "無限制",
    )
    logger.debug("URL_CRAWLER_UPLOAD_BATCH_SIZE is set to:", batch_size=URL_CRAWLER_UPLOAD_BATCH_SIZE)

    global_job_url_set: Set[str] = set()
    current_batch_urls: List[str] = []
    current_batch_url_categories: List[dict] = []
    recent_url_counts = deque(maxlen=CONSECUTIVE_EMPTY_PAGE_LIMIT)
    max_pages = 100000
    page = 1

    session = create_session_with_retries()
    base_category_url = JOB_LIST_URL_TEMPLATE.format(job_category_code=job_category_code)

    while True:
        # 檢查是否達到 URL 數量限制
        if url_limit and len(global_job_url_set) >= url_limit:
            logger.info(
                "已達到 URL 數量限制，提前結束任務",
                job_category_code=job_category_code,
                collected_urls=len(global_job_url_set),
            )
            break

        # 檢查是否已超過最大頁數
        if page > max_pages:
            logger.info("已處理完所有頁面，任務即將完成", max_pages=max_pages, job_category_code=job_category_code)
            break

        if page % 5 == 1: # Log every 5 pages
            logger.info(
                "正在處理頁面",
                page=page,
                job_category_code=job_category_code,
            )

        # 擷取職缺 URL 和最大頁數
        page_job_links, discovered_max_pages = fetch_yes123_job_urls_for_page(session, base_category_url, page=page) 

        if page == 1: # Update max_pages only on the first page fetch
            max_pages = discovered_max_pages
            logger.info("檢測到總頁數", max_pages=max_pages, job_category_code=job_category_code)

        if not page_job_links:
            logger.warning("當前頁面未找到職缺連結", page=page, job_category_code=job_category_code)

        new_urls_on_page = 0
        for full_job_link in page_job_links:
            if full_job_link not in global_job_url_set:
                global_job_url_set.add(full_job_link)
                current_batch_urls.append(full_job_link)
                current_batch_url_categories.append(
                    UrlCategoryPydantic(
                        source_url=full_job_link,
                        source_category_id=job_category_code,
                    ).model_dump()
                )
                new_urls_on_page += 1

        logger.debug(
            "頁面處理完畢",
            page=page,
            new_urls=new_urls_on_page,
            total_urls=len(global_job_url_set),
        )

        # 檢查是否連續多頁沒有新 URL
        recent_url_counts.append(len(global_job_url_set))
        if len(recent_url_counts) == CONSECUTIVE_EMPTY_PAGE_LIMIT and len(set(recent_url_counts)) == 3:
            logger.info(
                "連續多頁未發現新的 URL，提前結束任務",
                job_category_code=job_category_code,
            )
            break
 
        # 如果批次大小達到上限，則上傳資料
        if len(current_batch_urls) >= URL_CRAWLER_UPLOAD_BATCH_SIZE:
            logger.info(
                "達到批次上傳大小，開始上傳 URL",
                count=len(current_batch_urls),
                job_category_code=job_category_code,
            )
            upsert_urls(SourcePlatform.PLATFORM_YES123, current_batch_urls)
            upsert_url_categories(current_batch_url_categories)
            current_batch_urls.clear()
            current_batch_url_categories.clear()
            logger.info("批次上傳完成")
 
        page += 1

    # 上傳剩餘的 URL
    if current_batch_urls:
        logger.info(
            "任務完成，正在儲存剩餘的 URL 到資料庫",
            count=len(current_batch_urls),
            job_category_code=job_category_code,
        )
        upsert_urls(SourcePlatform.PLATFORM_YES123, current_batch_urls)
        upsert_url_categories(current_batch_url_categories)
    
    logger.info("任務執行完畢", job_category_code=job_category_code, total_collected=len(global_job_url_set))





if __name__ == "__main__":
    initialize_database()


    n_days = 7  # Define n_days for local testing
    url_limit = 1000000 # Set a high limit for full crawling during local testing

    all_categories_pydantic: List[CategorySourcePydantic] = get_all_categories_for_platform(SourcePlatform.PLATFORM_YES123)
    all_category_ids: Set[str] = {cat.source_category_id for cat in all_categories_pydantic}
    all_crawled_category_ids: Set[str] = get_all_crawled_category_ids_pandas(SourcePlatform.PLATFORM_YES123)
    stale_crawled_category_ids: Set[str] = get_stale_crawled_category_ids_pandas(SourcePlatform.PLATFORM_YES123, n_days)
    categories_to_dispatch_ids = (all_category_ids - all_crawled_category_ids) | stale_crawled_category_ids
    categories_to_dispatch = [
        cat for cat in all_categories_pydantic 
        if cat.source_category_id in categories_to_dispatch_ids
    ]

    # Only process the first category for local testing
    if categories_to_dispatch:
        # categories_to_process_single = [categories_to_dispatch[0]] # Uncomment to process only the first category
        
        for job_category in categories_to_dispatch:
            logger.info(
                "Dispatching crawl_and_store_yes123_category_urls task for local testing.",
                job_category_code=job_category.source_category_id,
                url_limit=url_limit,
            )
            crawl_and_store_yes123_category_urls(job_category.model_dump(), url_limit=url_limit)
    else:
        logger.info("No categories found to dispatch for testing.")



================================================
FILE: crawler/project_yes123/yes123_人力銀行_crawl.ipynb
================================================
# Jupyter notebook converted to Python script.

#  相關套件

import time
import random
import json
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import pandas as pd
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import urllib.parse

WEB_NAME = "yes123_人力銀行"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
    "Referer": "https://www.yes123.com.tw",
}

print(f"開始執行 {WEB_NAME} ")
# Output:
#   開始執行 yes123_人力銀行 


## 取得網站所有職業總覽
# 1. 取得 JSON 資料
# jobcat 檔案名稱

file_jobcat_json = f"{WEB_NAME}_jobcat_json.txt"
url_JobCat = "https://www.yes123.com.tw/json_file/work_mode.json"

response_jobcat = requests.get(url_JobCat, headers=HEADERS, timeout=10)
response_jobcat.raise_for_status()
jobcat_data = response_jobcat.json()["listObj"]
with open(file_jobcat_json, "w", encoding="utf-8") as f:
    json.dump(jobcat_data, f, ensure_ascii=False, indent=4)
print(f"職業總覽資料已儲存為 {file_jobcat_json}")


# 2. 定義遞迴函式展平資料
def flatten_jobcat_recursive(node_list, parent_des=None, parent_no=None):
    flat_list = []
    for level_1_node in node_list:
        parent_name = level_1_node.get("level_1_name")

        # Check if the inner list exists and is not empty
        if "list_2" in level_1_node and level_1_node["list_2"]:
            # Loop through the level 2 categories within this parent
            for level_2_node in level_1_node["list_2"]:
                row = {
                    "level_1_name": parent_name,
                    "level_2_code": level_2_node.get("code"),
                    "level_2_name": level_2_node.get("level_2_name"),
                }
                flat_list.append(row)
    return flat_list


# 3. 執行結果轉為 DataFrame
flattened_data = flatten_jobcat_recursive(jobcat_data)
df_jobcat = pd.json_normalize(flattened_data)
df_jobcat.to_excel(f"{WEB_NAME}_category.xlsx", index=False)
print(f"職業總覽資料已轉換為 '{WEB_NAME}_category.xlsx'")

# 篩選出 IT 相關的工作
mask = df_jobcat["level_2_code"].astype(str).str.startswith("2_1011")
df_it_jobs = df_jobcat[mask]
df_it_jobs.head(5)

import urllib.parse

# 產生 yes123 人力銀行網址 https://www.yes123.com.tw 根據提供的 (關鍵字和職缺類別) 轉換為職缺網址


def catch_yes123_url(KEYWORDS, CATEGORY, ORDER="date", PAGE_NUM=1):
    """
    這個函數會根據給定的關鍵字、類別、排序和頁碼參數，
    構建一個 yes123 求職網的完整職缺網址。

    參數:
    KEYWORDS (str): 職缺的關鍵字，例如 "雲端工程師"。若無則傳入空字串 ""。
    CATEGORY (str or list): 職缺的類別代碼，例如 "2_1011_0001_0000" 或者類別代碼的列表。
                            若無則傳入空字串 ""。
    ORDER (str, optional): 排序方式。可選值為 "relevance" (相關性) 或 "date" (最新日期)。
                           預設為 "date"。
    PAGE_NUM (int, optional): 指定的頁碼。預設為 1。

    返回:
    str: 生成的 yes123 職缺網址。
    """
    BASE_URL = "https://www.yes123.com.tw/wk_index/joblist.asp"

    # 確保頁碼至少為 1，避免負數或 0 造成計算錯誤
    safe_page_num = max(1, PAGE_NUM)
    # 根據頁碼計算 strrec 的值 (每頁 30 筆)
    strrec_value = (safe_page_num - 1) * 30

    # 建立一個參數字典來儲存所有查詢參數
    params = {"strrec": strrec_value, "search_type": "job", "search_from": "joblist"}

    # 根據排序參數設定 order_by 和 order_ascend
    if ORDER == "date":
        params["order_by"] = "m_date"
        params["order_ascend"] = "desc"
    else:  # 預設為相關性排序 (relevance)
        params["order_by"] = "neworder"
        params["order_ascend"] = "asc"

    # 如果有提供關鍵字，加入必要的 search_key_word 參數
    if KEYWORDS:
        params["search_key_word"] = KEYWORDS

    # 如果有提供職務類別，加入到參數中
    if CATEGORY:
        if isinstance(CATEGORY, list):
            # 如果是列表，將其轉換為字串，並用逗號分隔
            params["find_work_mode1"] = ",".join(CATEGORY)
        else:
            params["find_work_mode1"] = CATEGORY

    # 使用 urllib.parse.urlencode 將字典轉換為查詢字串，它會自動處理 & 和編碼
    query_string = urllib.parse.urlencode(params)

    return f"{BASE_URL}?{query_string}"



# # --- 測試範例 ---

# KEYWORDS_STR = "雲端工程師"
# CATEGORY_CODE1 = "2_1011_0001_0000"  # 單一類別
# CATEGORY_CODE2 = ["2_1011_0001_0000", "2_1011_0002_0000"]  # 多個類別

# # 1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁
# print("1. 有關鍵字, 單一類別, 相關性排序, 第 1 頁:")
# url_1 = catch_yes123_url(
#     KEYWORDS_STR, CATEGORY_CODE1, ORDER="relevance"
# )  # PAGE_NUM 省略，預設為 1
# print(url_1, "\n")

# # 2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁
# print("2. 無關鍵字, 多個類別, 最新日期排序, 第 2 頁:")
# url_2 = catch_yes123_url("", CATEGORY_CODE2, ORDER="date", PAGE_NUM=2)
# print(url_2, "\n")

# # 3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁
# print("3. 有關鍵字, 無類別, 最新日期排序, 第 3 頁:")
# url_3 = catch_yes123_url(KEYWORDS_STR, "", ORDER="date", PAGE_NUM=3)
# print(url_3, "\n")

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm  # 引入 tqdm
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def fetch_yes123_job_urls(
    initial_url: str, max_pages: int = None, timeout: int = 15
) -> list[str]:
    """
    從 yes123 網站擷取指定搜尋條件下的所有職缺網址。

    此函式優化了抓取流程：
    1. 只請求一次第一頁，同時用於獲取總頁數和解析第一頁的職缺。
    2. 為網路請求和頁面解析增加了錯誤處理，提升程式健壯性。
    3. 動態拼接 URL，移除了硬編碼的 BASE_URL。

    Args:
        initial_url (str): 搜尋結果的第一頁網址。
        max_pages (int, optional): 欲抓取的最大頁數。如果為 None，則自動抓取所有頁面。預設為 None。
        timeout (int, optional): 請求的超時秒數。預設為 15。

    Returns:
        list[str]: 包含所有不重複的工作職缺網址的列表。

    Raises:
        requests.exceptions.RequestException: 當初次請求失敗時拋出。
    """
    # 初始化一個空列表來儲存 URL
    job_url_list = []

    # 發送請求並解析 HTML
    response = requests.get(initial_url, headers=HEADERS, verify=False, timeout=timeout)
    response.raise_for_status()  # 若請求失敗，拋出例外
    soup = BeautifulSoup(response.text, "html.parser")

    # 解析職缺連結，並逐一 append 到列表中
    link_tags = soup.select(".Job_opening_M > a.Job_opening_block")
    for tag in link_tags:
        if "href" in tag.attrs:
            full_url = urljoin(initial_url, tag["href"])
            job_url_list.append(full_url)

    # 獲取最大頁碼
    options = soup.select("#inputState option")
    max_total_pages = max(
        int(option["value"]) for option in options if option["value"].isdigit()
    )

    # 如果指定了最大頁數，則使用該頁數，否則使用實際的最大頁數
    pages_to_fetch = min(max_total_pages, max_pages) if max_pages else max_total_pages

    # 解析指定的頁數，並顯示進度條
    for page in tqdm(range(2, pages_to_fetch + 1), desc="讀取頁面網址", unit="頁"):
        next_page_url = f"{initial_url}&strrec={(page - 1) * 30}"
        response = requests.get(next_page_url, headers=HEADERS, verify=False, timeout=timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        link_tags = soup.select(".Job_opening_M > a.Job_opening_block")
        for tag in link_tags:
            if "href" in tag.attrs:
                full_url = urljoin(initial_url, tag["href"])
                job_url_list.append(full_url)

    return job_url_list


# # 測試範例
# initial_url = "https://www.yes123.com.tw/wk_index/joblist.asp?strrec=0&search_type=job&search_from=joblist&order_by=neworder&order_ascend=asc&search_key_word=%E9%9B%B2%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%AB&find_work_mode1=2_1011_0001_0000"

# # 指定最大頁數
# specified_max_pages = 2  # 例如，想抓取前 2 頁

# job_urls = fetch_yes123_job_urls(initial_url, max_pages=specified_max_pages)
# print(f"總共筆數 : {len(job_urls)}")

# 從指定的職缺網址獲取職缺的相關數據


def fetch_yes123_job_data(job_url):
    """
    從指定的 yes123 職缺網址獲取詳細資訊，並依照預設順序整理。

    Args:
        job_url (str): 職缺的網址。

    Returns:
        dict: 包含職缺詳細資訊的字典，若頁面不存在或已關閉則返回 None。
    """

    try:
        response = requests.get(job_url, headers=HEADERS, timeout=15)
        response.raise_for_status()

        if "此工作機會已關閉" in response.text or "您要找的頁面不存在" in response.text:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        scraped_data = {"職缺網址": job_url}
        header_block = soup.select_one("div.box_job_header_center")
        if header_block:
            title_tag = header_block.select_one("h1")
            scraped_data["職缺名稱"] = (
                title_tag.get_text(strip=True) if title_tag else "N/A"
            )

            company_tag = header_block.select_one("a.link_text_black")
            scraped_data["公司名稱"] = (
                company_tag.get_text(strip=True) if company_tag else "N/A"
            )

        # --- 遍歷所有資訊區塊 (class="job_explain") ---
        all_sections = soup.select("div.job_explain")
        for section in all_sections:
            section_title_tag = section.select_one("h3")
            if not section_title_tag:
                continue

            section_title = section_title_tag.get_text(strip=True)

            # Key-Value 結構的區塊
            if section_title in ["徵才說明", "工作條件", "企業福利", "技能與求職專長"]:
                list_items = section.select("ul > li")
                for item in list_items:
                    key_tag = item.select_one("span.left_title")
                    value_tag = item.select_one("span.right_main")
                    if key_tag and value_tag:
                        key = key_tag.get_text(strip=True).replace("：", "")
                        value = value_tag.get_text(strip=True, separator="\n")
                        if key in scraped_data:  # 處理重複的 key (例如休假制度)
                            scraped_data[key] += f"\n(補充) {value}"
                        else:
                            scraped_data[key] = value

            # 純文字列表區塊 (法定保障)
            elif section_title == "法定保障":
                items = section.select("li > span.exception")
                scraped_data[section_title] = (
                    ", ".join([i.get_text(strip=True) for i in items])
                    if items
                    else "N/A"
                )

            # 純文字描述區塊 (其他條件)
            elif section_title == "其他條件":
                item = section.select_one("li > span.exception")
                scraped_data[section_title] = (
                    item.get_text(strip=True, separator="\n") if item else "N/A"
                )

            # 標籤結構區塊 (徵才特色)
            elif section_title == "徵才特色":
                tags = section.select("span.recruit_features")
                scraped_data[section_title] = (
                    ", ".join([tag.get_text(strip=True) for tag in tags])
                    if tags
                    else "N/A"
                )

            # 應徵方式區塊 (需獨立處理，因其結構不同)
            elif section_title == "應徵方式":
                contact_item = section.select_one("ul > li")
                if contact_item and "連絡人" in contact_item.get_text():
                    key_tag = contact_item.select_one("span.left_title")
                    value_tag = contact_item.select_one("span.right_main")
                    if key_tag and value_tag:
                        key = key_tag.get_text(strip=True).replace("：", "")
                        scraped_data[key] = value_tag.get_text(strip=True)

        # ---  根據預設順序，建立一個新的、排序好的字典 ---

        DESIRED_ORDER = [
            "職缺網址",
            "職缺名稱",
            "公司名稱",
            "工作內容",
            "薪資待遇",
            "休假制度",
            "上班日期",
            "上班時段",
            "工作性質",
            "工作地點",
            "職務類別",
            "需求人數",
            "管理人數",
            "出差說明",
            "徵才特色",
            "學歷要求",
            "科系要求",
            "工作經驗",
            "身份類別",
            "法定保障",
            "保險福利",
            "獎金制度",
            "輔助津貼",
            "休閒娛樂",
            "福利設施",
            "其他福利",
            "更多福利",
            "電腦技能",
            "其他條件",
            "連絡人",
        ]

        ordered_details = {}
        for key in DESIRED_ORDER:
            # 如果暫存字典裡有這個 key，就加到新的有序字典裡
            if key in scraped_data:
                ordered_details[key] = scraped_data[key]

        # 為了保險起見，如果爬到了未在樣板中的新欄位，也把它們加到最後面
        for key, value in scraped_data.items():
            if key not in ordered_details:
                ordered_details[key] = value

        # df = pd.json_normalize(ordered_details)
        return ordered_details

    except Exception as e:
        print(f"處理過程中發生未知錯誤: {e}")
        return None


# # 測試範例
# job_url_to_scrape = "https://www.yes123.com.tw/wk_index/job.asp?p_id=1518627_23225023&job_id=20220120031002_8984866"
# job_data = fetch_yes123_job_data(job_url_to_scrape)
# pd.json_normalize(job_data)

# 根據關鍵字與職業類別 獲取所有工作職位的資料

import time
import logging
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Any

SEARCH_TIMESTAMP = time.strftime("%Y-%m-%d", time.localtime(time.time()))
JOBCAT_CODE = "2_1011_0001_0000"
KEYWORDS = "雲端工程師"
FILE_NAME = f"({SEARCH_TIMESTAMP})_{WEB_NAME}_{KEYWORDS}_{JOBCAT_CODE}"
MAX_WORKERS = 10  # 同時運行的執行緒數量

print(f"開始執行 {FILE_NAME}")
cata_url = catch_yes123_url(KEYWORDS, JOBCAT_CODE, ORDER="date")
job_urls = fetch_yes123_job_urls(cata_url)


job_data_list = []
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    # 建立 future 物件對應每個 URL 的爬取任務
    future_to_url = {
        executor.submit(fetch_yes123_job_data, url): url for url in job_urls
    }

    # 使用 tqdm 顯示進度條
    progress_bar = tqdm(
        as_completed(future_to_url), total=len(job_urls), desc="獲取yes123職缺資料"
    )
    for future in progress_bar:
        result = future.result()
        if result:  # 只有在成功爬取到資料時才加入列表
            job_data_list.append(result)


all_jobs_df = pd.DataFrame(job_data_list)

print(all_jobs_df.shape)

all_jobs_df.head(1)
# Output:
#   開始執行 (2025-06-17)_yes123_人力銀行_雲端工程師_2_1011_0001_0000

#   抓取頁面進度: 100%|██████████| 18/18 [00:17<00:00,  1.01頁/s]

#   獲取yes123職缺資料: 100%|██████████| 570/570 [00:46<00:00, 12.13it/s]
#   (570, 39)

#   

#                                                   職缺網址        職缺名稱      公司名稱  \

#   0  https://www.yes123.com.tw/wk_index/job.asp?p_i...  韌體工程師-(苗栗)  浩誠科技有限公司   

#   

#                                                   工作內容                薪資待遇  \

#   0  1.﻿﻿撰寫與設計韌體程式\n2.﻿﻿與客戶 討論規格，並執行韌體產品的開發流程\n3.協助...  薪資面議(經常性薪資達4萬元含以上)   

#   

#      休假制度       上班時段 工作性質                 工作地點   職務類別  ... 輔助津貼 休閒娛樂 其他福利 福利設施  \

#   0  週休二日  依公司規定、白天班   全職  苗栗縣苗栗市南勢里八甲192之2號2樓  韌體工程師  ...  NaN  NaN  NaN  NaN   

#   

#     更多福利 其他條件 交通工具 外語能力 上班制度 取得認證  

#   0  NaN  NaN  NaN  NaN  NaN  NaN  

#   

#   [1 rows x 39 columns]

# all_jobs_df.to_csv (f"{FILE_NAME}.csv", index=False, encoding='utf-8-sig')
# print (f"已將所有職缺資料儲存到 {FILE_NAME}.csv")

all_jobs_df.to_excel(f"{FILE_NAME}.xlsx", index=False)
print(f"已將所有職缺資料儲存到 {FILE_NAME}.xlsx")
# Output:
#   已將所有職缺資料儲存到 (2025-06-17)_yes123_人力銀行_雲端工程師_2_1011_0001_0000.xlsx


df_columns = pd.DataFrame(all_jobs_df.columns, columns=["欄位名稱"])
df_columns.insert(0, "序號", range(1, len(df_columns) + 1))
df_columns
# Output:
#       序號  欄位名稱

#   0    1  職缺網址

#   1    2  職缺名稱

#   2    3  公司名稱

#   3    4  工作內容

#   4    5  薪資待遇

#   5    6  休假制度

#   6    7  上班時段

#   7    8  工作性質

#   8    9  工作地點

#   9   10  職務類別

#   10  11  需求人數

#   11  12  管理人數

#   12  13  出差說明

#   13  14  學歷要求

#   14  15  工作經驗

#   15  16  身份類別

#   16  17  電腦技能

#   17  18   連絡人

#   18  19  職位等級

#   19  20  外派說明

#   20  21  方言能力

#   21  22  工作技能

#   22  23  具備駕照

#   23  24  上班日期

#   24  25  徵才特色

#   25  26  科系要求

#   26  27  法定保障

#   27  28  保險福利

#   28  29  獎金制度

#   29  30  輔助津貼

#   30  31  休閒娛樂

#   31  32  其他福利

#   32  33  福利設施

#   33  34  更多福利

#   34  35  其他條件

#   35  36  交通工具

#   36  37  外語能力

#   37  38  上班制度

#   38  39  取得認證



================================================
FILE: crawler/project_yes123/yes123_人力銀行_jobcat_json.txt
================================================
[
    {
        "level_1_name": "行政／總務",
        "list_2": [
            {
                "code": "2_1001_0001_0000",
                "level_2_name": "行政／總務全部"
            },
            {
                "code": "2_1001_0001_0001",
                "level_2_name": "行政主管"
            },
            {
                "code": "2_1001_0001_0002",
                "level_2_name": "總務主管"
            },
            {
                "code": "2_1001_0001_0003",
                "level_2_name": "行政人員"
            },
            {
                "code": "2_1001_0001_0004",
                "level_2_name": "總務人員"
            },
            {
                "code": "2_1001_0001_0005",
                "level_2_name": "總機人員"
            },
            {
                "code": "2_1001_0001_0006",
                "level_2_name": "櫃檯接待"
            },
            {
                "code": "2_1001_0001_0007",
                "level_2_name": "秘書"
            },
            {
                "code": "2_1001_0001_0008",
                "level_2_name": "文件管理師"
            },
            {
                "code": "2_1001_0001_0009",
                "level_2_name": "圖書資料管理人員"
            },
            {
                "code": "2_1001_0001_0010",
                "level_2_name": "資料輸入人員"
            },
            {
                "code": "2_1001_0001_0011",
                "level_2_name": "行政助理"
            },
            {
                "code": "2_1001_0001_0012",
                "level_2_name": "工讀生"
            }
        ]
    },
    {
        "level_1_name": "法務／智財",
        "list_2": [
            {
                "code": "2_1001_0002_0000",
                "level_2_name": "法務／智財全部"
            },
            {
                "code": "2_1001_0002_0001",
                "level_2_name": "法務／智財主管"
            },
            {
                "code": "2_1001_0002_0002",
                "level_2_name": "律師"
            },
            {
                "code": "2_1001_0002_0003",
                "level_2_name": "法務人員"
            },
            {
                "code": "2_1001_0002_0004",
                "level_2_name": "代書／地政士"
            },
            {
                "code": "2_1001_0002_0005",
                "level_2_name": "商標／專利人員"
            },
            {
                "code": "2_1001_0002_0006",
                "level_2_name": "工商登記人員"
            },
            {
                "code": "2_1001_0002_0007",
                "level_2_name": "法務助理"
            },
            {
                "code": "2_1001_0002_8001",
                "level_2_name": "其他法律專業人員"
            }
        ]
    },
    {
        "level_1_name": "金融／保險",
        "list_2": [
            {
                "code": "2_1002_0002_0000",
                "level_2_name": "金融／保險全部"
            },
            {
                "code": "2_1002_0002_0001",
                "level_2_name": "金融專業主管"
            },
            {
                "code": "2_1002_0002_0002",
                "level_2_name": "金融研究員"
            },
            {
                "code": "2_1002_0002_0003",
                "level_2_name": "金融交易員"
            },
            {
                "code": "2_1002_0002_0004",
                "level_2_name": "金融承銷員"
            },
            {
                "code": "2_1002_0002_0005",
                "level_2_name": "金融營業員"
            },
            {
                "code": "2_1002_0002_0006",
                "level_2_name": "金融理財專員"
            },
            {
                "code": "2_1002_0002_0007",
                "level_2_name": "銀行辦事員"
            },
            {
                "code": "2_1002_0002_0008",
                "level_2_name": "統計精算師"
            },
            {
                "code": "2_1002_0002_0009",
                "level_2_name": "不動產產權審核／估價師"
            },
            {
                "code": "2_1002_0002_0017",
                "level_2_name": "資產管理人員"
            },
            {
                "code": "2_1002_0002_0010",
                "level_2_name": "信用／融資業務人員"
            },
            {
                "code": "2_1002_0002_0011",
                "level_2_name": "帳款催收人員"
            },
            {
                "code": "2_1002_0002_0012",
                "level_2_name": "券商後線人員"
            },
            {
                "code": "2_1002_0002_0013",
                "level_2_name": "股務人員"
            },
            {
                "code": "2_1002_0002_0014",
                "level_2_name": "保險業務／經紀人"
            },
            {
                "code": "2_1002_0002_0015",
                "level_2_name": "核保／保險內勤人員"
            },
            {
                "code": "2_1002_0002_0016",
                "level_2_name": "理賠人員"
            },
            {
                "code": "2_1002_0002_0018",
                "level_2_name": "風險管理師"
            }
        ]
    },
    {
        "level_1_name": "財會／稅務",
        "list_2": [
            {
                "code": "2_1002_0001_0000",
                "level_2_name": "財會／稅務全部"
            },
            {
                "code": "2_1002_0001_0001",
                "level_2_name": "財務／會計主管"
            },
            {
                "code": "2_1002_0001_0002",
                "level_2_name": "會計師"
            },
            {
                "code": "2_1002_0001_0003",
                "level_2_name": "主辦會計"
            },
            {
                "code": "2_1002_0001_0004",
                "level_2_name": "成本會計"
            },
            {
                "code": "2_1002_0001_0005",
                "level_2_name": "會計／出納／記帳"
            },
            {
                "code": "2_1002_0001_0006",
                "level_2_name": "財務／分析人員"
            },
            {
                "code": "2_1002_0001_0007",
                "level_2_name": "查帳／審計人員"
            },
            {
                "code": "2_1002_0001_0008",
                "level_2_name": "稽核人員"
            },
            {
                "code": "2_1002_0001_0009",
                "level_2_name": "稅務人員"
            },
            {
                "code": "2_1002_0001_0010",
                "level_2_name": "財會助理"
            }
        ]
    },
    {
        "level_1_name": "人力資源",
        "list_2": [
            {
                "code": "2_1003_0001_0000",
                "level_2_name": "人力資源全部"
            },
            {
                "code": "2_1003_0001_0001",
                "level_2_name": "人力資源主管"
            },
            {
                "code": "2_1003_0001_0002",
                "level_2_name": "人力資源"
            },
            {
                "code": "2_1003_0001_0003",
                "level_2_name": "教育訓練"
            },
            {
                "code": "2_1003_0001_0004",
                "level_2_name": "人力招募／派遣／仲介"
            },
            {
                "code": "2_1003_0001_0005",
                "level_2_name": "外勞仲介"
            },
            {
                "code": "2_1003_0001_0006",
                "level_2_name": "人力資源助理"
            }
        ]
    },
    {
        "level_1_name": "經營／幕僚",
        "list_2": [
            {
                "code": "2_1003_0002_0000",
                "level_2_name": "經營／幕僚全部"
            },
            {
                "code": "2_1003_0002_0001",
                "level_2_name": "經營管理主管"
            },
            {
                "code": "2_1003_0002_0002",
                "level_2_name": "儲備幹部"
            },
            {
                "code": "2_1003_0002_0003",
                "level_2_name": "特別助理"
            }
        ]
    },
    {
        "level_1_name": "門市幹部",
        "list_2": [
            {
                "code": "2_1004_0001_0000",
                "level_2_name": "門市幹部全部"
            },
            {
                "code": "2_1004_0001_0001",
                "level_2_name": "店長／賣場管理人員"
            },
            {
                "code": "2_1004_0001_0002",
                "level_2_name": "連鎖店管理人員"
            },
            {
                "code": "2_1004_0001_0003",
                "level_2_name": "創業／經銷／加盟"
            }
        ]
    },
    {
        "level_1_name": "客戶服務",
        "list_2": [
            {
                "code": "2_1004_0002_0000",
                "level_2_name": "客戶服務全部"
            },
            {
                "code": "2_1004_0002_0001",
                "level_2_name": "客服主管"
            },
            {
                "code": "2_1004_0002_0002",
                "level_2_name": "門市／展示／專櫃／賣場人員"
            },
            {
                "code": "2_1004_0002_0003",
                "level_2_name": "收銀／售票員"
            },
            {
                "code": "2_1004_0002_0006",
                "level_2_name": "招生人員"
            },
            {
                "code": "2_1004_0002_0004",
                "level_2_name": "電話客服"
            },
            {
                "code": "2_1004_0002_0005",
                "level_2_name": "技術客服"
            },
            {
                "code": "2_1004_0002_0007",
                "level_2_name": "禮服秘書"
            },
            {
                "code": "2_1004_0002_8001",
                "level_2_name": "其他客服"
            }
        ]
    },
    {
        "level_1_name": "國際貿易",
        "list_2": [
            {
                "code": "2_1004_0004_0000",
                "level_2_name": "國際貿易全部"
            },
            {
                "code": "2_1004_0004_0001",
                "level_2_name": "國際貿易人員"
            },
            {
                "code": "2_1004_0004_0002",
                "level_2_name": "押匯／報關／船務人員"
            },
            {
                "code": "2_1004_0004_0003",
                "level_2_name": "保稅人員"
            }
        ]
    },
    {
        "level_1_name": "業務銷售",
        "list_2": [
            {
                "code": "2_1004_0003_0000",
                "level_2_name": "業務銷售全部"
            },
            {
                "code": "2_1004_0003_0001",
                "level_2_name": "國內業務主管"
            },
            {
                "code": "2_1004_0003_0002",
                "level_2_name": "國外業務主管"
            },
            {
                "code": "2_1004_0003_0003",
                "level_2_name": "專案業務主管"
            },
            {
                "code": "2_1004_0003_0004",
                "level_2_name": "產品事業群主管"
            },
            {
                "code": "2_1004_0003_0005",
                "level_2_name": "國內業務"
            },
            {
                "code": "2_1004_0003_0006",
                "level_2_name": "國外業務"
            },
            {
                "code": "2_1004_0003_0007",
                "level_2_name": "廣告AE"
            },
            {
                "code": "2_1004_0003_0008",
                "level_2_name": "電話行銷"
            },
            {
                "code": "2_1004_0003_0009",
                "level_2_name": "不動產經紀人／房屋仲介"
            },
            {
                "code": "2_1004_0003_0010",
                "level_2_name": "汽車銷售業務"
            },
            {
                "code": "2_1004_0003_0011",
                "level_2_name": "醫藥業務"
            },
            {
                "code": "2_1004_0003_0012",
                "level_2_name": "傳銷／直銷"
            },
            {
                "code": "2_1004_0003_0013",
                "level_2_name": "駐校代表"
            },
            {
                "code": "2_1004_0003_0014",
                "level_2_name": "業務助理"
            },
            {
                "code": "2_1004_0003_0015",
                "level_2_name": "國外業務助理"
            }
        ]
    },
    {
        "level_1_name": "美容美髮",
        "list_2": [
            {
                "code": "2_1005_0003_0000",
                "level_2_name": "美容美髮全部"
            },
            {
                "code": "2_1005_0003_0001",
                "level_2_name": "美容工作者"
            },
            {
                "code": "2_1005_0003_0002",
                "level_2_name": "美髮設計師"
            },
            {
                "code": "2_1005_0003_0003",
                "level_2_name": "造型師"
            },
            {
                "code": "2_1005_0003_0004",
                "level_2_name": "芳療／美療師"
            },
            {
                "code": "2_1005_0003_0005",
                "level_2_name": "指甲彩繪人員"
            },
            {
                "code": "2_1005_0003_0006",
                "level_2_name": "美姿美儀人員"
            },
            {
                "code": "2_1005_0003_0007",
                "level_2_name": "寵物美容專業人員"
            },
            {
                "code": "2_1005_0003_0008",
                "level_2_name": "美髮相關助理"
            },
            {
                "code": "2_1005_0003_0009",
                "level_2_name": "美容相關助理"
            },
            {
                "code": "2_1005_0003_0010",
                "level_2_name": "美睫師"
            },
            {
                "code": "2_1005_0003_0011",
                "level_2_name": "醫美諮詢師"
            }
        ]
    },
    {
        "level_1_name": "旅遊休閒",
        "list_2": [
            {
                "code": "2_1005_0001_0000",
                "level_2_name": "旅遊休閒全部"
            },
            {
                "code": "2_1005_0001_0001",
                "level_2_name": "旅遊休閒主管"
            },
            {
                "code": "2_1005_0001_0002",
                "level_2_name": "飯店主管"
            },
            {
                "code": "2_1005_0001_0003",
                "level_2_name": "飯店工作人員"
            },
            {
                "code": "2_1005_0001_0004",
                "level_2_name": "代辦OP／旅行社人員"
            },
            {
                "code": "2_1005_0001_0005",
                "level_2_name": "導遊"
            },
            {
                "code": "2_1005_0001_0006",
                "level_2_name": "領隊"
            },
            {
                "code": "2_1005_0001_0007",
                "level_2_name": "空服員"
            },
            {
                "code": "2_1005_0001_0008",
                "level_2_name": "地勤人員"
            },
            {
                "code": "2_1005_0001_0009",
                "level_2_name": "房務人員"
            }
        ]
    },
    {
        "level_1_name": "餐飲服務",
        "list_2": [
            {
                "code": "2_1005_0002_0000",
                "level_2_name": "餐飲服務全部"
            },
            {
                "code": "2_1005_0002_0001",
                "level_2_name": "餐廳主管"
            },
            {
                "code": "2_1005_0002_0002",
                "level_2_name": "中餐廚師"
            },
            {
                "code": "2_1005_0002_0003",
                "level_2_name": "西餐廚師"
            },
            {
                "code": "2_1005_0002_0004",
                "level_2_name": "西點／蛋糕師／各式點心師傅"
            },
            {
                "code": "2_1005_0002_0005",
                "level_2_name": "麵包師"
            },
            {
                "code": "2_1005_0002_0006",
                "level_2_name": "調酒師／吧台人員"
            },
            {
                "code": "2_1005_0002_0007",
                "level_2_name": "食品衛生管理師"
            },
            {
                "code": "2_1005_0002_0008",
                "level_2_name": "餐廚助手"
            },
            {
                "code": "2_1005_0002_0009",
                "level_2_name": "餐飲服務生"
            },
            {
                "code": "2_1005_0002_0010",
                "level_2_name": "日式料理廚師"
            },
            {
                "code": "2_1005_0002_0011",
                "level_2_name": "洗碗人員"
            },
            {
                "code": "2_1005_0002_0012",
                "level_2_name": "飲品調製販售員"
            },
            {
                "code": "2_1005_0002_8001",
                "level_2_name": "其他類廚師"
            }
        ]
    },
    {
        "level_1_name": "行銷／企劃",
        "list_2": [
            {
                "code": "2_1006_0001_0000",
                "level_2_name": "行銷／企劃全部"
            },
            {
                "code": "2_1006_0001_0001",
                "level_2_name": "行銷企劃主管"
            },
            {
                "code": "2_1006_0001_0002",
                "level_2_name": "產品企劃主管"
            },
            {
                "code": "2_1006_0001_0003",
                "level_2_name": "廣告企劃主管"
            },
            {
                "code": "2_1006_0001_0004",
                "level_2_name": "品牌宣傳／媒體公關主管"
            },
            {
                "code": "2_1006_0001_0005",
                "level_2_name": "行銷企劃"
            },
            {
                "code": "2_1006_0001_0006",
                "level_2_name": "活動企劃"
            },
            {
                "code": "2_1006_0001_0007",
                "level_2_name": "網站行銷企劃"
            },
            {
                "code": "2_1006_0001_0020",
                "level_2_name": "影音企劃"
            },
            {
                "code": "2_1006_0001_0018",
                "level_2_name": "遊戲企劃"
            },
            {
                "code": "2_1006_0001_0019",
                "level_2_name": "婚禮企劃"
            },
            {
                "code": "2_1006_0001_0008",
                "level_2_name": "產品行銷人員"
            },
            {
                "code": "2_1006_0001_0009",
                "level_2_name": "產品企劃／開發"
            },
            {
                "code": "2_1006_0001_0010",
                "level_2_name": "廣告文案／企劃"
            },
            {
                "code": "2_1006_0001_0011",
                "level_2_name": "傳媒企劃"
            },
            {
                "code": "2_1006_0001_0012",
                "level_2_name": "媒體購買"
            },
            {
                "code": "2_1006_0001_0013",
                "level_2_name": "媒體公關"
            },
            {
                "code": "2_1006_0001_0014",
                "level_2_name": "市場調查／市場分析"
            },
            {
                "code": "2_1006_0001_0015",
                "level_2_name": "發行／出版企劃"
            },
            {
                "code": "2_1006_0001_0016",
                "level_2_name": "不動產／商場開發人員"
            },
            {
                "code": "2_1006_0001_0017",
                "level_2_name": "行銷企劃助理"
            }
        ]
    },
    {
        "level_1_name": "專案管理",
        "list_2": [
            {
                "code": "2_1006_0002_0000",
                "level_2_name": "專案管理全部"
            },
            {
                "code": "2_1006_0002_0001",
                "level_2_name": "專案管理主管"
            },
            {
                "code": "2_1006_0002_0002",
                "level_2_name": "系統整合／ERP專案師／營運管理師"
            },
            {
                "code": "2_1006_0002_0003",
                "level_2_name": "軟體相關專案管理師"
            },
            {
                "code": "2_1006_0002_0004",
                "level_2_name": "產品管理師"
            },
            {
                "code": "2_1006_0002_0005",
                "level_2_name": "永續管理師"
            },
            {
                "code": "2_1006_0002_8001",
                "level_2_name": "其他專案管理師"
            }
        ]
    },
    {
        "level_1_name": "文字編譯",
        "list_2": [
            {
                "code": "2_1007_0003_0000",
                "level_2_name": "文字編譯全部"
            },
            {
                "code": "2_1007_0003_0001",
                "level_2_name": "排版人員"
            },
            {
                "code": "2_1007_0003_0002",
                "level_2_name": "技術文件／說明書編譯"
            },
            {
                "code": "2_1007_0003_0003",
                "level_2_name": "英文翻譯／口譯"
            },
            {
                "code": "2_1007_0003_0004",
                "level_2_name": "日文翻譯／口譯"
            },
            {
                "code": "2_1007_0003_0006",
                "level_2_name": "韓文翻譯"
            },
            {
                "code": "2_1007_0003_0007",
                "level_2_name": "印尼翻譯"
            },
            {
                "code": "2_1007_0003_0008",
                "level_2_name": "越南翻譯"
            },
            {
                "code": "2_1007_0003_0009",
                "level_2_name": "泰文翻譯"
            },
            {
                "code": "2_1007_0003_0005",
                "level_2_name": "文編／校對／文字工作者"
            },
            {
                "code": "2_1007_0003_8001",
                "level_2_name": "其他語文翻譯／口譯"
            }
        ]
    },
    {
        "level_1_name": "採訪類人員",
        "list_2": [
            {
                "code": "2_1007_0004_0000",
                "level_2_name": "採訪類人員全部"
            },
            {
                "code": "2_1007_0004_0001",
                "level_2_name": "記者／採編"
            },
            {
                "code": "2_1007_0004_8001",
                "level_2_name": "其他採訪相關工作"
            }
        ]
    },
    {
        "level_1_name": "設計人員",
        "list_2": [
            {
                "code": "2_1007_0001_0000",
                "level_2_name": "設計人員全部"
            },
            {
                "code": "2_1007_0001_0001",
                "level_2_name": "設計主管"
            },
            {
                "code": "2_1007_0001_0014",
                "level_2_name": "多媒體設計主管"
            },
            {
                "code": "2_1007_0001_0002",
                "level_2_name": "多媒體動畫設計師"
            },
            {
                "code": "2_1007_0001_0003",
                "level_2_name": "廣告設計"
            },
            {
                "code": "2_1007_0001_0017",
                "level_2_name": "視覺設計師"
            },
            {
                "code": "2_1007_0001_0018",
                "level_2_name": "UI/UX 設計師"
            },
            {
                "code": "2_1007_0001_0004",
                "level_2_name": "網頁設計師"
            },
            {
                "code": "2_1007_0001_0005",
                "level_2_name": "平面設計／美編"
            },
            {
                "code": "2_1007_0001_0006",
                "level_2_name": "美術設計"
            },
            {
                "code": "2_1007_0001_0007",
                "level_2_name": "服裝設計"
            },
            {
                "code": "2_1007_0001_0015",
                "level_2_name": "織品設計"
            },
            {
                "code": "2_1007_0001_0016",
                "level_2_name": "皮件／鞋類設計"
            },
            {
                "code": "2_1007_0001_0008",
                "level_2_name": "包裝設計"
            },
            {
                "code": "2_1007_0001_0009",
                "level_2_name": "商業設計"
            },
            {
                "code": "2_1007_0001_0010",
                "level_2_name": "工業設計"
            },
            {
                "code": "2_1007_0001_0011",
                "level_2_name": "電腦繪圖人員"
            },
            {
                "code": "2_1007_0001_0012",
                "level_2_name": "展場／櫥窗布置人員"
            },
            {
                "code": "2_1007_0001_0013",
                "level_2_name": "設計助理"
            }
        ]
    },
    {
        "level_1_name": "傳播藝術",
        "list_2": [
            {
                "code": "2_1007_0002_0000",
                "level_2_name": "傳播藝術全部"
            },
            {
                "code": "2_1007_0002_0001",
                "level_2_name": "節目製作人／廣告製片"
            },
            {
                "code": "2_1007_0002_0002",
                "level_2_name": "導演／導播"
            },
            {
                "code": "2_1007_0002_0003",
                "level_2_name": "攝影師"
            },
            {
                "code": "2_1007_0002_0004",
                "level_2_name": "燈光／音響師"
            },
            {
                "code": "2_1007_0002_0005",
                "level_2_name": "影片製作技術"
            },
            {
                "code": "2_1007_0002_0006",
                "level_2_name": "視聽工程人員"
            },
            {
                "code": "2_1007_0002_0007",
                "level_2_name": "播音／配音人員"
            },
            {
                "code": "2_1007_0002_0008",
                "level_2_name": "電台工作人員"
            },
            {
                "code": "2_1007_0002_0009",
                "level_2_name": "藝術指導／總監"
            },
            {
                "code": "2_1007_0002_0010",
                "level_2_name": "舞蹈／舞蹈指導"
            },
            {
                "code": "2_1007_0002_0011",
                "level_2_name": "音樂／作曲／演唱／演奏"
            },
            {
                "code": "2_1007_0002_0012",
                "level_2_name": "演員"
            },
            {
                "code": "2_1007_0002_0013",
                "level_2_name": "模特兒"
            },
            {
                "code": "2_1007_0002_0014",
                "level_2_name": "節目助理"
            },
            {
                "code": "2_1007_0002_0017",
                "level_2_name": "直播主"
            },
            {
                "code": "2_1007_0002_0015",
                "level_2_name": "攝影助理"
            },
            {
                "code": "2_1007_0002_0016",
                "level_2_name": "媒體或出版主管"
            },
            {
                "code": "2_1007_0002_8001",
                "level_2_name": "其他娛樂事業人員"
            }
        ]
    },
    {
        "level_1_name": "製圖／測量",
        "list_2": [
            {
                "code": "2_1008_0003_0000",
                "level_2_name": "製圖／測量全部"
            },
            {
                "code": "2_1008_0003_0001",
                "level_2_name": "CAD／CAM工程師"
            },
            {
                "code": "2_1008_0003_0002",
                "level_2_name": "建築設計／繪圖人員"
            },
            {
                "code": "2_1008_0003_8001",
                "level_2_name": "水電／其他工程繪圖人員"
            },
            {
                "code": "2_1008_0003_0004",
                "level_2_name": "室內設計／裝潢人員"
            },
            {
                "code": "2_1008_0003_0005",
                "level_2_name": "景觀設計師"
            },
            {
                "code": "2_1008_0003_0008",
                "level_2_name": "軟裝設計師"
            },
            {
                "code": "2_1008_0003_0006",
                "level_2_name": "機械設計／繪圖人員"
            },
            {
                "code": "2_1008_0003_0007",
                "level_2_name": "量測／儀校人員"
            }
        ]
    },
    {
        "level_1_name": "營建施作",
        "list_2": [
            {
                "code": "2_1008_0002_0000",
                "level_2_name": "營建施作全部"
            },
            {
                "code": "2_1008_0002_0001",
                "level_2_name": "營造工程師"
            },
            {
                "code": "2_1008_0002_0002",
                "level_2_name": "工地監工"
            },
            {
                "code": "2_1008_0002_0003",
                "level_2_name": "建築電力系統維修工"
            },
            {
                "code": "2_1008_0002_0004",
                "level_2_name": "金屬建材架構人員"
            },
            {
                "code": "2_1008_0002_0005",
                "level_2_name": "營建木工"
            },
            {
                "code": "2_1008_0002_0006",
                "level_2_name": "油漆工"
            },
            {
                "code": "2_1008_0002_0007",
                "level_2_name": "砌磚／砌石工"
            },
            {
                "code": "2_1008_0002_0008",
                "level_2_name": "混凝土工"
            },
            {
                "code": "2_1008_0002_0009",
                "level_2_name": "泥水工"
            },
            {
                "code": "2_1008_0002_0010",
                "level_2_name": "泥水小工相關工作者"
            },
            {
                "code": "2_1008_0002_0011",
                "level_2_name": "建築物清潔工"
            },
            {
                "code": "2_1008_0002_0012",
                "level_2_name": "水電工"
            },
            {
                "code": "2_1008_0002_0013",
                "level_2_name": "防水施工"
            },
            {
                "code": "2_1008_0002_0014",
                "level_2_name": "營建防鏽技師"
            },
            {
                "code": "2_1008_0002_0015",
                "level_2_name": "粗工"
            },
            {
                "code": "2_1008_0002_8001",
                "level_2_name": "其他營建相關工作"
            }
        ]
    },
    {
        "level_1_name": "營建規劃",
        "list_2": [
            {
                "code": "2_1008_0001_0000",
                "level_2_name": "營建規劃全部"
            },
            {
                "code": "2_1008_0001_0001",
                "level_2_name": "營建主管"
            },
            {
                "code": "2_1008_0001_0002",
                "level_2_name": "建築師"
            },
            {
                "code": "2_1008_0001_0003",
                "level_2_name": "都市／交通規劃"
            },
            {
                "code": "2_1008_0001_0004",
                "level_2_name": "營造土木技師／工程師"
            },
            {
                "code": "2_1008_0001_0005",
                "level_2_name": "結構技師／工程師"
            },
            {
                "code": "2_1008_0001_0006",
                "level_2_name": "水利工程師／技師"
            },
            {
                "code": "2_1008_0001_0007",
                "level_2_name": "水保工程師／技師"
            },
            {
                "code": "2_1008_0001_0011",
                "level_2_name": "水電工程師"
            },
            {
                "code": "2_1008_0001_0008",
                "level_2_name": "內業工程師"
            },
            {
                "code": "2_1008_0001_0009",
                "level_2_name": "工程配管繪圖"
            },
            {
                "code": "2_1008_0001_0010",
                "level_2_name": "工務人員／助理"
            },
            {
                "code": "2_1008_0001_0012",
                "level_2_name": "估算人員"
            },
            {
                "code": "2_1008_0001_0013",
                "level_2_name": "採購／發包人員"
            },
            {
                "code": "2_1008_0001_0014",
                "level_2_name": "土地開發人員"
            }
        ]
    },
    {
        "level_1_name": "工程研發",
        "list_2": [
            {
                "code": "2_1009_0001_0000",
                "level_2_name": "工程研發全部"
            },
            {
                "code": "2_1009_0001_0001",
                "level_2_name": "光電工程研發主管"
            },
            {
                "code": "2_1009_0001_0002",
                "level_2_name": "通訊工程研發主管"
            },
            {
                "code": "2_1009_0001_0003",
                "level_2_name": "硬體工程研發主管"
            },
            {
                "code": "2_1009_0001_0004",
                "level_2_name": "電機技師／工程師"
            },
            {
                "code": "2_1009_0001_0005",
                "level_2_name": "電子工程師"
            },
            {
                "code": "2_1009_0001_0006",
                "level_2_name": "機械工程師"
            },
            {
                "code": "2_1009_0001_0007",
                "level_2_name": "機構工程師"
            },
            {
                "code": "2_1009_0001_0008",
                "level_2_name": "機電技師／工程師"
            },
            {
                "code": "2_1009_0001_0009",
                "level_2_name": "零件工程師"
            },
            {
                "code": "2_1009_0001_0010",
                "level_2_name": "電腦硬體工程師"
            },
            {
                "code": "2_1009_0001_0011",
                "level_2_name": "PCB佈線工程師"
            },
            {
                "code": "2_1009_0001_0012",
                "level_2_name": "電源工程師"
            },
            {
                "code": "2_1009_0001_0013",
                "level_2_name": "類比IC設計工程師"
            },
            {
                "code": "2_1009_0001_0014",
                "level_2_name": "數位IC設計工程師"
            },
            {
                "code": "2_1009_0001_0015",
                "level_2_name": "半導體工程師"
            },
            {
                "code": "2_1009_0001_0016",
                "level_2_name": "微機電工程師"
            },
            {
                "code": "2_1009_0001_0017",
                "level_2_name": "光電工程師"
            },
            {
                "code": "2_1009_0001_0018",
                "level_2_name": "光學工程師"
            },
            {
                "code": "2_1009_0001_0019",
                "level_2_name": "通訊／電信系統工程師"
            },
            {
                "code": "2_1009_0001_0020",
                "level_2_name": "RF通訊工程師"
            },
            {
                "code": "2_1009_0001_0021",
                "level_2_name": "IC佈局工程師"
            },
            {
                "code": "2_1009_0001_0022",
                "level_2_name": "電子產品系統工程師"
            },
            {
                "code": "2_1009_0001_0023",
                "level_2_name": "太陽能技術工程師"
            },
            {
                "code": "2_1009_0001_0024",
                "level_2_name": "熱傳工程師"
            },
            {
                "code": "2_1009_0001_0025",
                "level_2_name": "聲學／噪音工程師"
            },
            {
                "code": "2_1009_0001_0026",
                "level_2_name": "助理工程師"
            },
            {
                "code": "2_1009_0001_0027",
                "level_2_name": "工程研發助理"
            },
            {
                "code": "2_1009_0001_8000",
                "level_2_name": "其他特殊工程師"
            },
            {
                "code": "2_1009_0001_8001",
                "level_2_name": "其他工程研發主管"
            }
        ]
    },
    {
        "level_1_name": "化工／材料研發",
        "list_2": [
            {
                "code": "2_1009_0002_0000",
                "level_2_name": "化工／材料研發全部"
            },
            {
                "code": "2_1009_0002_0001",
                "level_2_name": "化工化學工程師"
            },
            {
                "code": "2_1009_0002_0002",
                "level_2_name": "化學工程研發人員"
            },
            {
                "code": "2_1009_0002_0003",
                "level_2_name": "紡織化學工程師"
            },
            {
                "code": "2_1009_0002_0004",
                "level_2_name": "材料研發人員"
            },
            {
                "code": "2_1009_0002_0005",
                "level_2_name": "實驗化驗人員"
            },
            {
                "code": "2_1009_0002_0006",
                "level_2_name": "特用化學工程師"
            }
        ]
    },
    {
        "level_1_name": "生技研發",
        "list_2": [
            {
                "code": "2_1009_0003_0000",
                "level_2_name": "生技研發全部"
            },
            {
                "code": "2_1009_0003_0001",
                "level_2_name": "醫藥研發人員"
            },
            {
                "code": "2_1009_0003_0002",
                "level_2_name": "醫療工程／研發"
            },
            {
                "code": "2_1009_0003_0003",
                "level_2_name": "生物科技研發人員"
            },
            {
                "code": "2_1009_0003_0004",
                "level_2_name": "食品研發人員"
            },
            {
                "code": "2_1009_0003_0005",
                "level_2_name": "病理藥理研究人員"
            },
            {
                "code": "2_1009_0003_0006",
                "level_2_name": "農藝／畜產研究人員"
            },
            {
                "code": "2_1009_0003_0007",
                "level_2_name": "醫療器材研發人員"
            }
        ]
    },
    {
        "level_1_name": "生產管理",
        "list_2": [
            {
                "code": "2_1010_0001_0000",
                "level_2_name": "生產管理全部"
            },
            {
                "code": "2_1010_0001_0001",
                "level_2_name": "生產管理主管"
            },
            {
                "code": "2_1010_0001_0002",
                "level_2_name": "工廠主管"
            },
            {
                "code": "2_1010_0001_0003",
                "level_2_name": "工業／生產線規劃工程師"
            },
            {
                "code": "2_1010_0001_0004",
                "level_2_name": "生管人員"
            },
            {
                "code": "2_1010_0001_0005",
                "level_2_name": "生管助理"
            }
        ]
    },
    {
        "level_1_name": "品管／品保",
        "list_2": [
            {
                "code": "2_1010_0003_0000",
                "level_2_name": "品管／品保全部"
            },
            {
                "code": "2_1010_0003_0001",
                "level_2_name": "品管／品保主管"
            },
            {
                "code": "2_1010_0003_0002",
                "level_2_name": "檢驗／品管人員"
            },
            {
                "code": "2_1010_0003_0003",
                "level_2_name": "可靠度工程師"
            },
            {
                "code": "2_1010_0003_0004",
                "level_2_name": "測試工程師"
            },
            {
                "code": "2_1010_0003_0005",
                "level_2_name": "測試人員"
            },
            {
                "code": "2_1010_0003_0006",
                "level_2_name": "IC封裝／測試工程師"
            },
            {
                "code": "2_1010_0003_0007",
                "level_2_name": "軟韌體測試工程師"
            },
            {
                "code": "2_1010_0003_0008",
                "level_2_name": "EMC／電子安規工程師"
            },
            {
                "code": "2_1010_0003_0009",
                "level_2_name": "品保工程師"
            },
            {
                "code": "2_1010_0003_0010",
                "level_2_name": "ISO／品保人員"
            }
        ]
    },
    {
        "level_1_name": "製程規劃",
        "list_2": [
            {
                "code": "2_1010_0002_0000",
                "level_2_name": "製程規劃全部"
            },
            {
                "code": "2_1010_0002_0001",
                "level_2_name": "生產技術／製程工程師"
            },
            {
                "code": "2_1010_0002_0002",
                "level_2_name": "生產設備工程師"
            },
            {
                "code": "2_1010_0002_0003",
                "level_2_name": "自動控制工程師"
            },
            {
                "code": "2_1010_0002_0004",
                "level_2_name": "SMT工程師／技術員"
            },
            {
                "code": "2_1010_0002_0005",
                "level_2_name": "半導體製程工程師"
            },
            {
                "code": "2_1010_0002_0006",
                "level_2_name": "半導體設備工程師"
            },
            {
                "code": "2_1010_0002_0007",
                "level_2_name": "LCD製程工程師"
            },
            {
                "code": "2_1010_0002_0008",
                "level_2_name": "LCD設備工程師"
            }
        ]
    },
    {
        "level_1_name": "環境安全衛生",
        "list_2": [
            {
                "code": "2_1010_0004_0000",
                "level_2_name": "環境安全衛生全部"
            },
            {
                "code": "2_1010_0004_0001",
                "level_2_name": "勞工安全衛生管理師"
            },
            {
                "code": "2_1010_0004_0002",
                "level_2_name": "環境工程人員"
            },
            {
                "code": "2_1010_0004_0003",
                "level_2_name": "安全／衛生相關檢驗人員"
            },
            {
                "code": "2_1010_0004_0004",
                "level_2_name": "公共衛生人員"
            },
            {
                "code": "2_1010_0004_0005",
                "level_2_name": "防火／建築檢驗人員"
            },
            {
                "code": "2_1010_0004_0006",
                "level_2_name": "廠務管理人員"
            },
            {
                "code": "2_1010_0004_0007",
                "level_2_name": "廠務助理"
            }
        ]
    },
    {
        "level_1_name": "MIS／網管",
        "list_2": [
            {
                "code": "2_1011_0002_0000",
                "level_2_name": "MIS／網管全部"
            },
            {
                "code": "2_1011_0002_0001",
                "level_2_name": "MIS／網管主管"
            },
            {
                "code": "2_1011_0002_0002",
                "level_2_name": "資料庫管理"
            },
            {
                "code": "2_1011_0002_0003",
                "level_2_name": "MIS程式設計師"
            },
            {
                "code": "2_1011_0002_0004",
                "level_2_name": "網路管理工程師"
            },
            {
                "code": "2_1011_0002_0005",
                "level_2_name": "網路安全分析師"
            },
            {
                "code": "2_1011_0002_0006",
                "level_2_name": "MES工程師"
            },
            {
                "code": "2_1011_0002_0007",
                "level_2_name": "系統維護／操作人員"
            },
            {
                "code": "2_1011_0002_0008",
                "level_2_name": "資訊設備管制人員"
            },
            {
                "code": "2_1011_0002_0009",
                "level_2_name": "資安工程師"
            }
        ]
    },
    {
        "level_1_name": "軟體／工程",
        "list_2": [
            {
                "code": "2_1011_0001_0000",
                "level_2_name": "軟體／工程全部"
            },
            {
                "code": "2_1011_0001_0001",
                "level_2_name": "軟體專案主管"
            },
            {
                "code": "2_1011_0001_0002",
                "level_2_name": "電子商務主管"
            },
            {
                "code": "2_1011_0001_0003",
                "level_2_name": "網路程式設計師"
            },
            {
                "code": "2_1011_0001_0004",
                "level_2_name": "系統規劃分析師"
            },
            {
                "code": "2_1011_0001_0005",
                "level_2_name": "軟體工程師"
            },
            {
                "code": "2_1011_0001_0006",
                "level_2_name": "演算法開發工程師"
            },
            {
                "code": "2_1011_0001_0007",
                "level_2_name": "通訊軟體工程師"
            },
            {
                "code": "2_1011_0001_0008",
                "level_2_name": "韌體工程師"
            },
            {
                "code": "2_1011_0001_0009",
                "level_2_name": "BIOS工程師"
            },
            {
                "code": "2_1011_0001_0010",
                "level_2_name": "電玩程式設計師"
            },
            {
                "code": "2_1011_0001_0012",
                "level_2_name": "iOS工程師"
            },
            {
                "code": "2_1011_0001_0013",
                "level_2_name": "Android工程師"
            },
            {
                "code": "2_1011_0001_0014",
                "level_2_name": "全端工程師"
            },
            {
                "code": "2_1011_0001_0015",
                "level_2_name": "前端工程師"
            },
            {
                "code": "2_1011_0001_0016",
                "level_2_name": "後端工程師"
            },
            {
                "code": "2_1011_0001_0017",
                "level_2_name": "資料工程師"
            },
            {
                "code": "2_1011_0001_0018",
                "level_2_name": "數據分析師"
            },
            {
                "code": "2_1011_0001_0019",
                "level_2_name": "資料科學家"
            },
            {
                "code": "2_1011_0001_0011",
                "level_2_name": "資訊助理人員"
            },
            {
                "code": "2_1011_0001_8001",
                "level_2_name": "其他資訊專業人員"
            }
        ]
    },
    {
        "level_1_name": "維修／技術服務",
        "list_2": [
            {
                "code": "2_1012_0002_0000",
                "level_2_name": "維修／技術服務全部"
            },
            {
                "code": "2_1012_0002_0001",
                "level_2_name": "產品售後技術服務"
            },
            {
                "code": "2_1012_0002_0002",
                "level_2_name": "產品維修人員"
            },
            {
                "code": "2_1012_0002_0003",
                "level_2_name": "電腦組裝／測試人員"
            },
            {
                "code": "2_1012_0002_0004",
                "level_2_name": "通信測試／維修人員"
            },
            {
                "code": "2_1012_0002_0005",
                "level_2_name": "空調冷凍技術人員"
            },
            {
                "code": "2_1012_0002_0006",
                "level_2_name": "業務支援工程師"
            },
            {
                "code": "2_1012_0002_0007",
                "level_2_name": "FAE工程師"
            },
            {
                "code": "2_1012_0002_0008",
                "level_2_name": "汽車引擎技術人員"
            },
            {
                "code": "2_1012_0002_0009",
                "level_2_name": "汽車技術維修"
            },
            {
                "code": "2_1012_0002_0017",
                "level_2_name": "機車引擎技術人員"
            },
            {
                "code": "2_1012_0002_0018",
                "level_2_name": "機車技術維修"
            },
            {
                "code": "2_1012_0002_0010",
                "level_2_name": "電信／電力線路架設人員"
            },
            {
                "code": "2_1012_0002_0011",
                "level_2_name": "電話／電報機裝修人員"
            },
            {
                "code": "2_1012_0002_0012",
                "level_2_name": "電機設備裝修人員"
            },
            {
                "code": "2_1012_0002_0013",
                "level_2_name": "電子設備裝修工"
            },
            {
                "code": "2_1012_0002_0014",
                "level_2_name": "精密儀器製造／維修人員"
            },
            {
                "code": "2_1012_0002_0015",
                "level_2_name": "農業／工業用機器裝修人員"
            },
            {
                "code": "2_1012_0002_0016",
                "level_2_name": "飛機裝修人員"
            },
            {
                "code": "2_1012_0002_0019",
                "level_2_name": "其他汽車／機車技術維修人員"
            }
        ]
    },
    {
        "level_1_name": "操作／技術",
        "list_2": [
            {
                "code": "2_1012_0001_0000",
                "level_2_name": "操作／技術全部"
            },
            {
                "code": "2_1012_0001_0001",
                "level_2_name": "領班人員"
            },
            {
                "code": "2_1012_0001_0002",
                "level_2_name": "作業員／包裝員"
            },
            {
                "code": "2_1012_0001_0003",
                "level_2_name": "電機工程技術員"
            },
            {
                "code": "2_1012_0001_0004",
                "level_2_name": "CNC機台操作人員"
            },
            {
                "code": "2_1012_0001_0005",
                "level_2_name": "CNC程式編排人員"
            },
            {
                "code": "2_1012_0001_0043",
                "level_2_name": "CNC車床人員"
            },
            {
                "code": "2_1012_0001_0006",
                "level_2_name": "車床人員"
            },
            {
                "code": "2_1012_0001_0007",
                "level_2_name": "銑床人員"
            },
            {
                "code": "2_1012_0001_0008",
                "level_2_name": "沖壓模具技術人員"
            },
            {
                "code": "2_1012_0001_0009",
                "level_2_name": "塑膠模具技術人員"
            },
            {
                "code": "2_1012_0001_0010",
                "level_2_name": "塑膠射出技術人員"
            },
            {
                "code": "2_1012_0001_0011",
                "level_2_name": "機械加工技術人員"
            },
            {
                "code": "2_1012_0001_0012",
                "level_2_name": "印前製作／印刷技術人員"
            },
            {
                "code": "2_1012_0001_0013",
                "level_2_name": "噴漆技術員"
            },
            {
                "code": "2_1012_0001_0014",
                "level_2_name": "板金技術員"
            },
            {
                "code": "2_1012_0001_0015",
                "level_2_name": "PCB技術員"
            },
            {
                "code": "2_1012_0001_0016",
                "level_2_name": "焊接／切割技術員"
            },
            {
                "code": "2_1012_0001_0017",
                "level_2_name": "鑄造／鍛造模具技術人員"
            },
            {
                "code": "2_1012_0001_0018",
                "level_2_name": "粉末冶金模具技術人員"
            },
            {
                "code": "2_1012_0001_0019",
                "level_2_name": "壓鑄模具技術人員"
            },
            {
                "code": "2_1012_0001_0020",
                "level_2_name": "電鍍／表面處理技術人員"
            },
            {
                "code": "2_1012_0001_0021",
                "level_2_name": "精密拋光技術人員"
            },
            {
                "code": "2_1012_0001_0022",
                "level_2_name": "線切割技術員"
            },
            {
                "code": "2_1012_0001_0023",
                "level_2_name": "機械裝配員"
            },
            {
                "code": "2_1012_0001_0024",
                "level_2_name": "電機設備裝配員"
            },
            {
                "code": "2_1012_0001_0025",
                "level_2_name": "農業／林業設備操作員"
            },
            {
                "code": "2_1012_0001_0026",
                "level_2_name": "推土機相關設備操作員"
            },
            {
                "code": "2_1012_0001_0042",
                "level_2_name": "堆高機相關設備操作員"
            },
            {
                "code": "2_1012_0001_0027",
                "level_2_name": "吊車／起重機相關設備操作員"
            },
            {
                "code": "2_1012_0001_0028",
                "level_2_name": "紡織工務"
            },
            {
                "code": "2_1012_0001_0029",
                "level_2_name": "打版人員"
            },
            {
                "code": "2_1012_0001_0030",
                "level_2_name": "製鞋人員"
            },
            {
                "code": "2_1012_0001_0031",
                "level_2_name": "打樣人員"
            },
            {
                "code": "2_1012_0001_0032",
                "level_2_name": "裁縫／車縫人員"
            },
            {
                "code": "2_1012_0001_0033",
                "level_2_name": "樂器製造／調音員"
            },
            {
                "code": "2_1012_0001_0034",
                "level_2_name": "珠寶及貴金屬技術員"
            },
            {
                "code": "2_1012_0001_0040",
                "level_2_name": "裱框師父"
            },
            {
                "code": "2_1012_0001_0035",
                "level_2_name": "手工包裝加工／代工"
            },
            {
                "code": "2_1012_0001_0036",
                "level_2_name": "染整技術員"
            },
            {
                "code": "2_1012_0001_0037",
                "level_2_name": "鍋爐操作員"
            },
            {
                "code": "2_1012_0001_0038",
                "level_2_name": "塗裝技術員"
            },
            {
                "code": "2_1012_0001_0039",
                "level_2_name": "雷射操作技術員"
            },
            {
                "code": "2_1012_0001_0041",
                "level_2_name": "防鏽技師"
            },
            {
                "code": "2_1012_0001_0044",
                "level_2_name": "學徒"
            },
            {
                "code": "2_1012_0001_8001",
                "level_2_name": "其他機械操作員"
            }
        ]
    },
    {
        "level_1_name": "倉管／資材／採購",
        "list_2": [
            {
                "code": "2_1013_0001_0000",
                "level_2_name": "倉管／資材／採購全部"
            },
            {
                "code": "2_1013_0001_0001",
                "level_2_name": "資材主管"
            },
            {
                "code": "2_1013_0001_0002",
                "level_2_name": "採購主管"
            },
            {
                "code": "2_1013_0001_0003",
                "level_2_name": "倉儲管理人員"
            },
            {
                "code": "2_1013_0001_0004",
                "level_2_name": "採購／發包人員"
            },
            {
                "code": "2_1013_0001_0005",
                "level_2_name": "物料／資材管理"
            },
            {
                "code": "2_1013_0001_0006",
                "level_2_name": "採購助理"
            }
        ]
    },
    {
        "level_1_name": "運輸物流",
        "list_2": [
            {
                "code": "2_1013_0002_0000",
                "level_2_name": "運輸物流全部"
            },
            {
                "code": "2_1013_0002_0001",
                "level_2_name": "運輸物流主管"
            },
            {
                "code": "2_1013_0002_0002",
                "level_2_name": "運輸交通專業人員"
            },
            {
                "code": "2_1013_0002_0003",
                "level_2_name": "快遞／送貨／外務"
            },
            {
                "code": "2_1013_0002_0004",
                "level_2_name": "倉儲物流人員"
            },
            {
                "code": "2_1013_0002_0005",
                "level_2_name": "小客車／計程車／小貨車司機"
            },
            {
                "code": "2_1013_0002_0010",
                "level_2_name": "大客車司機"
            },
            {
                "code": "2_1013_0002_0012",
                "level_2_name": "聯結車司機"
            },
            {
                "code": "2_1013_0002_0006",
                "level_2_name": "鐵路／捷運駕駛員"
            },
            {
                "code": "2_1013_0002_0007",
                "level_2_name": "船長／大副／船員"
            },
            {
                "code": "2_1013_0002_0008",
                "level_2_name": "飛行機師"
            },
            {
                "code": "2_1013_0002_0009",
                "level_2_name": "飛安人員"
            },
            {
                "code": "2_1013_0002_8001",
                "level_2_name": "大貨車／其他司機"
            },
            {
                "code": "2_1013_0002_0011",
                "level_2_name": "隨車員"
            }
        ]
    },
    {
        "level_1_name": "醫療／保健服務",
        "list_2": [
            {
                "code": "2_1014_0002_0000",
                "level_2_name": "醫療／保健服務全部"
            },
            {
                "code": "2_1014_0002_0001",
                "level_2_name": "醫院行政人員"
            },
            {
                "code": "2_1014_0002_0002",
                "level_2_name": "看護"
            },
            {
                "code": "2_1014_0002_0003",
                "level_2_name": "按摩／推拿師"
            },
            {
                "code": "2_1014_0002_0004",
                "level_2_name": "診所助理"
            },
            {
                "code": "2_1014_0002_0005",
                "level_2_name": "牙醫助理"
            },
            {
                "code": "2_1014_0002_0008",
                "level_2_name": "牙體技術師／齒模師"
            },
            {
                "code": "2_1014_0002_0006",
                "level_2_name": "醫療設備控制人員"
            },
            {
                "code": "2_1014_0002_0007",
                "level_2_name": "放射性設備人員"
            },
            {
                "code": "2_1014_0002_0009",
                "level_2_name": "照顧服務員"
            },
            {
                "code": "2_1014_0002_0010",
                "level_2_name": "居家服務督導員"
            },
            {
                "code": "2_1014_0002_0011",
                "level_2_name": "居家護理師"
            },
            {
                "code": "2_1014_0002_0012",
                "level_2_name": "個案管理師"
            },
            {
                "code": "2_1014_0002_8001",
                "level_2_name": "其他醫療保健人員"
            }
        ]
    },
    {
        "level_1_name": "醫療專業",
        "list_2": [
            {
                "code": "2_1014_0001_0000",
                "level_2_name": "醫療專業全部"
            },
            {
                "code": "2_1014_0001_0001",
                "level_2_name": "各專科醫師"
            },
            {
                "code": "2_1014_0001_0002",
                "level_2_name": "中醫師"
            },
            {
                "code": "2_1014_0001_0003",
                "level_2_name": "牙醫師"
            },
            {
                "code": "2_1014_0001_0021",
                "level_2_name": "整型醫師"
            },
            {
                "code": "2_1014_0001_0004",
                "level_2_name": "麻醉醫師"
            },
            {
                "code": "2_1014_0001_0005",
                "level_2_name": "公共衛生醫師"
            },
            {
                "code": "2_1014_0001_0007",
                "level_2_name": "藥師"
            },
            {
                "code": "2_1014_0001_0015",
                "level_2_name": "藥學助理"
            },
            {
                "code": "2_1014_0001_0008",
                "level_2_name": "醫事檢驗師"
            },
            {
                "code": "2_1014_0001_0009",
                "level_2_name": "醫事放射師"
            },
            {
                "code": "2_1014_0001_0010",
                "level_2_name": "復建技術師"
            },
            {
                "code": "2_1014_0001_0016",
                "level_2_name": "語言治療師"
            },
            {
                "code": "2_1014_0001_0017",
                "level_2_name": "物理治療師"
            },
            {
                "code": "2_1014_0001_0018",
                "level_2_name": "呼吸治療師"
            },
            {
                "code": "2_1014_0001_0019",
                "level_2_name": "職能治療師"
            },
            {
                "code": "2_1014_0001_0023",
                "level_2_name": "心理師"
            },
            {
                "code": "2_1014_0001_0011",
                "level_2_name": "治療師"
            },
            {
                "code": "2_1014_0001_0014",
                "level_2_name": "護理師及護士"
            },
            {
                "code": "2_1014_0001_0020",
                "level_2_name": "專科護理師"
            },
            {
                "code": "2_1014_0001_0022",
                "level_2_name": "勞工健康服務護理人員"
            },
            {
                "code": "2_1014_0001_0013",
                "level_2_name": "獸醫師"
            },
            {
                "code": "2_1014_0001_0024",
                "level_2_name": "獸醫助理"
            },
            {
                "code": "2_1014_0001_0006",
                "level_2_name": "營養師"
            },
            {
                "code": "2_1014_0001_0012",
                "level_2_name": "驗光師"
            },
            {
                "code": "2_1014_0001_8001",
                "level_2_name": "其他醫療人員"
            }
        ]
    },
    {
        "level_1_name": "才藝輔導",
        "list_2": [
            {
                "code": "2_1015_0001_0000",
                "level_2_name": "才藝輔導全部"
            },
            {
                "code": "2_1015_0001_0001",
                "level_2_name": "珠心算老師"
            },
            {
                "code": "2_1015_0001_0002",
                "level_2_name": "數學老師"
            },
            {
                "code": "2_1015_0001_0003",
                "level_2_name": "美術老師"
            },
            {
                "code": "2_1015_0001_0004",
                "level_2_name": "音樂老師"
            },
            {
                "code": "2_1015_0001_0005",
                "level_2_name": "助教"
            },
            {
                "code": "2_1015_0001_8001",
                "level_2_name": "其他才藝老師"
            }
        ]
    },
    {
        "level_1_name": "教育／輔導",
        "list_2": [
            {
                "code": "2_1015_0002_0000",
                "level_2_name": "教育／輔導全部"
            },
            {
                "code": "2_1015_0002_0001",
                "level_2_name": "幼教班老師"
            },
            {
                "code": "2_1015_0002_0002",
                "level_2_name": "安親班老師"
            },
            {
                "code": "2_1015_0002_0003",
                "level_2_name": "補習班導師／管理人員"
            },
            {
                "code": "2_1015_0002_0004",
                "level_2_name": "電腦補習班老師"
            },
            {
                "code": "2_1015_0002_0005",
                "level_2_name": "英文老師"
            },
            {
                "code": "2_1015_0002_0006",
                "level_2_name": "日文老師"
            },
            {
                "code": "2_1015_0002_0007",
                "level_2_name": "其他語文補習班老師"
            },
            {
                "code": "2_1015_0002_0008",
                "level_2_name": "升學補習班老師"
            },
            {
                "code": "2_1015_0002_0009",
                "level_2_name": "其他補習班老師"
            },
            {
                "code": "2_1015_0002_0021",
                "level_2_name": "課輔老師"
            },
            {
                "code": "2_1015_0002_0010",
                "level_2_name": "國小學校教師"
            },
            {
                "code": "2_1015_0002_0011",
                "level_2_name": "中等學校教師"
            },
            {
                "code": "2_1015_0002_0012",
                "level_2_name": "教授／副教授／助理教授"
            },
            {
                "code": "2_1015_0002_0013",
                "level_2_name": "講師"
            },
            {
                "code": "2_1015_0002_0019",
                "level_2_name": "大專院校講師"
            },
            {
                "code": "2_1015_0002_0014",
                "level_2_name": "大專院校助教"
            },
            {
                "code": "2_1015_0002_0015",
                "level_2_name": "特殊教育老師"
            },
            {
                "code": "2_1015_0002_0016",
                "level_2_name": "運動教練"
            },
            {
                "code": "2_1015_0002_0017",
                "level_2_name": "社工人員"
            },
            {
                "code": "2_1015_0002_0018",
                "level_2_name": "保育員／教保員"
            },
            {
                "code": "2_1015_0002_0020",
                "level_2_name": "托育人員"
            },
            {
                "code": "2_1015_0002_8001",
                "level_2_name": "其他教育輔導人員"
            }
        ]
    },
    {
        "level_1_name": "學術研究",
        "list_2": [
            {
                "code": "2_1015_0003_0000",
                "level_2_name": "學術研究全部"
            },
            {
                "code": "2_1015_0003_0001",
                "level_2_name": "數學研究員"
            },
            {
                "code": "2_1015_0003_0002",
                "level_2_name": "統計研究員"
            },
            {
                "code": "2_1015_0003_0003",
                "level_2_name": "氣象相關研究員"
            },
            {
                "code": "2_1015_0003_0004",
                "level_2_name": "天文相關研究員"
            },
            {
                "code": "2_1015_0003_0005",
                "level_2_name": "地質／地球科學研究員"
            },
            {
                "code": "2_1015_0003_0006",
                "level_2_name": "物理相關研究員"
            },
            {
                "code": "2_1015_0003_0007",
                "level_2_name": "化學相關研究員"
            },
            {
                "code": "2_1015_0003_0008",
                "level_2_name": "生物學研究員"
            },
            {
                "code": "2_1015_0003_0009",
                "level_2_name": "應用科學研究員"
            },
            {
                "code": "2_1015_0003_0010",
                "level_2_name": "心理學研究員"
            },
            {
                "code": "2_1015_0003_0011",
                "level_2_name": "政治／歷史／哲學相關研究員"
            },
            {
                "code": "2_1015_0003_0012",
                "level_2_name": "人類／社會學研究員"
            },
            {
                "code": "2_1015_0003_0013",
                "level_2_name": "研究助理"
            },
            {
                "code": "2_1015_0003_8001",
                "level_2_name": "其他研究人員"
            }
        ]
    },
    {
        "level_1_name": "保全／樓管",
        "list_2": [
            {
                "code": "2_1016_0001_0000",
                "level_2_name": "保全／樓管全部"
            },
            {
                "code": "2_1016_0001_0001",
                "level_2_name": "大樓管理員／總幹事"
            },
            {
                "code": "2_1016_0001_0002",
                "level_2_name": "保全人員／警衛"
            },
            {
                "code": "2_1016_0001_0003",
                "level_2_name": "保全技術人員"
            },
            {
                "code": "2_1016_0001_0004",
                "level_2_name": "社區秘書"
            },
            {
                "code": "2_1016_0001_8001",
                "level_2_name": "其他保全／樓管相關工作"
            }
        ]
    },
    {
        "level_1_name": "消防／軍警",
        "list_2": [
            {
                "code": "2_1016_0002_0000",
                "level_2_name": "消防／軍警全部"
            },
            {
                "code": "2_1016_0002_0001",
                "level_2_name": "消防員"
            },
            {
                "code": "2_1016_0002_0002",
                "level_2_name": "消防專業人員"
            },
            {
                "code": "2_1016_0002_0003",
                "level_2_name": "志願役軍官／士官／士兵"
            },
            {
                "code": "2_1016_0002_0004",
                "level_2_name": "救生員"
            }
        ]
    },
    {
        "level_1_name": "其他職務人員",
        "list_2": [
            {
                "code": "2_1017_0001_0000",
                "level_2_name": "其他職務人員全部"
            },
            {
                "code": "2_1017_0001_0001",
                "level_2_name": "顧問人員"
            },
            {
                "code": "2_1017_0001_0002",
                "level_2_name": "星象占卜人員"
            },
            {
                "code": "2_1017_0001_0003",
                "level_2_name": "藝術品／珠寶鑑價／拍賣顧問"
            },
            {
                "code": "2_1017_0001_0004",
                "level_2_name": "生命禮儀師"
            },
            {
                "code": "2_1017_0001_0005",
                "level_2_name": "志工人員"
            },
            {
                "code": "2_1017_0001_0006",
                "level_2_name": "公家機關相關人員"
            },
            {
                "code": "2_1017_0001_0007",
                "level_2_name": "清潔人員／資源回收人員"
            },
            {
                "code": "2_1017_0001_0014",
                "level_2_name": "汽車美容"
            },
            {
                "code": "2_1017_0001_0008",
                "level_2_name": "家庭代工"
            },
            {
                "code": "2_1017_0001_0009",
                "level_2_name": "生鮮人員"
            },
            {
                "code": "2_1017_0001_0010",
                "level_2_name": "加油員"
            },
            {
                "code": "2_1017_0001_0011",
                "level_2_name": "派報／傳單"
            },
            {
                "code": "2_1017_0001_0012",
                "level_2_name": "花藝／園藝人員"
            },
            {
                "code": "2_1017_0001_0013",
                "level_2_name": "管家／家事服務／清潔"
            },
            {
                "code": "2_1017_0001_0015",
                "level_2_name": "泊車人員"
            }
        ]
    },
    {
        "level_1_name": "農林漁牧相關",
        "list_2": [
            {
                "code": "2_1017_0002_0000",
                "level_2_name": "農林漁牧相關全部"
            },
            {
                "code": "2_1017_0002_0001",
                "level_2_name": "農作物栽培"
            },
            {
                "code": "2_1017_0002_0002",
                "level_2_name": "動物飼育"
            },
            {
                "code": "2_1017_0002_0003",
                "level_2_name": "林木伐運"
            },
            {
                "code": "2_1017_0002_0004",
                "level_2_name": "水產養殖"
            }
        ]
    }
]


================================================
FILE: crawler/utils/clean_text.py
================================================
import re
from html import unescape

def clean_text(text: str) -> str:
    """
    Cleans the input text by removing HTML tags and decoding HTML entities.
    """
    if not isinstance(text, str):
        return text
    # Remove HTML tags
    cleaned_text = re.sub(r'<[^>]*>', '', text)
    # Decode HTML entities
    cleaned_text = unescape(cleaned_text)
    return cleaned_text.strip()



================================================
FILE: crawler/utils/salary_parser.py
================================================
[Binary file]


================================================
FILE: docs/development_manual.md
================================================
# Crawler System 開發手冊

## 1. 總體哲學 (Philosophy)

本專案所有開發與重構工作，應遵循以下核心哲學：

- **清晰性 (Clarity)**：程式碼首先是寫給人看的，其次才是給機器執行的。優先選擇清晰、易於理解的寫法，避免過度炫技或使用晦澀的語法。
- **單一職責 (Single Responsibility)**：每個模組、每個類別、每個函式都應該只有一個明確的職責。這使得程式碼更容易測試、重用和維護。
- **穩定性 (Robustness)**：應用程式應具備容錯能力，並透過**嚴格的測試**來確保其穩定性。對於外部依賴（如資料庫、訊息佇列），必須有適當的重試和錯誤處理機制。
- **配置外部化 (Externalized Configuration)**：程式碼本身不應包含任何環境特定的設定（如密碼、主機位址）。所有設定應透過外部設定檔管理。

---

## 2. 環境設定 (Environment Setup)

### 2.1. 必要工具

- **Python**: 版本定義於 `.python-version`。
- **uv**: 用於管理 Python 虛擬環境和套件。
- **Docker & Docker Compose**: 用於啟動外部服務（MySQL, RabbitMQ）。

### 2.2. 初始化步驟

1.  **初始化專案 (使用 uv)**:
    ```bash
    uv init
    ```

2.  **啟動基礎服務**: 
    ```bash
    # 啟動 MySQL 和 RabbitMQ 服務
    docker-compose -f mysql-network.yml up -d
    docker-compose -f rabbitmq-network.yml up -d
    ```

3.  **建立虛擬環境並安裝依賴**: 
    ```bash
    # 建立 .venv 虛擬環境
    uv venv

    # 啟用虛擬環境
    source .venv/bin/activate

    # 安裝專案依賴
    uv pip install -r requirements.txt
    ```

3.  **設定環境變數**: 
    複製 `local.ini.example` (如果有的話) 為 `local.ini`，並根據本地開發需求修改。`APP_ENV` 環境變數用於切換不同的設定區塊。

---

## 3. 設定檔管理 (Configuration)

- **`local.ini`**: 這是唯一的設定來源 (Single Source of Truth)。它被分為不同的區塊，例如 `[DEV]`, `[DOCKER]`, `[PROD]`。
- **`crawler/config.py`**: 這是讀取 `local.ini` 的唯一模組。專案中任何其他地方需要設定值時，都應該**直接從 `crawler.config` 匯入**，而不是自己重新讀取 `.ini` 檔案。
- **`APP_ENV` 環境變數**: 這個環境變數決定了 `config.py` 要讀取 `local.ini` 中的哪一個區塊。預設為 `DOCKER`。

### 3.1. 核心爬蟲設定

以下是 `local.ini` 中一些影響爬蟲行為的關鍵設定：

-   **`PRODUCER_BATCH_SIZE`**:
    -   **作用**: 限制 Producer (例如 `producer_jobs_104`) 每次從資料庫讀取並分發的任務數量。
    -   **用途**: 在本地測試時，可以設定較小的值 (例如 `20`)，以控制單次測試的資料量，避免一次性處理過多任務。在生產環境中，可以根據系統資源和任務量設定較大的值。

-   **`PRODUCER_DISPATCH_INTERVAL_SECONDS`**:
    -   **作用**: 設定**持續運行**的 Producer (使用 `while True` 迴圈) 在分發完一批任務後，等待多久才開始下一批。
    -   **用途**: 用於控制常駐型 Producer 的任務分發頻率。對於**排程驅動、單次執行**的 Producer (如目前的 `producer_jobs_104`)，此設定**無效**，因為腳本執行完畢後即會終止。

-   **`URL_CRAWLER_SLEEP_MIN_SECONDS` / `URL_CRAWLER_SLEEP_MAX_SECONDS`**:
    -   **作用**: 定義 Worker 在每次發送 API 請求前，隨機暫停的最小和最大秒數。
    -   **用途**: 這是**避免被目標網站 API 封鎖的關鍵設定**。透過引入隨機延遲，模擬人類的瀏覽行為，降低請求頻率，有效規避 `429 Too Many Requests` 等反爬機制。在測試或生產環境中，應根據目標網站的限制策略進行調整。

---

## 4. 核心編碼原則 (Core Coding Principles)

### 4.1. 單一職責原則 (SRP)

- **模組層級**: `crawler/database/connection.py` 只負責資料庫連接，`crawler/config.py` 只負責設定讀取。
- **函式層級**: 一個函式只做一件事情。例如，`get_engine` 只負責取得引擎，而不應該包含建立資料庫的邏輯。

### 4.2. DRY (Don't Repeat Yourself)

- **避免重複程式碼**: 如果一段邏輯在兩個以上的地方出現，就應該將它抽像成一個函式或類別。
- **範例**: 專案中所有讀取設定的地方都應從 `crawler.config` 匯入，這就是 DRY 的體現。

### 4.3. 日誌記錄 (Logging)

- **使用 `structlog`**: 全面使用 `structlog` 進行結構化日誌記錄，而不是 `print()`。
- **日誌級別**:
    - `logger.debug()`: 用於開發時的詳細除錯資訊。
    - `logger.info()`: 用於記錄關鍵的業務流程節點（例如「服務啟動」、「收到新任務」）。
    - `logger.warning()`: 用於記錄可預期的、但需要注意的異常情況（例如「設定檔缺少某個非關鍵值，使用預設值」）。
    - `logger.error()`: 用於記錄發生了錯誤，但應用程式仍可繼續運行的情況（例如「處理單一任務失敗，但 worker 會繼續接收下一個任務」）。
    - `logger.critical()`: 用於記錄導致應用程式無法繼續運行的致命錯誤（例如「資料庫連接失敗」）。
- **包含上下文**: 在記錄日誌時，盡可能帶上關鍵的上下文資訊，例如 `logger.info("任務處理完成", task_id=123, duration_ms=500)`。

---

## 5. 測試策略 (Testing Strategy)

本專案高度重視程式碼品質與穩定性，因此測試是開發流程中不可或缺的一環。所有程式碼在提交前都必須經過適當的測試。

### 5.1. 本地測試 (Local Testing)

為了方便在本地開發環境中快速驗證 `task` 的功能，我們採用了基於環境變數的測試模式。

- **核心機制**: 透過在執行腳本前設定 `CRAWLER_DB_NAME` 環境變數，可以讓應用程式連線到指定的測試資料庫（例如 `test_db`），而不是 `local.ini` 中設定的預設資料庫。
- **標準實踐**: 在每個 `task_*.py` 檔案的頂部，都加入了以下標準化的程式碼區塊：
  ```python
  import os
  if __name__ == "__main__":
      os.environ['CRAWLER_DB_NAME'] = 'test_db'
  ```
- **如何運作**: 當你使用 `python -m crawler.project_xxx.task_yyy` 執行一個 `task` 檔案時，這個區塊會被觸發，從而確保所有後續的資料庫操作都發生在 `test_db` 中。這使得本地測試既簡單又安全，完全不會影響到正式的開發資料庫。

### 5.2. 測試工具

-   **`pytest`**: 作為主要的測試框架，提供豐富的功能和靈活的擴展性。
-   **`unittest.mock`**: 用於單元測試中模擬外部依賴，確保測試的隔離性。

### 5.3. 測試流程與規範

1.  **編寫測試**：為新功能或修復的 Bug 編寫相應的測試案例。
2.  **運行測試**：在提交程式碼前，必須運行所有相關測試，並確保其通過。
    ```bash
    # 運行所有測試 (如果配置正確)
    python -m pytest

    # 運行特定測試檔案
    python -m pytest tests/path/to/your_test_file.py
    ```
3.  **測試覆蓋率**：鼓勵提高測試覆蓋率，但更重要的是測試的品質和有效性。

---

## 6. 資料庫互動 (Database Interaction)

本專案的資料庫互動遵循清晰的結構化原則，以確保程式碼的可維護性和資料的完整性。

### 6.1. 模組職責

- **`connection.py`**: 唯一的資料庫連線管理模組。它會根據 `CRAWLER_DB_NAME` 環境變數自動判斷應連線至正式資料庫還是測試資料庫。所有資料庫 Session 的取得都必須透過此模組的 `get_session()`。
- **`models.py`**: 只包含 SQLAlchemy 的 ORM 模型定義（例如 `Job`, `Url` 等），負責定義資料庫的資料表結構。
- **`schemas.py`**: 只包含 Pydantic 的資料驗證模型（例如 `JobPydantic`, `UrlPydantic`），負責定義應用程式內部流動的資料結構，並提供資料驗證。
- **`repository.py`**: 資料庫操作的唯一入口（Repository Pattern）。所有對資料庫的 CRUD (Create, Read, Update, Delete) 操作都應封裝在此模組的函式中。

### 6.2. ORM 與 Session 管理

-   **使用 `get_session`**: 所有對資料庫的讀寫操作，都必須透過 `crawler.database.connection.get_session` 的上下文管理器來完成。
    ```python
    from crawler.database.connection import get_session
    from crawler.database.models import MyORMModel
    from crawler.database.schemas import MyPydanticModel

    with get_session() as session:
        # 從資料庫讀取資料後，應立即轉換為 Pydantic 模型
        orm_object = session.query(MyORMModel).first()
        if orm_object:
            pydantic_instance = MyPydanticModel.model_validate(orm_object)
            # 現在你可以安全地使用 pydantic_instance

        # 寫入資料庫時，應使用 Pydantic 模型定義的資料
        new_data = MyPydanticModel(field1="value1", field2="value2")
        session.add(MyORMModel(**new_data.model_dump())) # 將 Pydantic 轉換為 ORM 可接受的格式
        # session.commit() 和 session.rollback() 會由 get_session 自動處理
    ```
-   **`tb_jobs` 唯一約束**: `tb_jobs` 表格現在包含 `(source_platform, url)` 的唯一約束。這確保了對於同一平台和 URL 的職缺，資料庫中只會保留一筆最新記錄，避免重複數據。當插入重複的 `(source_platform, url)` 組合時，現有記錄將會被更新。
-   **禁止直接使用 `engine.execute()`**: 除非是像 `initialize_database` 這樣的一次性管理腳本，否則業務邏輯中應避免直接使用 `engine`。

### 6.3. 爬取狀態生命週期 (Crawl Status Lifecycle)

為了確保任務不被重複抓取且具備重試能力，URL 的爬取狀態遵循以下生命週期：

1.  **`PENDING`**:
    -   **定義**: URL 已被收集，等待 Producer 分發。這是 URL 的初始狀態。
    -   **觸發**: `producer_category_*` 或 `producer_urls_*` 首次將 URL 存入資料庫時。

2.  **`QUEUED`**:
    -   **定義**: Producer 已從資料庫讀取此 URL，並準備將其作為任務發送到訊息佇列 (RabbitMQ)。
    -   **觸發**: `producer_jobs_*` 讀取到 `PENDING` 或 `FAILED` 的 URL 後，會**立即**將其狀態更新為 `QUEUED`，以防止其他 Producer 實例重複選取。

3.  **`PROCESSING`**:
    -   **定義**: Worker 已從訊息佇列接收到任務，正在進行資料抓取和處理。
    -   **觸發**: `worker.py` 中的 `fetch_url_data_*` 任務開始執行時，會將 URL 狀態更新為 `PROCESSING`。

4.  **`SUCCESS`**:
    -   **定義**: Worker 已成功完成資料抓取和儲存。
    -   **觸發**: `fetch_url_data_*` 任務成功執行完畢。

5.  **`FAILED`**:
    -   **定義**: Worker 在處理過程中遇到錯誤 (例如 API 請求失敗、資料驗證錯誤)。
    -   **觸發**: `fetch_url_data_*` 任務執行期間發生異常。失敗的任務將在未來的某個時間點由 Producer 重新選取並分發。

這個狀態機能夠確保系統的穩定性和資料處理的原子性。

### 6.4. 地理編碼處理 (Geocoding Processing)

地理編碼任務現在是獨立於職缺資料抓取和儲存流程的。這意味著 `task_jobs_*.py` 任務不再直接觸發地理編碼。你需要另外建立一個任務來處理地理編碼，例如一個定時任務，它會從資料庫中讀取尚未進行地理編碼的職缺，然後呼叫 `geocode_job_location` 進行處理。

### 6.5. 透過 Pandas 直接連線資料庫 (僅限讀取或特定用途)

在某些特定場景下，例如進行資料分析或快速查詢時，你可能希望直接透過 Pandas 連線到資料庫。此時，你可以使用 `sqlalchemy` 的 `create_engine` 搭配專案的設定來建立連接。

**注意**：這種方式通常用於讀取資料，對於寫入操作，仍建議使用 `get_session` 和 ORM，並透過 Pydantic 模型來確保事務的完整性和一致性。

1.  **安裝必要套件**: 
    ```bash
    uv pip install pandas sqlalchemy mysql-connector-python
    ```

2.  **範例程式碼**: 
    ```python
    import pandas as pd
    from sqlalchemy import create_engine
    from crawler.config import (
        MYSQL_HOST,
        MYSQL_PORT,
        MYSQL_ACCOUNT,
        MYSQL_PASSWORD,
        MYSQL_DATABASE,
    )

    # 建立資料庫連接 URL
    # 使用 mysql+pymysql 驅動
    db_url = (
        f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@"
        f"{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}"
    )

    # 建立 SQLAlchemy 引擎
    engine = create_engine(db_url)

    try:
        # 使用 pandas 讀取資料表
        # 替換 'your_table_name' 為你實際的資料表名稱
        df = pd.read_sql("SELECT * FROM your_table_name", engine)

        # 顯示 DataFrame 的前幾行
        print(df.head())

    except Exception as e:
        print(f"連線或查詢時發生錯誤: {e}")

    finally:
        # 關閉引擎連接池
        engine.dispose()
    ```

---

## 7. 執行爬蟲 (Running the Crawler)

為了確保 Python 的 `import` 路徑正確，應從專案根目錄使用 `-m` 參數來執行模組。

- **啟動 Producer**:
  ```bash
   # 啟動 104 爬蟲的 Producer
   python -m crawler.project_104.producer_category_104
   python -m crawler.project_104.producer_urls_104
   python -m crawler.project_104.producer_jobs_104

   # 啟動 1111 爬蟲的 Producer
   python -m crawler.project_1111.producer_category_1111
   python -m crawler.project_1111.producer_urls_1111
   python -m crawler.project_1111.producer_jobs_1111

   # 啟動 CakeResume 爬蟲的 Producer
   python -m crawler.project_cakeresume.producer_category_cakeresume
   python -m crawler.project_cakeresume.producer_urls_cakeresume
   python -m crawler.project_cakeresume.producer_jobs_cakeresume

   # 啟動 yes123 爬蟲的 Producer
   python -m crawler.project_yes123.producer_category_yes123
   python -m crawler.project_yes123.producer_urls_yes123
   python -m crawler.project_yes123.producer_jobs_yes123
  ```
- **啟動 Worker**:
  ```bash
   celery -A crawler.worker worker --loglevel=info
  ```

---

## 8. 程式碼風格與檢查 (Linting & Formatting)

為了確保程式碼的一致性、可讀性和品質，本專案強制執行自動化的程式碼風格檢查和格式化。

### 8.1. 為什麼需要程式碼風格與檢查？

-   **提高可讀性**：統一的風格讓所有開發者更容易閱讀和理解程式碼。
-   **減少錯誤**：Linter 可以捕捉潛在的錯誤、不一致的行為和不良的程式碼實踐。
-   **加速開發**：減少程式碼審查中關於風格的討論，讓開發者專注於業務邏輯。
-   **自動化**：透過工具自動執行，減少人工干預。

### 8.2. 推薦工具

-   **`ruff` (Linter & Formatter)**: 一個極速的 Python Linter 和 Formatter，旨在取代 `Flake8`, `isort`, `pylint`, `black` 等多個工具，提供統一的程式碼檢查和格式化體驗。

### 8.3. 安裝與使用

請確保你的虛擬環境已啟用。

1.  **安裝工具**:
    ```bash
    uv pip install ruff
    ```

2.  **配置 `ruff`**:
    `ruff` 的配置通常放在 `pyproject.toml` 中。請確保 `pyproject.toml` 中包含以下或類似的配置：
    ```toml
    [tool.ruff]
    line-length = 120
    select = ["E", "F", "W", "I", "N", "D", "UP", "ANN", "ASYNC", "B", "C4", "DTZ", "ERA", "ISC", "ICN", "PIE", "PT", "RSE", "RET", "SIM", "TID", "ARG", "PLC", "PLE", "PLR", "PLW", "TRY", "PERF"]
    ignore = [] # ruff format 會處理行長度，所以不需要忽略 E501

    [tool.ruff.per-file-ignores]
    "__init__.py" = ["F401"] # 忽略 __init__.py 中未使用的 import 警告
    "tests/*" = ["S101"] # 忽略測試檔案中的 assert 警告
    ```
    （**注意**：上述 `select` 和 `ignore` 列表僅為範例，應根據專案實際需求進行調整。）

3.  **執行檢查與格式化**: 

    -   **格式化 (使用 `ruff`)**:
        ```bash
        ruff format .
        ```
        這會自動格式化專案中的所有 Python 檔案。

    -   **檢查 (使用 `ruff`)**:
        ```bash
        ruff check .
        ```
        這會檢查程式碼中的潛在問題。如果發現問題，`ruff` 會提供建議。

    -   **自動修復 (使用 `ruff`)**:
        ```bash
        ruff check . --fix
        ```
        `ruff` 可以自動修復大部分簡單的問題。

### 8.4. 開發流程整合

強烈建議在提交程式碼前執行 `ruff format .` 和 `ruff check . --fix`。

未來可以考慮整合 `pre-commit` hooks 或 CI/CD 流程，在程式碼提交或推送到遠端倉庫時自動執行這些檢查，以確保程式碼品質。



================================================
FILE: docs/project_104_docker_manual.md
================================================
# 專案 104 Docker 化操作手冊

本文件旨在說明如何將 `crawler_jobs` 專案打包成 Docker 映像檔，並如何使用 Docker Compose 管理整個爬蟲系統。

## 1. 前置準備 (Prerequisites)

請確保您的系統已安裝 Docker 和 Docker Compose (或 Docker CLI 內建的 `compose` 外掛)。

## 2. 環境設定 (Environment Setup)

### 2.1. 產生環境變數檔案 (`.env`)

專案使用 `local.ini` 來管理不同環境的設定。在 Docker 環境中，我們需要將 `local.ini` 中的 `[DOCKER]` 區塊設定轉換為 Docker Compose 可讀取的 `.env` 檔案。

在專案根目錄下執行：

```bash
APP_ENV=DOCKER python genenv.py
```

這將會根據 `local.ini` 中的 `[DOCKER]` 區塊產生一個 `.env` 檔案。

### 2.2. 建立共用網路

為了讓所有 Docker 容器能夠互相通訊，我們需要建立一個共用的 Docker 網路。

```bash
docker network create my_network || true
```
`|| true` 確保即使網路已存在也不會報錯。

## 3. Docker 映像檔建置 (Build)

我們使用多階段建置 (Multi-stage Build) 的 `Dockerfile` 來優化映像檔的大小和建置效率。

### 3.1. 建置指令

在專案的根目錄下，執行以下指令來建置映像檔：

```bash
# -f 指定 Dockerfile 的路徑
# -t 為映像檔命名並加上標籤 (tag)，格式為 <your-dockerhub-username>/<image-name>:<version>
# . 表示建置上下文 (build context) 為當前目錄
docker build -f Dockerfile -t benitorhuang/crawler_jobs:0.0.2 .
```

### 3.2. 驗證建置結果

建置完成後，你可以使用以下指令來查看本機的所有映像檔，確認 `benitorhuang/crawler_jobs:0.0.2` 是否已成功建立。

```bash
docker images
```

## 4. 啟動與管理服務 (Docker Compose)

我們使用 Docker Compose 來同時管理多個服務 (MySQL, RabbitMQ, Worker, Producer)。

### 4.1. 啟動核心服務 (MySQL, RabbitMQ, Flower)

首先，啟動資料庫和訊息佇列服務。這些服務通常會長時間運行。

```bash
# 啟動 MySQL 和 phpMyAdmin
docker compose -f mysql-network.yml up -d

# 啟動 RabbitMQ 和 Flower (Celery 監控工具)
docker compose -f rabbitmq-network.yml up -d
```
`-d` 參數表示在背景以分離模式 (detached mode) 執行。

### 4.2. 啟動應用服務 (Worker, Producer)

接下來，啟動我們的爬蟲應用服務。

#### 4.2.1. 啟動 Worker

Worker 是長時間運行的背景服務，它會持續監聽並處理來自 RabbitMQ 的任務。

```bash
# 啟動 Worker 服務
docker compose -f docker-compose-worker-network.yml up -d
```
**注意**：Worker 容器會監聽 `producer_jobs_104`、`producer_category_104`、`producer_urls_104`、`producer_jobs_1111`、`producer_category_1111`、`producer_urls_1111`、`producer_jobs_cakeresume`、`producer_category_cakeresume`、`producer_urls_cakeresume`、`producer_jobs_yes123`、`producer_category_yes123` 和 `producer_urls_yes123` 佇列。

#### 4.2.2. 執行 Producer

Producer 的職責是讀取資料庫中的 URL 並分派任務。它是一個短時間執行的腳本，執行完畢後容器就會停止。

```bash
# 啟動所有 Producer 服務
docker compose -f docker-compose-producer-network.yml up
```

**小量測試範例**：

如果你想進行小量測試，例如只抓取特定數量的 URL 或分類，可以透過修改 Producer 的 `command` 來實現。這需要你直接在 `docker-compose-producer-network.yml` 中修改對應服務的 `command` 行，或者在執行時覆蓋它。

**範例：只分派 1 個分類的 URL 抓取任務 (並限制每個任務只抓取 5 個 URL)**

修改 `docker-compose-producer-network.yml` 中 `producer_104_urls` 服務的 `command`：

```yaml
services:
  producer_104_urls:
    # ... 其他設定 ...
    command: python -m crawler.project_104.producer_urls_104 --limit 1 --url-limit 5
    # ... 其他設定 ...
```

然後運行：

```bash
docker compose -f docker-compose-producer-network.yml up producer_104_urls
```

**注意**：`--limit` 參數用於限制分派的分類數量，`--url-limit` 參數用於限制每個分類任務抓取的 URL 數量。

### 4.3. 查看服務日誌

你可以使用以下指令來查看特定服務的日誌輸出：

```bash
# 查看 Worker 服務的日誌
docker logs -f crawler_system-crawler_104-1

# 查看 Producer 服務的日誌 (如果它還在運行或剛結束)
docker logs -f crawler_system-producer_104_jobs-1
docker logs -f crawler_system-producer_104_category-1
docker logs -f crawler_system-producer_104_urls-1

# 查看 RabbitMQ 服務的日誌
docker logs -f crawler_system-rabbitmq-1
```
**提示**：`crawler_system-` 是 Docker Compose 預設的專案名稱前綴。服務名稱加上 `-1` 是容器的預設名稱。

### 4.4. 停止所有服務

當你完成測試或開發後，可以使用以下指令停止所有由 Docker Compose 啟動的服務：

```bash
# 停止並移除所有服務容器、網路和卷 (如果沒有被其他服務使用)
docker compose -f mysql-network.yml -f rabbitmq-network.yml -f docker-compose-worker-network.yml -f docker-compose-producer-network.yml down
```
**注意**：`down` 命令會停止並移除容器。如果你想保留容器，只停止它們，可以使用 `stop` 命令。

## 5. 推送至 Docker Hub (Optional)

如果你希望將此映像檔分享給團隊或部署到其他環境，可以將其推送到 Docker Hub。

```bash
# 首先登入 Docker Hub
docker login

# 推送映像檔
docker push benitorhuang/crawler_jobs:0.0.2
```

## 6. Docker 環境與本地環境差異

### 6.1. 服務間通訊

在 Docker 環境中，各服務（如 Producer, Worker, MySQL, RabbitMQ）透過 Docker 網路和服務名稱進行通訊。例如，RabbitMQ 的主機名設定為 `rabbitmq`，MySQL 的主機名設定為 `mysql`。

### 6.2. 環境變數管理

Docker 環境透過 `genenv.py` 腳本，根據 `local.ini` 中的 `[DOCKER]` 區塊生成 `.env` 檔案，供 Docker Compose 使用。這確保了 Docker 容器內部使用正確的服務位址和配置。

### 6.3. 執行方式

在 Docker 環境中，Producer、Worker 和 Celery 服務都是作為 Docker 容器運行，並由 Docker Compose 進行編排和管理。這與本地環境中直接透過 `python -m` 或 `celery -A` 命令執行的方式不同。

### 6.4. 資料庫連接

Docker 環境中的服務會連接到 Docker 網路中的 MySQL 容器。而本地環境中的 Python 腳本則會連接到 `local.ini` 中 `[DEV]` 區塊設定的 `127.0.0.1` 上的 MySQL 服務。


================================================
FILE: docs/project_104_local_test_plan.md
================================================
# Project 104 本地測試計畫

本文件旨在提供 `project_104` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_category_104
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 104 的類別資料。

### 測試 `task_urls_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_urls_104
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_104.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_104.task_jobs_104
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_category_104
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_104_categories` 任務被接收並成功執行。

### 測試 `producer_urls_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_urls_104
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_104`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_104.producer_jobs_104
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_104` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```



================================================
FILE: docs/project_1111_local_test_plan.md
================================================
# Project 1111 本地測試計畫

本文件旨在提供 `project_1111` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_category_1111
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 1111 的類別資料。

### 測試 `task_urls_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_urls_1111
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_1111.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_1111.task_jobs_1111
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_category_1111
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_1111_categories` 任務被接收並成功執行。

### 測試 `producer_urls_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_urls_1111
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_1111_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_1111`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_1111.producer_jobs_1111
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_1111` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```



================================================
FILE: docs/project_cakeresume_local_test_plan.md
================================================
[Binary file]


================================================
FILE: docs/project_yes123_local_test_plan.md
================================================
# Project yes123 本地測試計畫

本文件旨在提供 `project_yes123` 相關 Producer 和 Task 的本地測試步驟，確保任務分發、Worker 執行及資料庫寫入流程正常運作。

## 測試前準備

1.  **確保所有 Docker 服務已啟動**：
    ```bash
    docker compose -f mysql-network.yml up -d
    docker compose -f rabbitmq-network.yml up -d
    ```

2.  **確保 `local.ini` 配置正確**：
    在 `local.ini` 的 `[DEV]` 區塊中，確保 `RABBITMQ_HOST` 和 `MYSQL_HOST` 都設定為 `127.0.0.1`，以便本地 Python 腳本能連接到 Docker 容器。

3.  **啟動 Celery Worker**：
    在一個**獨立的終端視窗**中，啟動 Celery Worker。讓此視窗保持開啟，以便觀察 Worker 的日誌輸出。
    ```bash
     celery -A crawler.worker worker --loglevel=info
    ```

## 測試 Task (自動使用 test_db)

本專案中的所有 `task_*.py` 檔案都已內建本地測試模式。當你直接執行這些檔案時，它們會自動將資料庫連線指向 `test_db`，確保測試不會影響到正式的開發資料庫 (`crawler_db`)。

### 測試 `task_category_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_category_yes123
    ```
2.  **觀察日誌**：日誌會顯示 `Connecting to database: test_db@...`，並記錄後續的抓取與同步過程。
3.  **驗證資料庫**：你可以連線到 `test_db` 來驗證 `tb_category_source` 表中是否已寫入 yes123 的類別資料。

### 測試 `task_urls_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_urls_yes123
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取類別，然後抓取 URL 並存入 `test_db` 的過程。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_urls` 和 `tb_url_categories` 表是否已寫入資料。

### 測試 `task_jobs_yes123.py`

1.  **執行任務**：
    ```bash
    python -m crawler.project_yes123.task_jobs_yes123
    ```
2.  **觀察日誌**：日誌會顯示從 `test_db` 讀取待處理的 URL，抓取職缺詳情，並將結果存回 `test_db`。
3.  **驗證資料庫**：驗證 `test_db` 中的 `tb_jobs` 表是否已寫入資料，以及 `tb_urls` 表的 `details_crawl_status` 是否已更新。

## 測試 Producer (使用正式 crawler_db)

Producer 的職責是與正式的 `crawler_db` 互動，產生任務並發送到 RabbitMQ。測試 Producer 時，我們通常會驗證它是否能正確地將任務發送給 Worker。

### 測試 `producer_category_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_category_yes123
    ```
2.  **觀察 Worker 日誌**：回到 Celery Worker 的終端視窗，觀察是否有 `fetch_and_sync_yes123_categories` 任務被接收並成功執行。

### 測試 `producer_urls_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_urls_yes123
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `crawl_and_store_yes123_category_urls` 任務被接收並成功執行。

### 測試 `producer_jobs_yes123`

1.  **執行 Producer**：
    ```bash
    python -m crawler.project_yes123.producer_jobs_yes123
    ```
2.  **觀察 Worker 日誌**：觀察是否有 `fetch_url_data_yes123` 任務被接收並成功執行。

## 監控任務與 Worker 狀態

你可以使用 Flower UI 或 Celery 的命令列工具來監控任務和 Worker 的狀態。

-   **Flower UI**: 訪問 `http://localhost:5555`
-   **Celery Inspect**: 
    ```bash
    # 顯示活躍的任務
    celery -A crawler.worker inspect active

    # 顯示排隊中的任務
    celery -A crawler.worker inspect scheduled
    ```


